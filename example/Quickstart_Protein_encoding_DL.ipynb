{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Get the directory path of the current script\n",
    "current_script_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "# Construct the path to the src directory\n",
    "src_directory = os.path.join(current_script_directory, \"..\", \"src\")\n",
    "srcpro_directory = os.path.join(current_script_directory, \"..\", \"src/propythia\")\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "sys.path.append(src_directory)\n",
    "sys.path.append(srcpro_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Quickstart Protein encodings and DL\n",
    "This jupyter notebook will demonstrate how to obtain protein encodings and use DL models to classify sequences. \n",
    "We will use the same dataset of antimicrobial peptides as the other protein notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 12:15:48.826092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "\n",
    "from propythia.protein.sequence import ReadSequence\n",
    "from propythia.protein.encoding import Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets get the data. \n",
    "\n",
    "We will use the Antimicrobial peptides case study using Veltri study.\n",
    "\n",
    "The collection of data is available at\n",
    "https://www.dveltri.com/ascan/v2/news.html\n",
    "\n",
    "D. Veltri, U. Kamath, A. Shehu, Deep learning improves antimicrobial\n",
    "peptide recognition, Bioinformatics 34 (16) (2018) 2740{2747. doi:10.1093/bioinformatics/bty179.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID                                          sequence  label\n",
      "0  AP02484                             GMASKAGSVLGKITKIALGAL      1\n",
      "1  AP02630       NIGLFTSTCFSSQCFSSKCFTDTCFSSNCFTGRHQCGYTHGSC      1\n",
      "2  AP01427                    GAIKDALKGAAKTVAVELLKKAQCKLEKTC      1\n",
      "3  AP02983                             FFGRLKAVFRGARQGWKEHRY      1\n",
      "4  AP01815  DFGCARGMIFVCMRRCARMYPGSTGYCQGFRCMCDTMIPIRRPPFIMG      1\n"
     ]
    }
   ],
   "source": [
    "amps_file = './AMP_Scan2_Feb2020_Dataset/AMPS_02182020.fasta'\n",
    "non_amps_file = './AMP_Scan2_Feb2020_Dataset/DECOYS_02182020.fasta'\n",
    "\n",
    "\n",
    "sequences = SeqIO.parse(amps_file, \"fasta\")\n",
    "data = []\n",
    "\n",
    "for record in sequences:\n",
    "    data.append([record.id, str(record.seq), 1])\n",
    "\n",
    "sequences = SeqIO.parse(non_amps_file, \"fasta\")\n",
    "for record in sequences:\n",
    "    data.append([record.id, str(record.seq), 0])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"ID\", \"sequence\", 'label'])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 183\n",
      "Minimum length: 11\n"
     ]
    }
   ],
   "source": [
    "# Get the maximum and minimum length of strings in the specified column\n",
    "max_length = df[\"sequence\"].str.len().max()\n",
    "min_length = df[\"sequence\"].str.len().min()\n",
    "\n",
    "print(f\"Maximum length: {max_length}\")\n",
    "print(f\"Minimum length: {min_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module ReadSequence contains functions built to preprocess the protein sequences, by replacing or remove certain amino acids. \n",
    "\n",
    "The function \"par_preprocessing\" was designed to deal with pandas dataframes (it is required to specify the atribute dataset and the column of protein sequences) while the function \"get_preprocessing\" was designed to process only one sequence.\n",
    "\n",
    "The preprocessing phase may be required to calculate certain descriptors features or encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP02484</td>\n",
       "      <td>GMASKAGSVLGKITKIALGAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP02630</td>\n",
       "      <td>NIGLFTSTCFSSQCFSSKCFTDTCFSSNCFTGRHQCGYTHGSC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP01427</td>\n",
       "      <td>GAIKDALKGAAKTVAVELLKKAQCKLEKTC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP02983</td>\n",
       "      <td>FFGRLKAVFRGARQGWKEHRY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP01815</td>\n",
       "      <td>DFGCARGMIFVCMRRCARMYPGSTGYCQGFRCMCDTMIPIRRPPFIMG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>UniRef50_C5DJ44</td>\n",
       "      <td>SSGNVNEVPKQNAKHPMDSCQNLEQSAGTTSAEKEAIRALESQSSG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>UniRef50_Q9XUP3</td>\n",
       "      <td>ESCNFAVFWKLVKGAYKPTTNPNEPFKVPGEVPKMIKPMVGFEDAV...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>UniRef50_Q9Y573</td>\n",
       "      <td>VAALNDCIYSVGGWNETQDALHTVEKYSFEEEKWVEVASMKVPRAG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>UniRef50_Q54H44</td>\n",
       "      <td>PHTHTQKEVITSSVD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>UniRef50_Q50L39</td>\n",
       "      <td>LTLSEFLKLDLEHTKIKMQKQMNLH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4042 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           sequence  \\\n",
       "0             AP02484                              GMASKAGSVLGKITKIALGAL   \n",
       "1             AP02630        NIGLFTSTCFSSQCFSSKCFTDTCFSSNCFTGRHQCGYTHGSC   \n",
       "2             AP01427                     GAIKDALKGAAKTVAVELLKKAQCKLEKTC   \n",
       "3             AP02983                              FFGRLKAVFRGARQGWKEHRY   \n",
       "4             AP01815   DFGCARGMIFVCMRRCARMYPGSTGYCQGFRCMCDTMIPIRRPPFIMG   \n",
       "...               ...                                                ...   \n",
       "4037  UniRef50_C5DJ44  SSGNVNEVPKQNAKHPMDSCQNLEQSAGTTSAEKEAIRALESQSSG...   \n",
       "4038  UniRef50_Q9XUP3  ESCNFAVFWKLVKGAYKPTTNPNEPFKVPGEVPKMIKPMVGFEDAV...   \n",
       "4039  UniRef50_Q9Y573  VAALNDCIYSVGGWNETQDALHTVEKYSFEEEKWVEVASMKVPRAG...   \n",
       "4040  UniRef50_Q54H44                                    PHTHTQKEVITSSVD   \n",
       "4041  UniRef50_Q50L39                          LTLSEFLKLDLEHTKIKMQKQMNLH   \n",
       "\n",
       "      label  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "4037      0  \n",
       "4038      0  \n",
       "4039      0  \n",
       "4040      0  \n",
       "4041      0  \n",
       "\n",
       "[4042 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_seqs = ReadSequence()\n",
    "res = read_seqs.par_preprocessing(dataset= df, col = 'sequence', B ='N', Z = 'Q', U = 'C', O = 'K', J = 'I', X = '')\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Encode sequences\n",
    "The module Encoding performs the encoding of the protein sequences.\n",
    "\n",
    "As the ProteinDescriptors it accepts as dataset a pandas dataframe, list of sequences and a sequence (string) as dataset. The parameter col is the name of the column to store the sequences, or the column where the sequences are present (pandas dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP02484</td>\n",
       "      <td>GMASKAGSVLGKITKIALGAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP02630</td>\n",
       "      <td>NIGLFTSTCFSSQCFSSKCFTDTCFSSNCFTGRHQCGYTHGSC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP01427</td>\n",
       "      <td>GAIKDALKGAAKTVAVELLKKAQCKLEKTC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP02983</td>\n",
       "      <td>FFGRLKAVFRGARQGWKEHRY</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP01815</td>\n",
       "      <td>DFGCARGMIFVCMRRCARMYPGSTGYCQGFRCMCDTMIPIRRPPFIMG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>UniRef50_C5DJ44</td>\n",
       "      <td>SSGNVNEVPKQNAKHPMDSCQNLEQSAGTTSAEKEAIRALESQSSG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>UniRef50_Q9XUP3</td>\n",
       "      <td>ESCNFAVFWKLVKGAYKPTTNPNEPFKVPGEVPKMIKPMVGFEDAV...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>UniRef50_Q9Y573</td>\n",
       "      <td>VAALNDCIYSVGGWNETQDALHTVEKYSFEEEKWVEVASMKVPRAG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>UniRef50_Q54H44</td>\n",
       "      <td>PHTHTQKEVITSSVD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>UniRef50_Q50L39</td>\n",
       "      <td>LTLSEFLKLDLEHTKIKMQKQMNLH</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4042 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           sequence  \\\n",
       "0             AP02484                              GMASKAGSVLGKITKIALGAL   \n",
       "1             AP02630        NIGLFTSTCFSSQCFSSKCFTDTCFSSNCFTGRHQCGYTHGSC   \n",
       "2             AP01427                     GAIKDALKGAAKTVAVELLKKAQCKLEKTC   \n",
       "3             AP02983                              FFGRLKAVFRGARQGWKEHRY   \n",
       "4             AP01815   DFGCARGMIFVCMRRCARMYPGSTGYCQGFRCMCDTMIPIRRPPFIMG   \n",
       "...               ...                                                ...   \n",
       "4037  UniRef50_C5DJ44  SSGNVNEVPKQNAKHPMDSCQNLEQSAGTTSAEKEAIRALESQSSG...   \n",
       "4038  UniRef50_Q9XUP3  ESCNFAVFWKLVKGAYKPTTNPNEPFKVPGEVPKMIKPMVGFEDAV...   \n",
       "4039  UniRef50_Q9Y573  VAALNDCIYSVGGWNETQDALHTVEKYSFEEEKWVEVASMKVPRAG...   \n",
       "4040  UniRef50_Q54H44                                    PHTHTQKEVITSSVD   \n",
       "4041  UniRef50_Q50L39                          LTLSEFLKLDLEHTKIKMQKQMNLH   \n",
       "\n",
       "      label  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  \n",
       "...     ...  \n",
       "4037      0  \n",
       "4038      0  \n",
       "4039      0  \n",
       "4040      0  \n",
       "4041      0  \n",
       "\n",
       "[4042 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enconde_df = Encoding(dataset= df ,  col= 'sequence')\n",
    "enconde_df.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the encoding operations the intended function must be called. \n",
    "\n",
    "It can be one-hot-encoded, NLF, Blosum, z_scale. It also can be performed a padding to all sequences in the dataframe.\n",
    "Lets look at them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 12:15:50.597914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-25 12:15:50.597914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-25 12:15:50.597914: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-25 12:15:50.597915: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "2023-08-25 12:15:52.635252: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-25 12:15:52.948726: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-25 12:15:53.272182: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-25 12:15:53.601635: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 21)\n",
      "0       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...\n",
      "1       [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "2       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...\n",
      "3       [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "4       [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "                              ...                        \n",
      "4037    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "4038    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...\n",
      "4039    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "4040    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "4041    [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n",
      "Name: One_hot_encoding, Length: 4042, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZNK3c1010TensorImpl36is_contiguous_nondefault_policy_implENS_12MemoryFormatE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "hot_encoded = enconde_df.get_hot_encoded()\n",
    "print(hot_encoded['One_hot_encoding'][0].shape)\n",
    "print(hot_encoded['One_hot_encoding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 18)\n",
      "0       [[1.32, 2.05, 0.6, 0.31, 0.61, 0.58, 0.0, 0.3,...\n",
      "1       [[1.68, 0.3, 0.49, 0.15, 0.09, 0.59, 0.06, 0.0...\n",
      "2       [[1.32, 2.05, 0.6, 0.31, 0.61, 0.58, 0.0, 0.3,...\n",
      "3       [[2.37, 0.23, 0.09, 0.37, 0.19, 0.04, 0.03, 0....\n",
      "4       [[0.81, 0.13, 1.36, 0.63, 0.15, 0.1, 0.45, 0.3...\n",
      "                              ...                        \n",
      "4037    [[1.47, 1.11, 0.27, 0.13, 0.15, 0.22, 0.09, 0....\n",
      "4038    [[1.56, 0.48, 0.87, 0.02, 0.07, 0.13, 0.22, 0....\n",
      "4039    [[1.33, 1.39, 0.15, 0.4, 0.04, 0.27, 0.07, 0.1...\n",
      "4040    [[1.41, 0.27, 1.09, 0.77, 0.87, 0.33, 0.04, 0....\n",
      "4041    [[1.29, 1.21, 0.25, 0.96, 0.18, 0.06, 0.04, 0....\n",
      "Name: nlf, Length: 4042, dtype: object\n"
     ]
    }
   ],
   "source": [
    "nlf = enconde_df.get_nlf()\n",
    "print(np.array(nlf['nlf'][0]).shape)\n",
    "print(nlf['nlf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 23)\n",
      "0       [[0, -2, 0, -1, -3, -2, -2, 6, -2, -4, -4, -2,...\n",
      "1       [[-2, 0, 6, 1, -3, 0, 0, 0, 1, -3, -3, 0, -2, ...\n",
      "2       [[0, -2, 0, -1, -3, -2, -2, 6, -2, -4, -4, -2,...\n",
      "3       [[-2, -3, -3, -3, -2, -3, -3, -3, -1, 0, 0, -3...\n",
      "4       [[-2, -2, 1, 6, -3, 0, 2, -1, -1, -3, -4, -1, ...\n",
      "                              ...                        \n",
      "4037    [[1, -1, 1, 0, -1, 0, 0, 0, -1, -2, -2, 0, -1,...\n",
      "4038    [[-1, 0, 0, 2, -4, 2, 5, -2, 0, -3, -3, 1, -2,...\n",
      "4039    [[0, -3, -3, -3, -1, -2, -2, -3, -3, 3, 1, -2,...\n",
      "4040    [[-1, -2, -2, -1, -3, -1, -1, -2, -2, -3, -3, ...\n",
      "4041    [[-1, -2, -3, -4, -1, -2, -3, -4, -3, 2, 4, -2...\n",
      "Name: blosum, Length: 4042, dtype: object\n"
     ]
    }
   ],
   "source": [
    "blosum = enconde_df.get_blosum()\n",
    "print(np.array(blosum['blosum'][0]).shape)\n",
    "print(blosum['blosum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 5)\n",
      "0       [[2.05, -4.06, 0.36, -0.82, -0.38], [-2.85, -0...\n",
      "1       [[3.05, 1.62, 1.04, -1.15, 1.61], [-3.89, -1.7...\n",
      "2       [[2.05, -4.06, 0.36, -0.82, -0.38], [0.24, -2....\n",
      "3       [[-4.22, 1.94, 1.06, 0.54, -0.62], [-4.22, 1.9...\n",
      "4       [[3.98, 0.93, 1.93, -2.46, 0.75], [-4.22, 1.94...\n",
      "                              ...                        \n",
      "4037    [[2.39, -1.07, 1.15, -1.39, 0.67], [2.39, -1.0...\n",
      "4038    [[3.11, 0.26, -0.11, -0.34, -0.25], [2.39, -1....\n",
      "4039    [[-2.59, -2.64, -1.54, -0.85, -0.02], [0.24, -...\n",
      "4040    [[-1.66, 0.27, 1.84, 0.7, 2.0], [2.47, 1.95, 0...\n",
      "4041    [[-4.28, -1.3, -1.49, -0.72, 0.84], [0.75, -2....\n",
      "Name: zscale, Length: 4042, dtype: object\n"
     ]
    }
   ],
   "source": [
    "zscale = enconde_df.get_zscale()\n",
    "print(np.array(zscale['zscale'][0]).shape)\n",
    "print(zscale['zscale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. one hot encoding + DL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encodings will have give to each aminoacid a vector of 21 (hot encoded), 18 (nlf), 23 (blosum)  or 5 (z scale) dimensions. \n",
    "This means that each sequence will be represented as a vector of X * sequence length. \n",
    "In the lines above we print the first sequence that has 21 aminoacids. however, different sequences will have different lengths. \n",
    "Deep learning models cannot deal with different shape inputs. \n",
    "Therefore, one must assure that all sequences have the same lengths. This is achieved padding sequences with 0s until a defined length, or truncating sequences (cut) that have more than a defined length. \n",
    "\n",
    "\n",
    "The function get_pad_and_hot_encoding allows to perform the padding and the one hot encoding of the sequence at the same time. The one-hot-encoded sequence will have the shape of (length of the sequences, number of amino acids in the alphabet).\n",
    "\n",
    "As the antimicrobial peptides in the dataset are small. Is not very computationally expensive to padd all sequences to the max len - 183\n",
    "\n",
    "Scaling and feature selectin is not required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "      <th>pad_seques</th>\n",
       "      <th>One_hot_encoding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP02484</td>\n",
       "      <td>GMASKAGSVLGKITKIALGAL</td>\n",
       "      <td>1</td>\n",
       "      <td>GMASKAGSVLGKITKIALGALXXXXXXXXXXXXXXXXXXXXXXXXX...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP02630</td>\n",
       "      <td>NIGLFTSTCFSSQCFSSKCFTDTCFSSNCFTGRHQCGYTHGSC</td>\n",
       "      <td>1</td>\n",
       "      <td>NIGLFTSTCFSSQCFSSKCFTDTCFSSNCFTGRHQCGYTHGSCXXX...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP01427</td>\n",
       "      <td>GAIKDALKGAAKTVAVELLKKAQCKLEKTC</td>\n",
       "      <td>1</td>\n",
       "      <td>GAIKDALKGAAKTVAVELLKKAQCKLEKTCXXXXXXXXXXXXXXXX...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP02983</td>\n",
       "      <td>FFGRLKAVFRGARQGWKEHRY</td>\n",
       "      <td>1</td>\n",
       "      <td>FFGRLKAVFRGARQGWKEHRYXXXXXXXXXXXXXXXXXXXXXXXXX...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP01815</td>\n",
       "      <td>DFGCARGMIFVCMRRCARMYPGSTGYCQGFRCMCDTMIPIRRPPFIMG</td>\n",
       "      <td>1</td>\n",
       "      <td>DFGCARGMIFVCMRRCARMYPGSTGYCQGFRCMCDTMIPIRRPPFI...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>UniRef50_C5DJ44</td>\n",
       "      <td>SSGNVNEVPKQNAKHPMDSCQNLEQSAGTTSAEKEAIRALESQSSG...</td>\n",
       "      <td>0</td>\n",
       "      <td>SSGNVNEVPKQNAKHPMDSCQNLEQSAGTTSAEKEAIRALESQSSG...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>UniRef50_Q9XUP3</td>\n",
       "      <td>ESCNFAVFWKLVKGAYKPTTNPNEPFKVPGEVPKMIKPMVGFEDAV...</td>\n",
       "      <td>0</td>\n",
       "      <td>ESCNFAVFWKLVKGAYKPTTNPNEPFKVPGEVPKMIKPMVGFEDAV...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>UniRef50_Q9Y573</td>\n",
       "      <td>VAALNDCIYSVGGWNETQDALHTVEKYSFEEEKWVEVASMKVPRAG...</td>\n",
       "      <td>0</td>\n",
       "      <td>VAALNDCIYSVGGWNETQDALHTVEKYSFEEEKWVEVASMKVPRAG...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>UniRef50_Q54H44</td>\n",
       "      <td>PHTHTQKEVITSSVD</td>\n",
       "      <td>0</td>\n",
       "      <td>PHTHTQKEVITSSVDXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>UniRef50_Q50L39</td>\n",
       "      <td>LTLSEFLKLDLEHTKIKMQKQMNLH</td>\n",
       "      <td>0</td>\n",
       "      <td>LTLSEFLKLDLEHTKIKMQKQMNLHXXXXXXXXXXXXXXXXXXXXX...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4042 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                           sequence  \\\n",
       "0             AP02484                              GMASKAGSVLGKITKIALGAL   \n",
       "1             AP02630        NIGLFTSTCFSSQCFSSKCFTDTCFSSNCFTGRHQCGYTHGSC   \n",
       "2             AP01427                     GAIKDALKGAAKTVAVELLKKAQCKLEKTC   \n",
       "3             AP02983                              FFGRLKAVFRGARQGWKEHRY   \n",
       "4             AP01815   DFGCARGMIFVCMRRCARMYPGSTGYCQGFRCMCDTMIPIRRPPFIMG   \n",
       "...               ...                                                ...   \n",
       "4037  UniRef50_C5DJ44  SSGNVNEVPKQNAKHPMDSCQNLEQSAGTTSAEKEAIRALESQSSG...   \n",
       "4038  UniRef50_Q9XUP3  ESCNFAVFWKLVKGAYKPTTNPNEPFKVPGEVPKMIKPMVGFEDAV...   \n",
       "4039  UniRef50_Q9Y573  VAALNDCIYSVGGWNETQDALHTVEKYSFEEEKWVEVASMKVPRAG...   \n",
       "4040  UniRef50_Q54H44                                    PHTHTQKEVITSSVD   \n",
       "4041  UniRef50_Q50L39                          LTLSEFLKLDLEHTKIKMQKQMNLH   \n",
       "\n",
       "      label                                         pad_seques  \\\n",
       "0         1  GMASKAGSVLGKITKIALGALXXXXXXXXXXXXXXXXXXXXXXXXX...   \n",
       "1         1  NIGLFTSTCFSSQCFSSKCFTDTCFSSNCFTGRHQCGYTHGSCXXX...   \n",
       "2         1  GAIKDALKGAAKTVAVELLKKAQCKLEKTCXXXXXXXXXXXXXXXX...   \n",
       "3         1  FFGRLKAVFRGARQGWKEHRYXXXXXXXXXXXXXXXXXXXXXXXXX...   \n",
       "4         1  DFGCARGMIFVCMRRCARMYPGSTGYCQGFRCMCDTMIPIRRPPFI...   \n",
       "...     ...                                                ...   \n",
       "4037      0  SSGNVNEVPKQNAKHPMDSCQNLEQSAGTTSAEKEAIRALESQSSG...   \n",
       "4038      0  ESCNFAVFWKLVKGAYKPTTNPNEPFKVPGEVPKMIKPMVGFEDAV...   \n",
       "4039      0  VAALNDCIYSVGGWNETQDALHTVEKYSFEEEKWVEVASMKVPRAG...   \n",
       "4040      0  PHTHTQKEVITSSVDXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...   \n",
       "4041      0  LTLSEFLKLDLEHTKIKMQKQMNLHXXXXXXXXXXXXXXXXXXXXX...   \n",
       "\n",
       "                                       One_hot_encoding  \n",
       "0     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...  \n",
       "1     [[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "2     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0,...  \n",
       "3     [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4     [[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "...                                                 ...  \n",
       "4037  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4038  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0,...  \n",
       "4039  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4040  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "4041  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
       "\n",
       "[4042 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enconde_df = Encoding(dataset= df ,  col= 'sequence')\n",
    "\n",
    "res = enconde_df.get_pad_and_hot_encoding(seq_len=183)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (2708, 183, 21)\n",
      "test_x (1334, 183, 21)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "expanded_arrays = res['One_hot_encoding'].apply(lambda x: np.array(x))\n",
    "X = np.array(expanded_arrays.tolist())\n",
    "\n",
    "\n",
    "# X = np.array(res['One_hot_encoding'].apply(lambda x: np.array(x[0])))\n",
    "y = res['label']\n",
    "\n",
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "print('train_x', X_train.shape)\n",
    "print('test_x', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define a DL model. \n",
    "Convolutional and RNN are good choices for this problem. \n",
    "\n",
    "Besides that, adding callbacks such as early stopping and modelCheckpoint may be very beneficial\n",
    "\n",
    "We first will use the tensorfow library as example and then a DL model train with Propythia will be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D,Conv2D, Flatten, MaxPool1D,MaxPool2D, Dropout, Input,GRU\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint(filepath='best_model.h5', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 12:15:58.477915: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-25 12:15:58.478424: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-08-25 12:15:58.521453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:58.521579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-08-25 12:15:58.521621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:58.521719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-08-25 12:15:58.521735: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-25 12:15:58.522859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-08-25 12:15:58.522887: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-08-25 12:15:58.523849: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-08-25 12:15:58.524004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-08-25 12:15:58.525016: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-08-25 12:15:58.525540: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-08-25 12:15:58.527700: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-08-25 12:15:58.527780: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:58.527938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:58.528062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:58.528180: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:58.528271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2023-08-25 12:15:58.528587: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-25 12:15:58.528846: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-08-25 12:15:58.648463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:58.648579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-08-25 12:15:58.648646: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:58.648735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:02:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
      "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
      "2023-08-25 12:15:58.648760: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-25 12:15:58.648779: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-08-25 12:15:58.648790: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2023-08-25 12:15:58.648802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-08-25 12:15:58.648813: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-08-25 12:15:58.648824: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-08-25 12:15:58.648836: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-08-25 12:15:58.648847: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-08-25 12:15:58.648884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:58.649002: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:58.649116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:58.649230: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:58.649315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2023-08-25 12:15:58.649335: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-08-25 12:15:59.209302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-08-25 12:15:59.209322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2023-08-25 12:15:59.209326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N \n",
      "2023-08-25 12:15:59.209328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N \n",
      "2023-08-25 12:15:59.209507: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:59.209669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:59.209825: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:59.209962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:59.210064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10065 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
      "2023-08-25 12:15:59.210318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:59.210440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-25 12:15:59.210533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10065 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:02:00.0, compute capability: 7.5)\n",
      "2023-08-25 12:15:59.355243: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-08-25 12:15:59.355532: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3000000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-25 12:15:59.674957: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2023-08-25 12:15:59.814545: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-08-25 12:16:00.501274: W tensorflow/stream_executor/gpu/asm_compiler.cc:63] Running ptxas --version returned 256\n",
      "2023-08-25 12:16:00.589148: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: \n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 11s 190ms/step - loss: 0.7039 - accuracy: 0.4979 - val_loss: 0.6524 - val_accuracy: 0.7860\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.5635 - accuracy: 0.7892 - val_loss: 0.3239 - val_accuracy: 0.8708\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.3167 - accuracy: 0.8801 - val_loss: 0.3054 - val_accuracy: 0.8672\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2687 - accuracy: 0.8951 - val_loss: 0.3251 - val_accuracy: 0.8690\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.9181 - val_loss: 0.2939 - val_accuracy: 0.8653\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2132 - accuracy: 0.9209 - val_loss: 0.2883 - val_accuracy: 0.8745\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9467 - val_loss: 0.2859 - val_accuracy: 0.8801\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.1524 - accuracy: 0.9563 - val_loss: 0.3271 - val_accuracy: 0.8745\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.1110 - accuracy: 0.9694 - val_loss: 0.3686 - val_accuracy: 0.8782\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9763 - val_loss: 0.3472 - val_accuracy: 0.8819\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0647 - accuracy: 0.9768 - val_loss: 0.3902 - val_accuracy: 0.8838\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9917 - val_loss: 0.4035 - val_accuracy: 0.8985\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.9970 - val_loss: 0.4183 - val_accuracy: 0.8875\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9989 - val_loss: 0.4439 - val_accuracy: 0.8911\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.4712 - val_accuracy: 0.8911\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0081 - accuracy: 0.9990 - val_loss: 0.5840 - val_accuracy: 0.8745\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5587 - val_accuracy: 0.8893\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.5765 - val_accuracy: 0.9004\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.5974 - val_accuracy: 0.8930\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6076 - val_accuracy: 0.9004\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6365 - val_accuracy: 0.8911\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6278 - val_accuracy: 0.8967\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 9.8214e-04 - accuracy: 1.0000 - val_loss: 0.6415 - val_accuracy: 0.9004\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 4.1730e-04 - accuracy: 1.0000 - val_loss: 0.6689 - val_accuracy: 0.8948\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 4.4577e-04 - accuracy: 1.0000 - val_loss: 0.6762 - val_accuracy: 0.8967\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 3.0726e-04 - accuracy: 1.0000 - val_loss: 0.6967 - val_accuracy: 0.8985\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 3.2564e-04 - accuracy: 1.0000 - val_loss: 0.7128 - val_accuracy: 0.8893\n",
      "42/42 [==============================] - 4s 47ms/step - loss: 0.3145 - accuracy: 0.8823\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(183, 21)),\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    MaxPool1D(pool_size=2),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    MaxPool1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.2,callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Mean Training Accuracy: 0.9534899614475392\n",
      "Mean Validation Accuracy: 0.8823971549669901\n",
      "Mean Training Loss: 0.10858457520522212\n",
      "Mean Validation Loss: 0.48998429046736824\n",
      "===================================\n",
      "Test Loss: 0.3145\n",
      "Test Accuracy: 0.8823\n"
     ]
    }
   ],
   "source": [
    "print('===================================')\n",
    "print(\"Mean Training Accuracy:\", np.mean(history.history['accuracy']))\n",
    "print(\"Mean Validation Accuracy:\", np.mean(history.history['val_accuracy']))\n",
    "print(\"Mean Training Loss:\", np.mean(history.history['loss']))\n",
    "print(\"Mean Validation Loss:\", np.mean(history.history['val_loss']))\n",
    "\n",
    "print('===================================')\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a RNN - GRU based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/34 [==============================] - 1s 17ms/step - loss: 0.6875 - accuracy: 0.5490 - val_loss: 0.5578 - val_accuracy: 0.8395\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.5196 - accuracy: 0.7763 - val_loss: 0.3332 - val_accuracy: 0.8745\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.3583 - accuracy: 0.8500 - val_loss: 0.3301 - val_accuracy: 0.8598\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.3308 - accuracy: 0.8578 - val_loss: 0.2829 - val_accuracy: 0.8893\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2912 - accuracy: 0.8874 - val_loss: 0.2791 - val_accuracy: 0.8930\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.2884 - accuracy: 0.8797 - val_loss: 0.2964 - val_accuracy: 0.8635\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2906 - accuracy: 0.8766 - val_loss: 0.2674 - val_accuracy: 0.8819\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.2677 - accuracy: 0.8910 - val_loss: 0.2634 - val_accuracy: 0.8838\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.2538 - accuracy: 0.9049 - val_loss: 0.2710 - val_accuracy: 0.8782\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2534 - accuracy: 0.9189 - val_loss: 0.2826 - val_accuracy: 0.8930\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.2431 - accuracy: 0.9074 - val_loss: 0.2682 - val_accuracy: 0.8967\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2496 - accuracy: 0.9123 - val_loss: 0.2567 - val_accuracy: 0.8911\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.1967 - accuracy: 0.9329 - val_loss: 0.2577 - val_accuracy: 0.8930\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.2098 - accuracy: 0.9283 - val_loss: 0.2538 - val_accuracy: 0.8930\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2047 - accuracy: 0.9311 - val_loss: 0.2590 - val_accuracy: 0.8930\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.1890 - accuracy: 0.9299 - val_loss: 0.2583 - val_accuracy: 0.8875\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.1748 - accuracy: 0.9340 - val_loss: 0.2682 - val_accuracy: 0.8911\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1972 - accuracy: 0.9220 - val_loss: 0.2741 - val_accuracy: 0.8930\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.1824 - accuracy: 0.9238 - val_loss: 0.2663 - val_accuracy: 0.8875\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.1805 - accuracy: 0.9291 - val_loss: 0.2645 - val_accuracy: 0.8930\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1600 - accuracy: 0.9412 - val_loss: 0.2677 - val_accuracy: 0.8930\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.1434 - accuracy: 0.9499 - val_loss: 0.2692 - val_accuracy: 0.8893\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1543 - accuracy: 0.9480 - val_loss: 0.2813 - val_accuracy: 0.8911\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1536 - accuracy: 0.9424 - val_loss: 0.2799 - val_accuracy: 0.8838\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.1508 - accuracy: 0.9521 - val_loss: 0.2761 - val_accuracy: 0.8930\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.1424 - accuracy: 0.9504 - val_loss: 0.2934 - val_accuracy: 0.8856\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1324 - accuracy: 0.9523 - val_loss: 0.2889 - val_accuracy: 0.8911\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1194 - accuracy: 0.9535 - val_loss: 0.2996 - val_accuracy: 0.8819\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.1388 - accuracy: 0.9482 - val_loss: 0.2966 - val_accuracy: 0.8948\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.1273 - accuracy: 0.9512 - val_loss: 0.2949 - val_accuracy: 0.8893\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1067 - accuracy: 0.9590 - val_loss: 0.3063 - val_accuracy: 0.8856\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1047 - accuracy: 0.9610 - val_loss: 0.3070 - val_accuracy: 0.8985\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.1090 - accuracy: 0.9561 - val_loss: 0.3197 - val_accuracy: 0.8893\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.0980 - accuracy: 0.9654 - val_loss: 0.3256 - val_accuracy: 0.8930\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.2889 - accuracy: 0.8951\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(183, 21)),\n",
    "    GRU(units=32, return_sequences=True),\n",
    "    Flatten(),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=64,epochs = 100,validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Mean Training Accuracy: 0.9131225926034591\n",
      "Mean Validation Accuracy: 0.8863143044359544\n",
      "Mean Training Loss: 0.21726608758463578\n",
      "Mean Validation Loss: 0.2910834103822708\n",
      "===================================\n",
      "Test Loss: 0.2889\n",
      "Test Accuracy: 0.8951\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('===================================')\n",
    "print(\"Mean Training Accuracy:\", np.mean(history.history['accuracy']))\n",
    "print(\"Mean Validation Accuracy:\", np.mean(history.history['val_accuracy']))\n",
    "print(\"Mean Training Loss:\", np.mean(history.history['loss']))\n",
    "print(\"Mean Validation Loss:\", np.mean(history.history['val_loss']))\n",
    "\n",
    "print('===================================')\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network with CONV and LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 9s 281ms/step - loss: 0.6914 - accuracy: 0.5207 - val_loss: 0.6680 - val_accuracy: 0.6109\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.6055 - accuracy: 0.6898 - val_loss: 0.5222 - val_accuracy: 0.7978\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.4853 - accuracy: 0.8018 - val_loss: 0.4079 - val_accuracy: 0.8356\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.3981 - accuracy: 0.8520 - val_loss: 0.3751 - val_accuracy: 0.8477\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 0.3479 - accuracy: 0.8744 - val_loss: 0.3796 - val_accuracy: 0.8578\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.3511 - accuracy: 0.8673 - val_loss: 0.3777 - val_accuracy: 0.8427\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.3253 - accuracy: 0.8759 - val_loss: 0.3479 - val_accuracy: 0.8594\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.3058 - accuracy: 0.8840 - val_loss: 0.3700 - val_accuracy: 0.8424\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.3395 - accuracy: 0.8614 - val_loss: 0.3591 - val_accuracy: 0.8508\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.2990 - accuracy: 0.8877 - val_loss: 0.3171 - val_accuracy: 0.8755\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2758 - accuracy: 0.8993 - val_loss: 0.3215 - val_accuracy: 0.8638\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.2623 - accuracy: 0.9073 - val_loss: 0.3046 - val_accuracy: 0.8769\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2611 - accuracy: 0.9045 - val_loss: 0.4012 - val_accuracy: 0.8360\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2470 - accuracy: 0.9082 - val_loss: 0.3010 - val_accuracy: 0.8724\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.2422 - accuracy: 0.9128 - val_loss: 0.3721 - val_accuracy: 0.8574\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.2725 - accuracy: 0.8896 - val_loss: 0.3783 - val_accuracy: 0.8767\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.2219 - accuracy: 0.9218 - val_loss: 0.3133 - val_accuracy: 0.8662\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1912 - accuracy: 0.9364 - val_loss: 0.3572 - val_accuracy: 0.8553\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.2244 - accuracy: 0.9189 - val_loss: 0.3573 - val_accuracy: 0.8687\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 0.1894 - accuracy: 0.9382 - val_loss: 0.3220 - val_accuracy: 0.8851\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1634 - accuracy: 0.9481 - val_loss: 0.3470 - val_accuracy: 0.8773\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.1610 - accuracy: 0.9460 - val_loss: 0.5062 - val_accuracy: 0.7250\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 0.2625 - accuracy: 0.8818 - val_loss: 0.3636 - val_accuracy: 0.8586\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1925 - accuracy: 0.9346 - val_loss: 0.5255 - val_accuracy: 0.8475\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.1534 - accuracy: 0.9503 - val_loss: 0.3899 - val_accuracy: 0.8751\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.1474 - accuracy: 0.9548 - val_loss: 0.4328 - val_accuracy: 0.8568\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1355 - accuracy: 0.9589 - val_loss: 0.3901 - val_accuracy: 0.8862\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.1508 - accuracy: 0.9505 - val_loss: 0.3476 - val_accuracy: 0.8768\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.2618 - accuracy: 0.9064 - val_loss: 0.3877 - val_accuracy: 0.8835\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.1478 - accuracy: 0.9549 - val_loss: 0.4152 - val_accuracy: 0.8841\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1295 - accuracy: 0.9612 - val_loss: 0.3825 - val_accuracy: 0.8913\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.1170 - accuracy: 0.9655 - val_loss: 0.6053 - val_accuracy: 0.8660\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 9ms/step - loss: 0.1089 - accuracy: 0.9664 - val_loss: 0.5337 - val_accuracy: 0.8814\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 0.1580 - accuracy: 0.9448 - val_loss: 0.5144 - val_accuracy: 0.8468\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3285 - accuracy: 0.8665\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Input(shape=(183, 21)),\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    MaxPool1D(pool_size=2),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    MaxPool1D(pool_size=2),\n",
    "    LSTM(units=128, return_sequences=True),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=100, validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Mean Training Accuracy: 0.8972128489438225\n",
      "Mean Validation Accuracy: 0.8510492987492505\n",
      "Mean Training Loss: 0.2609128632089671\n",
      "Mean Validation Loss: 0.4027911407106063\n",
      "===================================\n",
      "Test Loss: 0.3285\n",
      "Test Accuracy: 0.8665\n"
     ]
    }
   ],
   "source": [
    "print('===================================')\n",
    "print(\"Mean Training Accuracy:\", np.mean(history.history['accuracy']))\n",
    "print(\"Mean Validation Accuracy:\", np.mean(history.history['val_accuracy']))\n",
    "print(\"Mean Training Loss:\", np.mean(history.history['loss']))\n",
    "print(\"Mean Validation Loss:\", np.mean(history.history['val_loss']))\n",
    "\n",
    "print('===================================')\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets try an RNN with other encoding models\n",
    "Be awre that for a fair comparison the same train test split should be used. \n",
    "In this case, we dont have a function to do it automatically, so we will pad sequences until 18 manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (2708, 183, 5)\n",
      "test_x (1334, 183, 5)\n"
     ]
    }
   ],
   "source": [
    "max_length = 183\n",
    "padding_value = 'X'\n",
    "df['padded_sequence'] = [seq + padding_value * (max_length - len(seq)) for seq in df['sequence']]\n",
    "\n",
    "#open \n",
    "enconde_df = Encoding(dataset= df ,  col= 'padded_sequence')\n",
    "\n",
    "zscale = enconde_df.get_zscale()\n",
    "\n",
    "expanded_arrays =  zscale['zscale'].apply(lambda x: np.array(x))\n",
    "X = np.array(expanded_arrays.tolist())\n",
    "\n",
    "y = zscale['label']\n",
    "\n",
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "print('train_x', X_train.shape)\n",
    "print('test_x', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/34 [==============================] - 1s 16ms/step - loss: 0.6294 - accuracy: 0.6483 - val_loss: 0.4132 - val_accuracy: 0.8247\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.4267 - accuracy: 0.8252 - val_loss: 0.3631 - val_accuracy: 0.8469\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.3823 - accuracy: 0.8348 - val_loss: 0.3342 - val_accuracy: 0.8598\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.3034 - accuracy: 0.8794 - val_loss: 0.3279 - val_accuracy: 0.8653\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.2888 - accuracy: 0.8814 - val_loss: 0.3088 - val_accuracy: 0.8672\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2498 - accuracy: 0.9042 - val_loss: 0.3038 - val_accuracy: 0.8653\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2507 - accuracy: 0.9029 - val_loss: 0.2919 - val_accuracy: 0.8690\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2166 - accuracy: 0.9128 - val_loss: 0.2800 - val_accuracy: 0.8801\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2195 - accuracy: 0.9102 - val_loss: 0.2871 - val_accuracy: 0.8745\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2166 - accuracy: 0.9283 - val_loss: 0.3009 - val_accuracy: 0.8764\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1990 - accuracy: 0.9255 - val_loss: 0.2842 - val_accuracy: 0.8875\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.1783 - accuracy: 0.9346 - val_loss: 0.2987 - val_accuracy: 0.8745\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1782 - accuracy: 0.9328 - val_loss: 0.3076 - val_accuracy: 0.8745\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.1612 - accuracy: 0.9365 - val_loss: 0.2866 - val_accuracy: 0.8893\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1703 - accuracy: 0.9350 - val_loss: 0.2934 - val_accuracy: 0.8911\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.1477 - accuracy: 0.9444 - val_loss: 0.3076 - val_accuracy: 0.8745\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.1512 - accuracy: 0.9486 - val_loss: 0.3122 - val_accuracy: 0.8838\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1385 - accuracy: 0.9540 - val_loss: 0.3070 - val_accuracy: 0.8893\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.1303 - accuracy: 0.9562 - val_loss: 0.3085 - val_accuracy: 0.8893\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.1264 - accuracy: 0.9599 - val_loss: 0.3242 - val_accuracy: 0.8819\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 0s 8ms/step - loss: 0.1243 - accuracy: 0.9565 - val_loss: 0.3148 - val_accuracy: 0.8893\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.1096 - accuracy: 0.9613 - val_loss: 0.3111 - val_accuracy: 0.8856\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.1199 - accuracy: 0.9545 - val_loss: 0.3280 - val_accuracy: 0.8911\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.1232 - accuracy: 0.9568 - val_loss: 0.3640 - val_accuracy: 0.8801\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.1213 - accuracy: 0.9556 - val_loss: 0.3338 - val_accuracy: 0.8911\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.1251 - accuracy: 0.9525 - val_loss: 0.3249 - val_accuracy: 0.8893\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.1039 - accuracy: 0.9607 - val_loss: 0.3702 - val_accuracy: 0.8838\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.0995 - accuracy: 0.9566 - val_loss: 0.3387 - val_accuracy: 0.8893\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.3027 - accuracy: 0.8793\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input(shape=(183, 5)),\n",
    "    LSTM(units=32, return_sequences=True),\n",
    "    Flatten(),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=64,epochs = 100,validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "Mean Training Accuracy: 0.9202611808265958\n",
      "Mean Validation Accuracy: 0.8773062740053449\n",
      "Mean Training Loss: 0.2002831588366202\n",
      "Mean Validation Loss: 0.3188035179461752\n",
      "===================================\n",
      "Test Loss: 0.3027\n",
      "Test Accuracy: 0.8793\n"
     ]
    }
   ],
   "source": [
    "print('===================================')\n",
    "print(\"Mean Training Accuracy:\", np.mean(history.history['accuracy']))\n",
    "print(\"Mean Validation Accuracy:\", np.mean(history.history['val_accuracy']))\n",
    "print(\"Mean Training Loss:\", np.mean(history.history['loss']))\n",
    "print(\"Mean Validation Loss:\", np.mean(history.history['val_loss']))\n",
    "\n",
    "print('===================================')\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blosum. lets also try a LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (2708, 183, 23)\n",
      "test_x (1334, 183, 23)\n",
      "Epoch 1/100\n",
      "34/34 [==============================] - 1s 17ms/step - loss: 0.8122 - accuracy: 0.5876 - val_loss: 0.4076 - val_accuracy: 0.8321\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 1s 16ms/step - loss: 0.3883 - accuracy: 0.8401 - val_loss: 0.3076 - val_accuracy: 0.8782\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.3059 - accuracy: 0.8699 - val_loss: 0.2794 - val_accuracy: 0.8782\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.2868 - accuracy: 0.8831 - val_loss: 0.2536 - val_accuracy: 0.8948\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.2458 - accuracy: 0.9021 - val_loss: 0.2488 - val_accuracy: 0.8930\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2454 - accuracy: 0.9115 - val_loss: 0.2515 - val_accuracy: 0.8948\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.2109 - accuracy: 0.9231 - val_loss: 0.2533 - val_accuracy: 0.8967\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.2156 - accuracy: 0.9204 - val_loss: 0.2736 - val_accuracy: 0.8856\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 1s 16ms/step - loss: 0.1920 - accuracy: 0.9269 - val_loss: 0.2452 - val_accuracy: 0.9004\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1728 - accuracy: 0.9335 - val_loss: 0.2509 - val_accuracy: 0.8930\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.1691 - accuracy: 0.9326 - val_loss: 0.2713 - val_accuracy: 0.8985\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.1500 - accuracy: 0.9426 - val_loss: 0.2638 - val_accuracy: 0.8967\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.1414 - accuracy: 0.9480 - val_loss: 0.2624 - val_accuracy: 0.8967\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.1016 - accuracy: 0.9605 - val_loss: 0.3004 - val_accuracy: 0.8819\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.1125 - accuracy: 0.9624 - val_loss: 0.2802 - val_accuracy: 0.9022\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.0877 - accuracy: 0.9718 - val_loss: 0.2810 - val_accuracy: 0.8967\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.0751 - accuracy: 0.9789 - val_loss: 0.3068 - val_accuracy: 0.9004\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.0564 - accuracy: 0.9817 - val_loss: 0.3071 - val_accuracy: 0.9077\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.0605 - accuracy: 0.9786 - val_loss: 0.3043 - val_accuracy: 0.9004\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.0606 - accuracy: 0.9778 - val_loss: 0.3166 - val_accuracy: 0.8985\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 1s 16ms/step - loss: 0.0511 - accuracy: 0.9827 - val_loss: 0.3473 - val_accuracy: 0.8967\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.0366 - accuracy: 0.9875 - val_loss: 0.3498 - val_accuracy: 0.8967\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 1s 16ms/step - loss: 0.0327 - accuracy: 0.9903 - val_loss: 0.4383 - val_accuracy: 0.8838\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.0368 - accuracy: 0.9853 - val_loss: 0.3830 - val_accuracy: 0.9004\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.0195 - accuracy: 0.9955 - val_loss: 0.3941 - val_accuracy: 0.9041\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.0216 - accuracy: 0.9923 - val_loss: 0.4321 - val_accuracy: 0.9041\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.0199 - accuracy: 0.9954 - val_loss: 0.3560 - val_accuracy: 0.9022\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.0221 - accuracy: 0.9941 - val_loss: 0.4211 - val_accuracy: 0.9022\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.0139 - accuracy: 0.9970 - val_loss: 0.4381 - val_accuracy: 0.9133\n",
      "42/42 [==============================] - 0s 4ms/step - loss: 0.2737 - accuracy: 0.9033\n",
      "===================================\n",
      "Mean Training Accuracy: 0.9430859397197592\n",
      "Mean Validation Accuracy: 0.894134116583857\n",
      "Mean Training Loss: 0.1441328083004417\n",
      "Mean Validation Loss: 0.31810459340440816\n",
      "===================================\n",
      "Test Loss: 0.2737\n",
      "Test Accuracy: 0.9033\n"
     ]
    }
   ],
   "source": [
    "#open \n",
    "enconde_df = Encoding(dataset= df ,  col= 'padded_sequence')\n",
    "\n",
    "blosum = enconde_df.get_blosum()\n",
    "\n",
    "expanded_arrays =  blosum['blosum'].apply(lambda x: np.array(x))\n",
    "X = np.array(expanded_arrays.tolist())\n",
    "\n",
    "y = blosum['label']\n",
    "\n",
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "print('train_x', X_train.shape)\n",
    "print('test_x', x_test.shape)\n",
    "\n",
    "\n",
    "# the input shape needs to be different\n",
    "model = Sequential([\n",
    "    Input(shape=(183, 23)),\n",
    "    LSTM(units=64, return_sequences=True),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train same model\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=64,epochs = 100,validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "\n",
    "print('===================================')\n",
    "print(\"Mean Training Accuracy:\", np.mean(history.history['accuracy']))\n",
    "print(\"Mean Validation Accuracy:\", np.mean(history.history['val_accuracy']))\n",
    "print(\"Mean Training Loss:\", np.mean(history.history['loss']))\n",
    "print(\"Mean Validation Loss:\", np.mean(history.history['val_loss']))\n",
    "\n",
    "print('===================================')\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (2708, 183, 18)\n",
      "test_x (1334, 183, 18)\n",
      "Epoch 1/100\n",
      "34/34 [==============================] - 1s 16ms/step - loss: 0.6980 - accuracy: 0.4945 - val_loss: 0.6890 - val_accuracy: 0.5148\n",
      "Epoch 2/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.6762 - accuracy: 0.5715 - val_loss: 0.5586 - val_accuracy: 0.7399\n",
      "Epoch 3/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.5452 - accuracy: 0.7329 - val_loss: 0.4900 - val_accuracy: 0.8100\n",
      "Epoch 4/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.4994 - accuracy: 0.7694 - val_loss: 0.4026 - val_accuracy: 0.8210\n",
      "Epoch 5/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.4143 - accuracy: 0.8222 - val_loss: 0.4278 - val_accuracy: 0.8063\n",
      "Epoch 6/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.4258 - accuracy: 0.8096 - val_loss: 0.3530 - val_accuracy: 0.8432\n",
      "Epoch 7/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.4156 - accuracy: 0.8184 - val_loss: 0.3464 - val_accuracy: 0.8506\n",
      "Epoch 8/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.3672 - accuracy: 0.8383 - val_loss: 0.3315 - val_accuracy: 0.8598\n",
      "Epoch 9/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.3241 - accuracy: 0.8628 - val_loss: 0.3179 - val_accuracy: 0.8579\n",
      "Epoch 10/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.3187 - accuracy: 0.8801 - val_loss: 0.3098 - val_accuracy: 0.8653\n",
      "Epoch 11/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.2962 - accuracy: 0.8858 - val_loss: 0.3009 - val_accuracy: 0.8653\n",
      "Epoch 12/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.3231 - accuracy: 0.8684 - val_loss: 0.2961 - val_accuracy: 0.8653\n",
      "Epoch 13/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.3016 - accuracy: 0.8701 - val_loss: 0.3006 - val_accuracy: 0.8782\n",
      "Epoch 14/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.2972 - accuracy: 0.8766 - val_loss: 0.2880 - val_accuracy: 0.8727\n",
      "Epoch 15/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.3028 - accuracy: 0.8794 - val_loss: 0.2879 - val_accuracy: 0.8764\n",
      "Epoch 16/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2713 - accuracy: 0.8962 - val_loss: 0.2920 - val_accuracy: 0.8838\n",
      "Epoch 17/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2737 - accuracy: 0.8799 - val_loss: 0.2893 - val_accuracy: 0.8801\n",
      "Epoch 18/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.2655 - accuracy: 0.8884 - val_loss: 0.3064 - val_accuracy: 0.8708\n",
      "Epoch 19/100\n",
      "34/34 [==============================] - 0s 11ms/step - loss: 0.2923 - accuracy: 0.8741 - val_loss: 0.2769 - val_accuracy: 0.8801\n",
      "Epoch 20/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2552 - accuracy: 0.9057 - val_loss: 0.2807 - val_accuracy: 0.8764\n",
      "Epoch 21/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2853 - accuracy: 0.8710 - val_loss: 0.2921 - val_accuracy: 0.8801\n",
      "Epoch 22/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.2508 - accuracy: 0.8974 - val_loss: 0.2698 - val_accuracy: 0.8801\n",
      "Epoch 23/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2445 - accuracy: 0.9022 - val_loss: 0.2841 - val_accuracy: 0.8875\n",
      "Epoch 24/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.2419 - accuracy: 0.8955 - val_loss: 0.2963 - val_accuracy: 0.8727\n",
      "Epoch 25/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2438 - accuracy: 0.8963 - val_loss: 0.3099 - val_accuracy: 0.8745\n",
      "Epoch 26/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.2460 - accuracy: 0.9004 - val_loss: 0.2773 - val_accuracy: 0.8875\n",
      "Epoch 27/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2287 - accuracy: 0.9122 - val_loss: 0.2816 - val_accuracy: 0.8801\n",
      "Epoch 28/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2462 - accuracy: 0.8961 - val_loss: 0.2744 - val_accuracy: 0.8948\n",
      "Epoch 29/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2002 - accuracy: 0.9183 - val_loss: 0.3159 - val_accuracy: 0.8690\n",
      "Epoch 30/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2426 - accuracy: 0.9037 - val_loss: 0.3126 - val_accuracy: 0.8745\n",
      "Epoch 31/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2143 - accuracy: 0.9104 - val_loss: 0.3973 - val_accuracy: 0.8413\n",
      "Epoch 32/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.2578 - accuracy: 0.9013 - val_loss: 0.2673 - val_accuracy: 0.8967\n",
      "Epoch 33/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.2038 - accuracy: 0.9187 - val_loss: 0.3076 - val_accuracy: 0.8653\n",
      "Epoch 34/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.2027 - accuracy: 0.9155 - val_loss: 0.2763 - val_accuracy: 0.8930\n",
      "Epoch 35/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2081 - accuracy: 0.9111 - val_loss: 0.2960 - val_accuracy: 0.8948\n",
      "Epoch 36/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2068 - accuracy: 0.9171 - val_loss: 0.2831 - val_accuracy: 0.8948\n",
      "Epoch 37/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.2016 - accuracy: 0.9212 - val_loss: 0.2870 - val_accuracy: 0.8819\n",
      "Epoch 38/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1975 - accuracy: 0.9180 - val_loss: 0.3178 - val_accuracy: 0.8764\n",
      "Epoch 39/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.2074 - accuracy: 0.9217 - val_loss: 0.2785 - val_accuracy: 0.8967\n",
      "Epoch 40/100\n",
      "34/34 [==============================] - 0s 10ms/step - loss: 0.2095 - accuracy: 0.9139 - val_loss: 0.2951 - val_accuracy: 0.8948\n",
      "Epoch 41/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1829 - accuracy: 0.9274 - val_loss: 0.3437 - val_accuracy: 0.8653\n",
      "Epoch 42/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.1980 - accuracy: 0.9241 - val_loss: 0.3028 - val_accuracy: 0.8875\n",
      "Epoch 43/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.1676 - accuracy: 0.9289 - val_loss: 0.2985 - val_accuracy: 0.8819\n",
      "Epoch 44/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1840 - accuracy: 0.9174 - val_loss: 0.2935 - val_accuracy: 0.8875\n",
      "Epoch 45/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.1857 - accuracy: 0.9213 - val_loss: 0.3152 - val_accuracy: 0.8911\n",
      "Epoch 46/100\n",
      "34/34 [==============================] - 0s 13ms/step - loss: 0.1659 - accuracy: 0.9341 - val_loss: 0.2884 - val_accuracy: 0.8856\n",
      "Epoch 47/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.1782 - accuracy: 0.9311 - val_loss: 0.3095 - val_accuracy: 0.8893\n",
      "Epoch 48/100\n",
      "34/34 [==============================] - 0s 12ms/step - loss: 0.1587 - accuracy: 0.9351 - val_loss: 0.3058 - val_accuracy: 0.8856\n",
      "Epoch 49/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1582 - accuracy: 0.9334 - val_loss: 0.3037 - val_accuracy: 0.8819\n",
      "Epoch 50/100\n",
      "34/34 [==============================] - 0s 15ms/step - loss: 0.1536 - accuracy: 0.9417 - val_loss: 0.3095 - val_accuracy: 0.8764\n",
      "Epoch 51/100\n",
      "34/34 [==============================] - 0s 14ms/step - loss: 0.1705 - accuracy: 0.9337 - val_loss: 0.3163 - val_accuracy: 0.8856\n",
      "Epoch 52/100\n",
      "34/34 [==============================] - 1s 15ms/step - loss: 0.1536 - accuracy: 0.9343 - val_loss: 0.3157 - val_accuracy: 0.8911\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.3199 - accuracy: 0.8778\n",
      "===================================\n",
      "Mean Training Accuracy: 0.8813924995752481\n",
      "Mean Validation Accuracy: 0.8646749911399988\n",
      "Mean Training Loss: 0.2707398980855942\n",
      "Mean Validation Loss: 0.32246709958865094\n",
      "===================================\n",
      "Test Loss: 0.3199\n",
      "Test Accuracy: 0.8778\n"
     ]
    }
   ],
   "source": [
    "#open \n",
    "enconde_df = Encoding(dataset= df ,  col= 'padded_sequence')\n",
    "\n",
    "nlf = enconde_df.get_nlf()\n",
    "\n",
    "expanded_arrays =  nlf['nlf'].apply(lambda x: np.array(x))\n",
    "X = np.array(expanded_arrays.tolist())\n",
    "\n",
    "y = nlf['label']\n",
    "\n",
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "print('train_x', X_train.shape)\n",
    "print('test_x', x_test.shape)\n",
    "\n",
    "\n",
    "# the input shape needs to be different\n",
    "model = Sequential([\n",
    "    Input(shape=(183, 18)),\n",
    "    LSTM(units=32, return_sequences=True),\n",
    "    Flatten(),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train same model\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, batch_size=64,epochs = 100,validation_split=0.2, callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "\n",
    "print('===================================')\n",
    "print(\"Mean Training Accuracy:\", np.mean(history.history['accuracy']))\n",
    "print(\"Mean Validation Accuracy:\", np.mean(history.history['val_accuracy']))\n",
    "print(\"Mean Training Loss:\", np.mean(history.history['loss']))\n",
    "print(\"Mean Validation Loss:\", np.mean(history.history['val_loss']))\n",
    "\n",
    "print('===================================')\n",
    "\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models can also be optimized to search for a better combination of parameters.\n",
    "Different architectures and encoding schemes are possible. \n",
    "One should explore optimization algorithms. \n",
    "\n",
    "\n",
    "Below is an example using Propythia. \n",
    "We will also use the same Veltri use in the paper describing the data. The modle uses an embedding layer followed by CONV and LSTM. As the model receives an embedding layer one should give the sequences as integers. Here we define a function that assignes a int to eachAminoacid and also do padding using tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x (2708, 183)\n",
      "test_x (1334, 183)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def pad_sequence(sequences):\n",
    "#     sequences = df['seq'].tolist()\n",
    "    alphabet = \"XARNDCEQGHILKMFPSTWYV\"\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    # {'X': 0,\n",
    "    #  'A': 1,\n",
    "    #  'R': 2,\n",
    "    #  'N': 3,\n",
    "    #  'D': 4,...\n",
    "    sequences_integer_ecoded = []\n",
    "    for seq in sequences:\n",
    "        integer_encoded = [char_to_int[char] for char in seq]\n",
    "        sequences_integer_ecoded.append(integer_encoded)\n",
    "    fps_x = pad_sequences(sequences_integer_ecoded, maxlen=183, padding='pre', value=0.0)   \n",
    "    return fps_x\n",
    "\n",
    "\n",
    "\n",
    "X = pad_sequence(df['sequence'])\n",
    "y = res['label']\n",
    "\n",
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "print('train_x', X_train.shape)\n",
    "print('test_x', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def veltri_model(units=100):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(183,)))\n",
    "    model.add(Embedding(input_dim=21, output_dim=128, input_length=200, mask_zero=True))\n",
    "    model.add(Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=16,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=5, strides=1, padding='same'))\n",
    "    model.add(LSTM(units=units,\n",
    "                   dropout=0.1,\n",
    "                   unroll=True,\n",
    "                   return_sequences=False,\n",
    "                   stateful=False))\n",
    "\n",
    "    # Add Classification Dense, Compile model and make it ready for optimization\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "39/39 [==============================] - 23s 158ms/step - loss: 0.5812 - accuracy: 0.6909 - val_loss: 0.3938 - val_accuracy: 0.8303\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39377, saving model to weights.hdf5\n",
      "Epoch 2/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.3310 - accuracy: 0.8576 - val_loss: 0.3966 - val_accuracy: 0.8192\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.39377\n",
      "Epoch 3/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.3115 - accuracy: 0.8717 - val_loss: 0.3599 - val_accuracy: 0.8524\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.39377 to 0.35992, saving model to weights.hdf5\n",
      "Epoch 4/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2673 - accuracy: 0.8980 - val_loss: 0.3348 - val_accuracy: 0.8708\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35992 to 0.33479, saving model to weights.hdf5\n",
      "Epoch 5/100\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.2523 - accuracy: 0.9003 - val_loss: 0.3130 - val_accuracy: 0.8782\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.33479 to 0.31303, saving model to weights.hdf5\n",
      "Epoch 6/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.2197 - accuracy: 0.9165 - val_loss: 0.2870 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.31303 to 0.28701, saving model to weights.hdf5\n",
      "Epoch 7/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1726 - accuracy: 0.9390 - val_loss: 0.3683 - val_accuracy: 0.8672\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.28701\n",
      "Epoch 8/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1505 - accuracy: 0.9474 - val_loss: 0.2953 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.28701\n",
      "Epoch 9/100\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.1369 - accuracy: 0.9530 - val_loss: 0.2863 - val_accuracy: 0.9041\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.28701 to 0.28632, saving model to weights.hdf5\n",
      "Epoch 10/100\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.1162 - accuracy: 0.9587 - val_loss: 0.3068 - val_accuracy: 0.8967\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.28632\n",
      "Epoch 11/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.1044 - accuracy: 0.9664 - val_loss: 0.3064 - val_accuracy: 0.8967\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.28632\n",
      "Epoch 12/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0780 - accuracy: 0.9770 - val_loss: 0.3484 - val_accuracy: 0.9041\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.28632\n",
      "Epoch 13/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0492 - accuracy: 0.9853 - val_loss: 0.4118 - val_accuracy: 0.8672\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.28632\n",
      "Epoch 14/100\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0415 - accuracy: 0.9887 - val_loss: 0.3653 - val_accuracy: 0.8782\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.28632\n",
      "Epoch 15/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0675 - accuracy: 0.9841 - val_loss: 0.3601 - val_accuracy: 0.8893\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.28632\n",
      "Epoch 16/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0564 - accuracy: 0.9818 - val_loss: 0.4445 - val_accuracy: 0.8745\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.28632\n",
      "Epoch 17/100\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0637 - accuracy: 0.9784 - val_loss: 0.4938 - val_accuracy: 0.8745\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.28632\n",
      "Epoch 18/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0356 - accuracy: 0.9889 - val_loss: 0.3552 - val_accuracy: 0.8893\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.28632\n",
      "Epoch 19/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0364 - accuracy: 0.9919 - val_loss: 0.4114 - val_accuracy: 0.9041\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.28632\n",
      "Epoch 20/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0268 - accuracy: 0.9929 - val_loss: 0.4855 - val_accuracy: 0.8819\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.28632\n",
      "Epoch 21/100\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0326 - accuracy: 0.9912 - val_loss: 0.5005 - val_accuracy: 0.8745\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.28632\n",
      "Epoch 22/100\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0392 - accuracy: 0.9871 - val_loss: 0.4322 - val_accuracy: 0.8672\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.28632\n",
      "Epoch 23/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0323 - accuracy: 0.9928 - val_loss: 0.4502 - val_accuracy: 0.8672\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.28632\n",
      "Epoch 24/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0201 - accuracy: 0.9953 - val_loss: 0.4765 - val_accuracy: 0.8856\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.28632\n",
      "Epoch 25/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0119 - accuracy: 0.9972 - val_loss: 0.5639 - val_accuracy: 0.8819\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.28632\n",
      "Epoch 26/100\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.5836 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.28632\n",
      "Epoch 27/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0033 - accuracy: 0.9998 - val_loss: 0.6826 - val_accuracy: 0.8672\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.28632\n",
      "Epoch 28/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.6549 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.28632\n",
      "Epoch 29/100\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0506 - accuracy: 0.9860 - val_loss: 0.6034 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.28632\n",
      "Epoch 30/100\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0276 - accuracy: 0.9903 - val_loss: 0.5788 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.28632\n",
      "Epoch 31/100\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0099 - accuracy: 0.9981 - val_loss: 0.6011 - val_accuracy: 0.8672\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.28632\n",
      "Epoch 32/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0060 - accuracy: 0.9991 - val_loss: 0.6295 - val_accuracy: 0.8672\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.28632\n",
      "Epoch 33/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0048 - accuracy: 0.9995 - val_loss: 0.6214 - val_accuracy: 0.8672\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.28632\n",
      "Epoch 34/100\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0046 - accuracy: 0.9996 - val_loss: 0.6275 - val_accuracy: 0.8598\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.28632\n",
      "Epoch 35/100\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.6474 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.28632\n",
      "Epoch 36/100\n",
      "39/39 [==============================] - 1s 32ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.6464 - val_accuracy: 0.8745\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.28632\n",
      "Epoch 37/100\n",
      "39/39 [==============================] - 1s 34ms/step - loss: 0.0092 - accuracy: 0.9972 - val_loss: 0.6387 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.28632\n",
      "Epoch 38/100\n",
      "39/39 [==============================] - 1s 33ms/step - loss: 0.0028 - accuracy: 0.9999 - val_loss: 0.6659 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.28632\n",
      "Epoch 39/100\n",
      "39/39 [==============================] - 1s 35ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.6817 - val_accuracy: 0.8635\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.28632\n",
      "Epoch 00039: early stopping\n",
      "('loss mean: ', 0.08614781110857923) \n",
      " ('val_loss mean: ', 0.47719119985898334) \n",
      " ('accuracy mean: ', 0.9681091751807775) \n",
      " ('val_accuracy mean: ', 0.8731194994388483)\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 183, 128)          2688      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 183, 64)           131136    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 183, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 199,925\n",
      "Trainable params: 199,925\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from propythia.ml.deep_ml import DeepML\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "dl=DeepML(X_train, y_train, x_test, y_test, number_classes=2, problem_type='binary',\n",
    "          x_dval=None, y_dval=None, epochs=100, batch_size=64,\n",
    "          path='', report_name=None, verbose=1,\n",
    "         early_stopping_patience=30, reduce_lr_patience=20, reduce_lr_factor=0.2, reduce_lr_min=0.00001,\n",
    "                 )\n",
    "\n",
    "model = KerasClassifier(build_fn= veltri_model)\n",
    "\n",
    "# run the model in Propythia\n",
    "history = dl.run_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': 0.8920539730134932,\n",
       " 'MCC': 0.7841114710106724,\n",
       " 'log_loss': 0.5132134758672886,\n",
       " 'f1 score': 0.891891891891892,\n",
       " 'roc_auc': 0.8920539730134934,\n",
       " 'Precision': array([0.5       , 0.89323308, 1.        ]),\n",
       " 'Recall': array([1.        , 0.89055472, 0.        ]),\n",
       " 'fdr': 0.10676691729323308,\n",
       " 'sn': 0.8905547226386806,\n",
       " 'sp': 0.8935532233883059}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate on propythia\n",
    "scores, report, cm, cm2 = dl.score_testset_classification()\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets demonstrate how a Cross validation model would run in ProPythia. Here the X and Y are the total X and Y (not splitted). The model and everything defined above in ml class is the same. We will define CV as 3 just for speed purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ID                                          sequence  label\n",
      "0  AP02484                             GMASKAGSVLGKITKIALGAL      1\n",
      "1  AP02630       NIGLFTSTCFSSQCFSSKCFTDTCFSSNCFTGRHQCGYTHGSC      1\n",
      "2  AP01427                    GAIKDALKGAAKTVAVELLKKAQCKLEKTC      1\n",
      "3  AP02983                             FFGRLKAVFRGARQGWKEHRY      1\n",
      "4  AP01815  DFGCARGMIFVCMRRCARMYPGSTGYCQGFRCMCDTMIPIRRPPFIMG      1\n",
      "train_x (2708, 183)\n",
      "test_x (1334, 183)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'/home/martinha/propythia/propythia/src/propythia/')\n",
    "sys.path.append(r'/home/martinha/propythia/propythia/src/')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "\n",
    "from propythia.protein.sequence import ReadSequence\n",
    "from propythia.protein.encoding import Encoding\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D,Conv2D, Flatten, MaxPool1D,MaxPool2D, Dropout, Input,GRU\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint(filepath='best_model.h5', save_best_only=True, monitor='val_accuracy', mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "amps_file = './AMP_Scan2_Feb2020_Dataset/AMPS_02182020.fasta'\n",
    "non_amps_file = './AMP_Scan2_Feb2020_Dataset/DECOYS_02182020.fasta'\n",
    "\n",
    "\n",
    "sequences = SeqIO.parse(amps_file, \"fasta\")\n",
    "data = []\n",
    "\n",
    "for record in sequences:\n",
    "    data.append([record.id, str(record.seq), 1])\n",
    "\n",
    "sequences = SeqIO.parse(non_amps_file, \"fasta\")\n",
    "for record in sequences:\n",
    "    data.append([record.id, str(record.seq), 0])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"ID\", \"sequence\", 'label'])\n",
    "print(df.head())\n",
    "\n",
    "read_seqs = ReadSequence()\n",
    "res = read_seqs.par_preprocessing(dataset= df, col = 'sequence', B ='N', Z = 'Q', U = 'C', O = 'K', J = 'I', X = '')\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def pad_sequence(sequences):\n",
    "#     sequences = df['seq'].tolist()\n",
    "    alphabet = \"XARNDCEQGHILKMFPSTWYV\"\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    # {'X': 0,\n",
    "    #  'A': 1,\n",
    "    #  'R': 2,\n",
    "    #  'N': 3,\n",
    "    #  'D': 4,...\n",
    "    sequences_integer_ecoded = []\n",
    "    for seq in sequences:\n",
    "        integer_encoded = [char_to_int[char] for char in seq]\n",
    "        sequences_integer_ecoded.append(integer_encoded)\n",
    "    fps_x = pad_sequences(sequences_integer_ecoded, maxlen=183, padding='pre', value=0.0)   \n",
    "    return fps_x\n",
    "\n",
    "\n",
    "\n",
    "X = pad_sequence(df['sequence'])\n",
    "y = res['label']\n",
    "\n",
    "X_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "print('train_x', X_train.shape)\n",
    "print('test_x', x_test.shape)\n",
    "\n",
    "def veltri_model(units=100):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(183,)))\n",
    "    model.add(Embedding(input_dim=21, output_dim=128, input_length=200, mask_zero=True))\n",
    "    model.add(Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=16,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        activation='relu'))\n",
    "    model.add(MaxPool1D(pool_size=5, strides=1, padding='same'))\n",
    "    model.add(LSTM(units=units,\n",
    "                   dropout=0.1,\n",
    "                   unroll=True,\n",
    "                   return_sequences=False,\n",
    "                   stateful=False))\n",
    "\n",
    "    # Add Classification Dense, Compile model and make it ready for optimization\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold  0\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 21s 136ms/step - loss: 0.5734 - accuracy: 0.6774 - val_loss: 0.4412 - val_accuracy: 0.8148\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.44123, saving model to weights.hdf5\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.3120 - accuracy: 0.8749 - val_loss: 0.2679 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.44123 to 0.26787, saving model to weights.hdf5\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.3136 - accuracy: 0.8709 - val_loss: 0.3674 - val_accuracy: 0.8704\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.26787\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.2378 - accuracy: 0.9066 - val_loss: 0.2885 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.26787\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.2192 - accuracy: 0.9127 - val_loss: 0.3597 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.26787\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.1858 - accuracy: 0.9277 - val_loss: 0.3885 - val_accuracy: 0.8642\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.26787\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1452 - accuracy: 0.9502 - val_loss: 0.4600 - val_accuracy: 0.8395\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.26787\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.1161 - accuracy: 0.9598 - val_loss: 0.2786 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.26787\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1030 - accuracy: 0.9640 - val_loss: 0.4069 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.26787\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0895 - accuracy: 0.9693 - val_loss: 0.2825 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.26787\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0730 - accuracy: 0.9753 - val_loss: 0.4434 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.26787\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0569 - accuracy: 0.9804 - val_loss: 0.3529 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.26787\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0774 - accuracy: 0.9723 - val_loss: 0.4269 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.26787\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0413 - accuracy: 0.9870 - val_loss: 0.1162 - val_accuracy: 0.9599\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.26787 to 0.11624, saving model to weights.hdf5\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0621 - accuracy: 0.9825 - val_loss: 0.2243 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.11624\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0398 - accuracy: 0.9884 - val_loss: 0.6802 - val_accuracy: 0.8426\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.11624\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0166 - accuracy: 0.9973 - val_loss: 0.5071 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.11624\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0293 - accuracy: 0.9913 - val_loss: 0.2781 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.11624\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0260 - accuracy: 0.9937 - val_loss: 0.4237 - val_accuracy: 0.8673\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.11624\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0519 - accuracy: 0.9821 - val_loss: 0.8187 - val_accuracy: 0.7932\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.11624\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0362 - accuracy: 0.9890 - val_loss: 0.5540 - val_accuracy: 0.8549\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.11624\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0179 - accuracy: 0.9954 - val_loss: 0.3422 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.11624\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0181 - accuracy: 0.9938 - val_loss: 0.6851 - val_accuracy: 0.8704\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.11624\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.7475 - val_accuracy: 0.8704\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.11624\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0049 - accuracy: 0.9976 - val_loss: 0.6617 - val_accuracy: 0.8796\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.11624\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0074 - accuracy: 0.9978 - val_loss: 1.2049 - val_accuracy: 0.7284\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.11624\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0240 - accuracy: 0.9907 - val_loss: 0.6192 - val_accuracy: 0.8642\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.11624\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0087 - accuracy: 0.9982 - val_loss: 0.5213 - val_accuracy: 0.8951\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.11624\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.6936 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.11624\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0051 - accuracy: 0.9983 - val_loss: 0.4869 - val_accuracy: 0.9012\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.11624\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0074 - accuracy: 0.9988 - val_loss: 0.3484 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.11624\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0086 - accuracy: 0.9969 - val_loss: 0.5773 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.11624\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0046 - accuracy: 0.9983 - val_loss: 0.2908 - val_accuracy: 0.9259\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.11624\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0312 - accuracy: 0.9880 - val_loss: 0.7050 - val_accuracy: 0.8611\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.11624\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.6234 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.11624\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.6166 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.11624\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 0.6999 - val_accuracy: 0.8642\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.11624\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.6312 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.11624\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0014 - accuracy: 0.9996 - val_loss: 0.5794 - val_accuracy: 0.8920\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.11624\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6089 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.11624\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7003 - val_accuracy: 0.8796\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.11624\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.7367 - val_accuracy: 0.8735\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.11624\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 8.9853e-04 - accuracy: 0.9995 - val_loss: 0.6998 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.11624\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0010 - accuracy: 0.9995 - val_loss: 0.6577 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.11624\n",
      "Epoch 00044: early stopping\n",
      "\n",
      "Fold  1\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 18s 73ms/step - loss: 0.5640 - accuracy: 0.6747 - val_loss: 0.3185 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.11624\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.3581 - accuracy: 0.8539 - val_loss: 0.3187 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.11624\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.3765 - accuracy: 0.8462 - val_loss: 0.2854 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.11624\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.2958 - accuracy: 0.8797 - val_loss: 0.3649 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.11624\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.2643 - accuracy: 0.8940 - val_loss: 0.3511 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.11624\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.2409 - accuracy: 0.9035 - val_loss: 0.2348 - val_accuracy: 0.9259\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.11624\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.2321 - accuracy: 0.9102 - val_loss: 0.2625 - val_accuracy: 0.9259\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.11624\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1906 - accuracy: 0.9337 - val_loss: 0.3338 - val_accuracy: 0.8951\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.11624\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.1576 - accuracy: 0.9451 - val_loss: 0.1925 - val_accuracy: 0.9414\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.11624\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1737 - accuracy: 0.9376 - val_loss: 0.3661 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.11624\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.1224 - accuracy: 0.9581 - val_loss: 0.2889 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.11624\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0917 - accuracy: 0.9702 - val_loss: 0.2523 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.11624\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0615 - accuracy: 0.9864 - val_loss: 0.2034 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.11624\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.1295 - accuracy: 0.9542 - val_loss: 0.4044 - val_accuracy: 0.8549\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.11624\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0616 - accuracy: 0.9833 - val_loss: 0.0935 - val_accuracy: 0.9722\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.11624 to 0.09354, saving model to weights.hdf5\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0483 - accuracy: 0.9862 - val_loss: 0.0952 - val_accuracy: 0.9753\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.09354\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1027 - accuracy: 0.9668 - val_loss: 0.2566 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09354\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0247 - accuracy: 0.9919 - val_loss: 0.4158 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.09354\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0194 - accuracy: 0.9938 - val_loss: 0.5555 - val_accuracy: 0.8611\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09354\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0172 - accuracy: 0.9941 - val_loss: 0.2266 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09354\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0157 - accuracy: 0.9963 - val_loss: 0.2228 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.09354\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0198 - accuracy: 0.9935 - val_loss: 0.3228 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.09354\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0542 - accuracy: 0.9819 - val_loss: 0.3783 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.09354\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0211 - accuracy: 0.9927 - val_loss: 0.2783 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.09354\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0083 - accuracy: 0.9983 - val_loss: 0.5468 - val_accuracy: 0.8457\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.09354\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0049 - accuracy: 0.9996 - val_loss: 0.4769 - val_accuracy: 0.8796\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.09354\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0061 - accuracy: 0.9979 - val_loss: 0.3783 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.09354\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.5169 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.09354\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0160 - accuracy: 0.9947 - val_loss: 0.8371 - val_accuracy: 0.8025\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.09354\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0142 - accuracy: 0.9942 - val_loss: 0.1814 - val_accuracy: 0.9630\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.09354\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0107 - accuracy: 0.9972 - val_loss: 0.2623 - val_accuracy: 0.9475\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.09354\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0234 - accuracy: 0.9900 - val_loss: 0.5378 - val_accuracy: 0.8642\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09354\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0102 - accuracy: 0.9975 - val_loss: 0.5897 - val_accuracy: 0.8611\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09354\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0134 - accuracy: 0.9956 - val_loss: 0.3361 - val_accuracy: 0.9259\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09354\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0182 - accuracy: 0.9959 - val_loss: 0.3119 - val_accuracy: 0.9352\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09354\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0099 - accuracy: 0.9950 - val_loss: 0.3894 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09354\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0042 - accuracy: 0.9992 - val_loss: 0.5109 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09354\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.4696 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09354\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.4106 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09354\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.4551 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09354\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0027 - accuracy: 0.9985 - val_loss: 0.4343 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09354\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0028 - accuracy: 0.9988 - val_loss: 0.6763 - val_accuracy: 0.8704\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09354\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.5517 - val_accuracy: 0.8951\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09354\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0032 - accuracy: 0.9978 - val_loss: 0.6006 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09354\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5025 - val_accuracy: 0.9012\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09354\n",
      "Epoch 00045: early stopping\n",
      "\n",
      "Fold  2\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 21s 142ms/step - loss: 0.5446 - accuracy: 0.6785 - val_loss: 1.2108 - val_accuracy: 0.6235\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.09354\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.4127 - accuracy: 0.8567 - val_loss: 0.2753 - val_accuracy: 0.8951\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.09354\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.2841 - accuracy: 0.8940 - val_loss: 0.2658 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.09354\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.2691 - accuracy: 0.8967 - val_loss: 0.3212 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.09354\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.2460 - accuracy: 0.9110 - val_loss: 0.2643 - val_accuracy: 0.9043\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.09354\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1926 - accuracy: 0.9286 - val_loss: 0.2028 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.09354\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1605 - accuracy: 0.9428 - val_loss: 0.2166 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09354\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1738 - accuracy: 0.9337 - val_loss: 0.5391 - val_accuracy: 0.8210\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09354\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1352 - accuracy: 0.9544 - val_loss: 0.2377 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09354\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0854 - accuracy: 0.9744 - val_loss: 0.3367 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.09354\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0712 - accuracy: 0.9775 - val_loss: 0.1946 - val_accuracy: 0.9444\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.09354\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1138 - accuracy: 0.9587 - val_loss: 0.2927 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.09354\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0606 - accuracy: 0.9806 - val_loss: 0.3357 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09354\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0559 - accuracy: 0.9819 - val_loss: 0.2825 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09354\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0376 - accuracy: 0.9874 - val_loss: 0.1590 - val_accuracy: 0.9537\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.09354\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0624 - accuracy: 0.9835 - val_loss: 0.4517 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.09354\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0213 - accuracy: 0.9935 - val_loss: 0.6493 - val_accuracy: 0.8302\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09354\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0434 - accuracy: 0.9840 - val_loss: 0.3850 - val_accuracy: 0.8796\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.09354\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0248 - accuracy: 0.9919 - val_loss: 0.3735 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09354\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0332 - accuracy: 0.9880 - val_loss: 0.6143 - val_accuracy: 0.8241\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09354\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0410 - accuracy: 0.9863 - val_loss: 0.4699 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.09354\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0116 - accuracy: 0.9962 - val_loss: 0.6603 - val_accuracy: 0.8549\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.09354\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0055 - accuracy: 0.9986 - val_loss: 0.6771 - val_accuracy: 0.8673\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.09354\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0140 - accuracy: 0.9974 - val_loss: 0.4739 - val_accuracy: 0.8951\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.09354\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0053 - accuracy: 0.9990 - val_loss: 0.2717 - val_accuracy: 0.9383\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.09354\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0115 - accuracy: 0.9964 - val_loss: 0.7691 - val_accuracy: 0.8735\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.09354\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0125 - accuracy: 0.9941 - val_loss: 0.5002 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.09354\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.3213 - val_accuracy: 0.9352\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.09354\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0091 - accuracy: 0.9965 - val_loss: 0.2863 - val_accuracy: 0.9506\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.09354\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0048 - accuracy: 0.9993 - val_loss: 0.2907 - val_accuracy: 0.9414\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.09354\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.4323 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.09354\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0084 - accuracy: 0.9978 - val_loss: 0.1215 - val_accuracy: 0.9815\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09354\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1703 - accuracy: 0.9544 - val_loss: 0.4894 - val_accuracy: 0.8735\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09354\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0165 - accuracy: 0.9968 - val_loss: 0.4683 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09354\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0076 - accuracy: 0.9983 - val_loss: 0.7493 - val_accuracy: 0.8457\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09354\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0088 - accuracy: 0.9978 - val_loss: 0.6327 - val_accuracy: 0.8796\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09354\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0027 - accuracy: 0.9996 - val_loss: 0.5217 - val_accuracy: 0.8981\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09354\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.5623 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09354\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0054 - accuracy: 0.9994 - val_loss: 0.6462 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09354\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.7865 - val_accuracy: 0.8735\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09354\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0105 - accuracy: 0.9973 - val_loss: 1.0103 - val_accuracy: 0.7901\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09354\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0174 - accuracy: 0.9917 - val_loss: 0.5775 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09354\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0073 - accuracy: 0.9980 - val_loss: 0.6514 - val_accuracy: 0.8673\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09354\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0061 - accuracy: 0.9973 - val_loss: 0.6967 - val_accuracy: 0.8519\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09354\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0081 - accuracy: 0.9978 - val_loss: 0.7568 - val_accuracy: 0.8395\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09354\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 2s 37ms/step - loss: 0.0087 - accuracy: 0.9966 - val_loss: 0.7341 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09354\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0210 - accuracy: 0.9941 - val_loss: 0.5367 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09354\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0219 - accuracy: 0.9922 - val_loss: 0.2939 - val_accuracy: 0.9259\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09354\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0111 - accuracy: 0.9955 - val_loss: 0.6171 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09354\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.4995 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09354\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.5449 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.09354\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0035 - accuracy: 0.9987 - val_loss: 0.4596 - val_accuracy: 0.9012\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.09354\n",
      "Epoch 53/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.5694 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.09354\n",
      "Epoch 54/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6177 - val_accuracy: 0.8796\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.09354\n",
      "Epoch 55/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.5405 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.09354\n",
      "Epoch 56/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 8.4335e-04 - accuracy: 1.0000 - val_loss: 0.6078 - val_accuracy: 0.8796\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.09354\n",
      "Epoch 57/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0014 - accuracy: 0.9993 - val_loss: 0.5451 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.09354\n",
      "Epoch 58/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 7.9984e-04 - accuracy: 1.0000 - val_loss: 0.6003 - val_accuracy: 0.8827\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.09354\n",
      "Epoch 59/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0012 - accuracy: 0.9996 - val_loss: 0.4818 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.09354\n",
      "Epoch 60/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 9.5476e-04 - accuracy: 0.9997 - val_loss: 0.6648 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.09354\n",
      "Epoch 61/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.4533 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.09354\n",
      "Epoch 62/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.5907 - val_accuracy: 0.8920\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.09354\n",
      "Epoch 00062: early stopping\n",
      "\n",
      "Fold  3\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 18s 73ms/step - loss: 0.5772 - accuracy: 0.6623 - val_loss: 0.3429 - val_accuracy: 0.8920\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.09354\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.3053 - accuracy: 0.8759 - val_loss: 0.3133 - val_accuracy: 0.8920\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.09354\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.2633 - accuracy: 0.8923 - val_loss: 0.3314 - val_accuracy: 0.9012\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.09354\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.2434 - accuracy: 0.9023 - val_loss: 0.2515 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.09354\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.2037 - accuracy: 0.9319 - val_loss: 0.2937 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.09354\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1960 - accuracy: 0.9266 - val_loss: 0.3037 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.09354\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1698 - accuracy: 0.9384 - val_loss: 0.4164 - val_accuracy: 0.8642\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09354\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.2320 - accuracy: 0.9178 - val_loss: 0.3384 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09354\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1539 - accuracy: 0.9512 - val_loss: 0.1557 - val_accuracy: 0.9568\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09354\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.1391 - accuracy: 0.9531 - val_loss: 0.5384 - val_accuracy: 0.8086\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.09354\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1279 - accuracy: 0.9528 - val_loss: 0.3617 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.09354\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0640 - accuracy: 0.9799 - val_loss: 0.2323 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.09354\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0760 - accuracy: 0.9796 - val_loss: 0.4037 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09354\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0371 - accuracy: 0.9886 - val_loss: 0.3237 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09354\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0401 - accuracy: 0.9869 - val_loss: 0.5850 - val_accuracy: 0.8488\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.09354\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0375 - accuracy: 0.9892 - val_loss: 0.3511 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.09354\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0389 - accuracy: 0.9866 - val_loss: 0.4549 - val_accuracy: 0.8951\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09354\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0334 - accuracy: 0.9888 - val_loss: 0.6752 - val_accuracy: 0.8241\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.09354\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0361 - accuracy: 0.9915 - val_loss: 0.4051 - val_accuracy: 0.9012\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09354\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0151 - accuracy: 0.9958 - val_loss: 0.3753 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09354\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0166 - accuracy: 0.9968 - val_loss: 0.6971 - val_accuracy: 0.7932\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.09354\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0429 - accuracy: 0.9840 - val_loss: 0.5028 - val_accuracy: 0.8735\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.09354\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0235 - accuracy: 0.9947 - val_loss: 0.3545 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.09354\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.4484 - accuracy: 0.8877 - val_loss: 0.4059 - val_accuracy: 0.8333\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.09354\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.3062 - accuracy: 0.8666 - val_loss: 0.4756 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.09354\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.1581 - accuracy: 0.9477 - val_loss: 0.7097 - val_accuracy: 0.6636\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.09354\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.2418 - accuracy: 0.9163 - val_loss: 0.3950 - val_accuracy: 0.8735\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.09354\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1313 - accuracy: 0.9560 - val_loss: 0.2884 - val_accuracy: 0.8951\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.09354\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0880 - accuracy: 0.9735 - val_loss: 0.3929 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.09354\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0617 - accuracy: 0.9821 - val_loss: 0.3559 - val_accuracy: 0.9012\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.09354\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0633 - accuracy: 0.9826 - val_loss: 0.5268 - val_accuracy: 0.8549\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.09354\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0434 - accuracy: 0.9867 - val_loss: 0.4510 - val_accuracy: 0.8765\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09354\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0404 - accuracy: 0.9898 - val_loss: 0.3965 - val_accuracy: 0.8951\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09354\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0383 - accuracy: 0.9877 - val_loss: 0.4366 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09354\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0292 - accuracy: 0.9922 - val_loss: 0.4455 - val_accuracy: 0.8858\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09354\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0330 - accuracy: 0.9923 - val_loss: 0.6070 - val_accuracy: 0.8457\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09354\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0315 - accuracy: 0.9908 - val_loss: 1.7290 - val_accuracy: 0.6481\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09354\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.2382 - accuracy: 0.9279 - val_loss: 0.3992 - val_accuracy: 0.8735\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09354\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0574 - accuracy: 0.9788 - val_loss: 0.3275 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09354\n",
      "Epoch 00039: early stopping\n",
      "\n",
      "Fold  4\n",
      "Epoch 1/100\n",
      "46/46 [==============================] - 18s 72ms/step - loss: 0.7396 - accuracy: 0.6495 - val_loss: 0.3871 - val_accuracy: 0.8704\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.09354\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.3892 - accuracy: 0.8392 - val_loss: 0.2646 - val_accuracy: 0.9043\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.09354\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.3303 - accuracy: 0.8635 - val_loss: 0.3340 - val_accuracy: 0.8673\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.09354\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.2592 - accuracy: 0.8983 - val_loss: 0.2556 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.09354\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.2278 - accuracy: 0.9161 - val_loss: 0.2105 - val_accuracy: 0.9352\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.09354\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1965 - accuracy: 0.9196 - val_loss: 0.4763 - val_accuracy: 0.8241\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.09354\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1819 - accuracy: 0.9312 - val_loss: 0.3589 - val_accuracy: 0.8673\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09354\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.1644 - accuracy: 0.9394 - val_loss: 0.2168 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09354\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1090 - accuracy: 0.9647 - val_loss: 0.2627 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09354\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1454 - accuracy: 0.9445 - val_loss: 0.1732 - val_accuracy: 0.9506\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.09354\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0947 - accuracy: 0.9727 - val_loss: 0.3556 - val_accuracy: 0.8920\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.09354\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0675 - accuracy: 0.9743 - val_loss: 0.5238 - val_accuracy: 0.8395\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.09354\n",
      "Epoch 13/100\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 0.0668 - accuracy: 0.9775 - val_loss: 0.2518 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.09354\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0760 - accuracy: 0.9746 - val_loss: 0.2483 - val_accuracy: 0.9383\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.09354\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0290 - accuracy: 0.9925 - val_loss: 0.1954 - val_accuracy: 0.9444\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.09354\n",
      "Epoch 16/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0419 - accuracy: 0.9862 - val_loss: 0.2498 - val_accuracy: 0.9290\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.09354\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0360 - accuracy: 0.9922 - val_loss: 0.1827 - val_accuracy: 0.9537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00017: val_loss did not improve from 0.09354\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0270 - accuracy: 0.9919 - val_loss: 0.1855 - val_accuracy: 0.9444\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.09354\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0240 - accuracy: 0.9946 - val_loss: 0.2993 - val_accuracy: 0.9259\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.09354\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0100 - accuracy: 0.9983 - val_loss: 0.3146 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.09354\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0074 - accuracy: 0.9988 - val_loss: 0.3013 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.09354\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.1334 - accuracy: 0.9539 - val_loss: 0.1327 - val_accuracy: 0.9568\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.09354\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0767 - accuracy: 0.9727 - val_loss: 0.3008 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.09354\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 1s 32ms/step - loss: 0.0369 - accuracy: 0.9916 - val_loss: 0.2517 - val_accuracy: 0.9228\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.09354\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0129 - accuracy: 0.9985 - val_loss: 0.3704 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.09354\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0151 - accuracy: 0.9958 - val_loss: 0.2098 - val_accuracy: 0.9352\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.09354\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0505 - accuracy: 0.9837 - val_loss: 0.2446 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.09354\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0168 - accuracy: 0.9980 - val_loss: 0.2863 - val_accuracy: 0.9259\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.09354\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0233 - accuracy: 0.9937 - val_loss: 0.3860 - val_accuracy: 0.9012\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.09354\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0349 - accuracy: 0.9870 - val_loss: 0.3188 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.09354\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0177 - accuracy: 0.9940 - val_loss: 0.2610 - val_accuracy: 0.9321\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.09354\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0134 - accuracy: 0.9965 - val_loss: 0.3801 - val_accuracy: 0.9136\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.09354\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0071 - accuracy: 0.9989 - val_loss: 0.3550 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.09354\n",
      "Epoch 34/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0052 - accuracy: 0.9987 - val_loss: 0.2588 - val_accuracy: 0.9352\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.09354\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.2287 - val_accuracy: 0.9414\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.09354\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0036 - accuracy: 0.9996 - val_loss: 0.2234 - val_accuracy: 0.9444\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.09354\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0251 - accuracy: 0.9921 - val_loss: 0.3099 - val_accuracy: 0.9043\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.09354\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.3529 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.09354\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0097 - accuracy: 0.9961 - val_loss: 0.5778 - val_accuracy: 0.8673\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.09354\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.2896 - val_accuracy: 0.9352\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.09354\n",
      "Epoch 41/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.4310 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.09354\n",
      "Epoch 42/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 0.0043 - accuracy: 0.9995 - val_loss: 0.4163 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.09354\n",
      "Epoch 43/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.4645 - val_accuracy: 0.9012\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.09354\n",
      "Epoch 44/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.3333 - val_accuracy: 0.9167\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.09354\n",
      "Epoch 45/100\n",
      "46/46 [==============================] - 2s 36ms/step - loss: 0.0033 - accuracy: 0.9995 - val_loss: 0.4254 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.09354\n",
      "Epoch 46/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.4074 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.09354\n",
      "Epoch 47/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4640 - val_accuracy: 0.9074\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.09354\n",
      "Epoch 48/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.4393 - val_accuracy: 0.9043\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.09354\n",
      "Epoch 49/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 0.0017 - accuracy: 0.9992 - val_loss: 0.3466 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.09354\n",
      "Epoch 50/100\n",
      "46/46 [==============================] - 2s 34ms/step - loss: 6.4645e-04 - accuracy: 1.0000 - val_loss: 0.3676 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.09354\n",
      "Epoch 51/100\n",
      "46/46 [==============================] - 2s 33ms/step - loss: 6.0647e-04 - accuracy: 1.0000 - val_loss: 0.4279 - val_accuracy: 0.9105\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.09354\n",
      "Epoch 52/100\n",
      "46/46 [==============================] - 2s 35ms/step - loss: 6.4978e-04 - accuracy: 1.0000 - val_loss: 0.4778 - val_accuracy: 0.9043\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.09354\n",
      "Epoch 00052: early stopping\n",
      "Finished train_model_cv in 491.6328 secs\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>MCC</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>f1 score</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>fdr</th>\n",
       "      <th>sn</th>\n",
       "      <th>sp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.906057</td>\n",
       "      <td>0.812113</td>\n",
       "      <td>0.531961</td>\n",
       "      <td>0.906173</td>\n",
       "      <td>0.906057</td>\n",
       "      <td>[0.5006180469715699, 0.9061728395061729, 1.0]</td>\n",
       "      <td>[1.0, 0.9061728395061729, 0.0]</td>\n",
       "      <td>0.093827</td>\n",
       "      <td>0.906173</td>\n",
       "      <td>0.905941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.908529</td>\n",
       "      <td>0.818281</td>\n",
       "      <td>0.531906</td>\n",
       "      <td>0.910843</td>\n",
       "      <td>0.908563</td>\n",
       "      <td>[0.49938195302843014, 0.8873239436619719, 1.0]</td>\n",
       "      <td>[1.0, 0.9356435643564357, 0.0]</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.935644</td>\n",
       "      <td>0.881481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.902228</td>\n",
       "      <td>0.804458</td>\n",
       "      <td>0.571435</td>\n",
       "      <td>0.902107</td>\n",
       "      <td>0.902228</td>\n",
       "      <td>[0.5, 0.9032258064516129, 1.0]</td>\n",
       "      <td>[1.0, 0.900990099009901, 0.0]</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.900990</td>\n",
       "      <td>0.903465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.893564</td>\n",
       "      <td>0.787602</td>\n",
       "      <td>0.321176</td>\n",
       "      <td>0.891688</td>\n",
       "      <td>0.893564</td>\n",
       "      <td>[0.5, 0.9076923076923077, 1.0]</td>\n",
       "      <td>[1.0, 0.8762376237623762, 0.0]</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.876238</td>\n",
       "      <td>0.910891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.891089</td>\n",
       "      <td>0.783562</td>\n",
       "      <td>0.585931</td>\n",
       "      <td>0.894231</td>\n",
       "      <td>0.891089</td>\n",
       "      <td>[0.5, 0.8691588785046729, 1.0]</td>\n",
       "      <td>[1.0, 0.9207920792079208, 0.0]</td>\n",
       "      <td>0.130841</td>\n",
       "      <td>0.920792</td>\n",
       "      <td>0.861386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.900293</td>\n",
       "      <td>0.801203</td>\n",
       "      <td>0.508482</td>\n",
       "      <td>0.901008</td>\n",
       "      <td>0.900300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.105285</td>\n",
       "      <td>0.907967</td>\n",
       "      <td>0.892633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.006853</td>\n",
       "      <td>0.013546</td>\n",
       "      <td>0.096068</td>\n",
       "      <td>0.007175</td>\n",
       "      <td>0.006861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014696</td>\n",
       "      <td>0.019946</td>\n",
       "      <td>0.018593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Accuracy       MCC  log_loss  f1 score   roc_auc  \\\n",
       "0     0.906057  0.812113  0.531961  0.906173  0.906057   \n",
       "1     0.908529  0.818281  0.531906  0.910843  0.908563   \n",
       "2     0.902228  0.804458  0.571435  0.902107  0.902228   \n",
       "3     0.893564  0.787602  0.321176  0.891688  0.893564   \n",
       "4     0.891089  0.783562  0.585931  0.894231  0.891089   \n",
       "mean  0.900293  0.801203  0.508482  0.901008  0.900300   \n",
       "std   0.006853  0.013546  0.096068  0.007175  0.006861   \n",
       "\n",
       "                                           Precision  \\\n",
       "0      [0.5006180469715699, 0.9061728395061729, 1.0]   \n",
       "1     [0.49938195302843014, 0.8873239436619719, 1.0]   \n",
       "2                     [0.5, 0.9032258064516129, 1.0]   \n",
       "3                     [0.5, 0.9076923076923077, 1.0]   \n",
       "4                     [0.5, 0.8691588785046729, 1.0]   \n",
       "mean                                             NaN   \n",
       "std                                              NaN   \n",
       "\n",
       "                              Recall       fdr        sn        sp  \n",
       "0     [1.0, 0.9061728395061729, 0.0]  0.093827  0.906173  0.905941  \n",
       "1     [1.0, 0.9356435643564357, 0.0]  0.112676  0.935644  0.881481  \n",
       "2      [1.0, 0.900990099009901, 0.0]  0.096774  0.900990  0.903465  \n",
       "3     [1.0, 0.8762376237623762, 0.0]  0.092308  0.876238  0.910891  \n",
       "4     [1.0, 0.9207920792079208, 0.0]  0.130841  0.920792  0.861386  \n",
       "mean                             NaN  0.105285  0.907967  0.892633  \n",
       "std                              NaN  0.014696  0.019946  0.018593  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try a CV model\n",
    "from propythia.ml.deep_ml import DeepML\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "dl=DeepML(X, y, x_test = None, y_test = None, number_classes=2, problem_type='binary',\n",
    "          x_dval=None, y_dval=None, epochs=100, batch_size=64,\n",
    "          path='', report_name=None, verbose=1,\n",
    "         early_stopping_patience=30, reduce_lr_patience=20, reduce_lr_factor=0.2, reduce_lr_min=0.00001,\n",
    "                 )\n",
    "\n",
    "model = KerasClassifier(build_fn= veltri_model)\n",
    "\n",
    "\n",
    "veltri_cv = dl.train_model_cv(x_cv = X, y_cv = y, cv=5, model=model)\n",
    "veltri_cv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Propythia",
   "language": "python",
   "name": "propythia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}