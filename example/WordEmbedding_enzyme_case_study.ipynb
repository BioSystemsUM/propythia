{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Get the directory path of the current script\n",
    "current_script_directory = os.path.dirname(os.path.abspath(__file__))\n",
    "# Construct the path to the src directory\n",
    "src_directory = os.path.join(current_script_directory, \"..\", \"src\")\n",
    "srcpro_directory = os.path.join(current_script_directory, \"..\", \"src/propythia\")\n",
    "\n",
    "# Add the src directory to sys.path\n",
    "sys.path.append(src_directory)\n",
    "sys.path.append(srcpro_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart building and using Word Embeddings with Propythia and application to ML and DL models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook intends to go over the building and application of Word embedding vectors to describe biological sequences and their use with ML and DL. The notebook uses protein sequences but the same principle may be used for DNA sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python module Bumblebee was developed for processing biological sequences aiming to search for semantic\n",
    "meaning in sequence ”words” (such as nucleotides and amino\n",
    "acids). This module was then integrated in ProPythia. It is organized in sub-modules so that\n",
    "the user can use them in different specific tasks and adapt\n",
    "them to fit the problem that is working on. The user can set\n",
    "specific values for the majority of the parameters, but default\n",
    "values are established. \n",
    "\n",
    "\n",
    "This include: \n",
    "\n",
    "    1) Read sequence sub-module: To read and/or change sequences. This is especially important to replace nonrelevant/not-common AAs simplifying the vocabulary.\n",
    "    \n",
    "    2) Sequence processing sub-module: To generate subsequences; Implements the segmentation of sequences by grams of size n and overlapping (or not) method.\n",
    "    \n",
    "    3) Create vocabulary list sub-module: To get all the vocabulary in the dataset, necessary to train the WE.\n",
    "    It allows to fetch a list of n-grams from pre-existing JSON file or create the list if it is not present.\n",
    "    \n",
    "    4) Training word embedding models sub-module: To train and save WE models; It is possible to train W2V\n",
    "    and FastText models with both CBOW or SG algorithms (based on gensim library). \n",
    "    \n",
    "    5) Load models list sub-module: To load a pre-trained embedding model;\n",
    "    \n",
    "    6) Protein Vector representation sub-module: To get a vector representation of a sequence or the matrix of\n",
    "    vectors accordingly to a model. It obtains a vector for a given n-gram and the number of occurrences of that\n",
    "    n-gram. Three methods of representing sequences as vectors are implemented as described above.\n",
    "    \n",
    "    7) Interpretability sub-module: To visualize WE in space and get similarities between vectors. It uses t-SNE to\n",
    "    create plots related to physicochemical properties of individual AA, including charge, volume, mass, Van der Waals Volume, polarity and hydrophobicity. For ngrams larger than 1, mean values of these properties are presented as described for Asgari et al. The sub-module also includes binding free energy values for trigrams, based on experimental data. If needed, users can define additional characteristics. Additionally, the models can also retrieve scores of similarity and neighborhood of the n-grams to aid in understanding vector similarities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important decisions to take are: \n",
    "\n",
    "    Either use a pretrained WE model or train your own model. \n",
    "    \n",
    "    Choose the size of the biological 'words' and the way to represent the final sequence\n",
    "    \n",
    "    \n",
    "For a more detailed explanation of the several modes please check the Quickstart_WordEmbedding jupyter.\n",
    "We will use the pretrained protvec model in this tutorial. But, as explained in the Quickstart_WordEmbedding jupyter one can train a model with different data and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets get the data. We will use an enzyme dataset as example.\n",
    "For simplification purposes we will just use the first level of EC number and delete enzymes that have more than one enzyme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169497, 9)\n",
      "2    51877\n",
      "3    32939\n",
      "0    22708\n",
      "1    18343\n",
      "6    16158\n",
      "4    12639\n",
      "5     8006\n",
      "7     6827\n",
      "Name: ec_1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./enzyme/datasets/ecpred_uniprot_uniref_90.csv')\n",
    "# drop entries without sequence\n",
    "data=data.dropna(subset=['sequence'])\n",
    "# drop entries with ! characters\n",
    "data =data[~data['sequence'].str.contains(\"!\")]\n",
    "\n",
    "# remove promiscue enzymes\n",
    "data = data[~data['ec_number'].str.contains(';')]\n",
    "#get first level\n",
    "data['ec_1'] = data['ec_number'].str.split('.').str[0]\n",
    "\n",
    "# just shuffling\n",
    "data=data.sample(frac=1)\n",
    "\n",
    "print(data.shape)\n",
    "# Count occurrences of specific values in the new column\n",
    "value_counts = data['ec_1'].value_counts()\n",
    "print(value_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Preprocess sequences\n",
    "After, we will replace not common aminoacids. Furthermore, protein sequences are of different length. Depending of the method of WE you are using, you may need to use the same length for all sequences. For comparison purposes, we are setting all sequences to a max length of 500. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_seq(seq, max_len):\n",
    "        seq1 = seq.replace('B', 'N')  # asparagine N / aspartic acid  D - asx - B\n",
    "        seq2 = seq1.replace('Z', 'Q')  # glutamine Q / glutamic acid  E - glx - Z\n",
    "        seq3 = seq2.replace('U',\n",
    "                            'C')  # selenocisteina, the closest is the cisteine. but it is a different aminoacid . take care.\n",
    "        seq4 = seq3.replace('O', 'K')  # Pyrrolysine to lysine\n",
    "        seq = seq4.replace('X', '')  # unknown character eliminated\n",
    "        if max_len:\n",
    "            seq = seq[0:max_len]\n",
    "        return seq\n",
    "\n",
    "\n",
    "seqs = data['sequence']\n",
    "max_len = 500\n",
    "seqs_new = list(map(lambda seq:transform_seq(seq, max_len),seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3 load the WE model\n",
    "Here we will use the Protvec model. This means that we will open the WordEmbedding class with a matrix file ( Protvec). The ngram len will be 3, used in Ptotvec and the vector dim is 100. \n",
    "\n",
    "protvec file can be obtained at https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JMFHTN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-15 10:34:45.778005: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordEmbedding is running..\n",
      "--MATRIX LOADED--\n"
     ]
    }
   ],
   "source": [
    "from propythia.wordembedding.word_embedding import WordEmbedding as wv\n",
    "\n",
    "protvec_file = '/home/martinha/propythia/propythia/src/propythia/wordembedding/protVec_100d_3grams.csv'\n",
    "\n",
    "w2v = wv(emb_matrix_file=protvec_file,\n",
    "         ngram_len=3 , sequence_max_len= max_len , vectordim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 get protein vectors representations\n",
    "With the WE model loaded now we will transform the sequences into vectors. \n",
    "Three methods can be used: \n",
    "\n",
    "    • Method 1: Substitute directly the n-grams presented in the sequence by the WE vector. Being K the dimension of the word and N the dimension of the WE vector, a sequence of size L will be represented by a final vector of\n",
    "    (L − k − 1) ∗ N elements. \n",
    "    This method preserves the spatial information of the location of biological words.\n",
    "    \n",
    "    • Method 2: k-mer word frequencies are calculated and multiplied by the corresponding WE vectors. A sequence,\n",
    "    independent of the size, will be represented by a matrix of dimensions Number of words ∗ N.\n",
    "    \n",
    "    • Method 3: All the vectors of Method 2 are summed to reproduce a single vector of dimension N.\n",
    "    \n",
    "The method choosed will also impact the model choice. Deep learning architectures such as LSTM make more sense with method 1, where the sequence order is maintained. Method 3, for simplicity is suitable for ML models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Protein representations with method 3 for ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method3: Each sequence will be represented by a vector of 100 dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169497, 100)\n"
     ]
    }
   ],
   "source": [
    "seqs_to_consider = seqs_new\n",
    "\n",
    "# Initialize an empty numpy array\n",
    "num_sequences = len(seqs_to_consider)  # Number of sequences\n",
    "result_array = np.zeros((num_sequences, 100))\n",
    "\n",
    "# Loop through the sequences and append vectors to the array\n",
    "for idx, i in enumerate(seqs_new):\n",
    "    vector = w2v.convert_seq2vec(method=3, sequence=i, padding = True)\n",
    "    result_array[idx] = vector\n",
    "\n",
    "print(result_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['ec_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WordEmbedding is running..\n",
      "--MATRIX LOADED--\n"
     ]
    }
   ],
   "source": [
    "from propythia.wordembedding.word_embedding import WordEmbedding as wv\n",
    "\n",
    "protvec_file = '/home/martinha/propythia/propythia/src/propythia/wordembedding/protVec_100d_3grams.csv'\n",
    "\n",
    "w2v = wv(emb_matrix_file=protvec_file,\n",
    "         ngram_len=3 , sequence_max_len= max_len , vectordim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169497, 100)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty numpy array\n",
    "num_sequences = len(seqs_to_consider)  # Number of sequences\n",
    "result_array = np.zeros((num_sequences, 100))\n",
    "\n",
    "# Loop through the sequences and append vectors to the array\n",
    "for idx, i in enumerate(seqs_to_consider ):\n",
    "    vector = w2v.convert_seq2vec(method=3, sequence=i, padding = True)\n",
    "    result_array[idx] = vector\n",
    "\n",
    "print(result_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting\n",
      "train_x (113562, 100)\n",
      "test_x (55935, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('splitting')\n",
    "X = result_array\n",
    "y = data['ec_1']\n",
    "\n",
    "df_x_train, df_x_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "print('train_x', df_x_train.shape)\n",
    "print('test_x', df_x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and open Shallow ML propythia class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'numpy.ndarray' object has no attribute 'columns'\n",
      "no features names listed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from propythia.ml.shallow_ml import ShallowML\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef\n",
    "\n",
    "# define ml class\n",
    "# create Machine learning object\n",
    "\n",
    "report = 'ml_svm_mth3_3ngram_100dim_protvec'\n",
    "ml = ShallowML(x_train=df_x_train, x_test=df_x_test, y_train=y_train, y_test=y_test,\n",
    "               report_name=report, columns_names= None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try a RF with no param grid ( will use one bby default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'numpy.ndarray' object has no attribute 'columns'\n",
      "no features names listed\n",
      "performing gridSearch...\n",
      "GridSearchCV took 1009.68 seconds for 6 candidate parameter settings.\n",
      "GridSearchCV(cv=3,\n",
      "             estimator=Pipeline(steps=[('scl', None),\n",
      "                                       ('clf',\n",
      "                                        RandomForestClassifier(random_state=1))]),\n",
      "             n_jobs=40,\n",
      "             param_grid=[{'clf__bootstrap': [True], 'clf__criterion': ['gini'],\n",
      "                          'clf__max_features': ['sqrt', 'log2'],\n",
      "                          'clf__n_estimators': [10, 100, 500]}],\n",
      "             scoring=make_scorer(matthews_corrcoef))\n",
      "Model with rank: 1\n",
      " Mean validation score: 0.531 (std: 0.002)\n",
      " Parameters: {'clf__bootstrap': True, 'clf__criterion': 'gini', 'clf__max_features': 'sqrt', 'clf__n_estimators': 500}\n",
      " \n",
      "\n",
      "Model with rank: 2\n",
      " Mean validation score: 0.524 (std: 0.002)\n",
      " Parameters: {'clf__bootstrap': True, 'clf__criterion': 'gini', 'clf__max_features': 'log2', 'clf__n_estimators': 500}\n",
      " \n",
      "\n",
      "Model with rank: 3\n",
      " Mean validation score: 0.521 (std: 0.002)\n",
      " Parameters: {'clf__bootstrap': True, 'clf__criterion': 'gini', 'clf__max_features': 'sqrt', 'clf__n_estimators': 100}\n",
      " \n",
      "\n",
      "make_scorer(matthews_corrcoef)\n",
      "3\n",
      "Best score (scorer: make_scorer(matthews_corrcoef)) and parameters from a 3-fold cross validation:\n",
      " MCC score:\t0.531\n",
      " Parameters:\t{'clf__bootstrap': True, 'clf__criterion': 'gini', 'clf__max_features': 'sqrt', 'clf__n_estimators': 500}\n",
      "\n",
      "0.417591 (0.001937) with: {'clf__bootstrap': True, 'clf__criterion': 'gini', 'clf__max_features': 'sqrt', 'clf__n_estimators': 10}\n",
      "0.521045 (0.002336) with: {'clf__bootstrap': True, 'clf__criterion': 'gini', 'clf__max_features': 'sqrt', 'clf__n_estimators': 100}\n",
      "0.531183 (0.001872) with: {'clf__bootstrap': True, 'clf__criterion': 'gini', 'clf__max_features': 'sqrt', 'clf__n_estimators': 500}\n",
      "0.408530 (0.001298) with: {'clf__bootstrap': True, 'clf__criterion': 'gini', 'clf__max_features': 'log2', 'clf__n_estimators': 10}\n",
      "0.511287 (0.004108) with: {'clf__bootstrap': True, 'clf__criterion': 'gini', 'clf__max_features': 'log2', 'clf__n_estimators': 100}\n",
      "0.524032 (0.002498) with: {'clf__bootstrap': True, 'clf__criterion': 'gini', 'clf__max_features': 'log2', 'clf__n_estimators': 500}\n",
      "   clf__bootstrap clf__criterion clf__max_features  clf__n_estimators  \\\n",
      "0            True           gini              sqrt                 10   \n",
      "1            True           gini              sqrt                100   \n",
      "2            True           gini              sqrt                500   \n",
      "3            True           gini              log2                 10   \n",
      "4            True           gini              log2                100   \n",
      "5            True           gini              log2                500   \n",
      "\n",
      "      means      stds  \n",
      "0  0.417591  0.001937  \n",
      "1  0.521045  0.002336  \n",
      "2  0.531183  0.001872  \n",
      "3  0.408530  0.001298  \n",
      "4  0.511287  0.004108  \n",
      "5  0.524032  0.002498  \n"
     ]
    }
   ],
   "source": [
    "report = 'ml_rf_mth3_3ngram_100dim_protvec'\n",
    "ml = ShallowML(x_train=df_x_train, x_test=df_x_test, y_train=y_train, y_test=y_test,\n",
    "               report_name=report, columns_names= None)\n",
    "\n",
    "\n",
    "\n",
    "# TRAIN BEST MODEL\n",
    "best_model = ml.train_best_model(model = 'rf', scaler=None,\n",
    "                                     score=make_scorer(matthews_corrcoef),\n",
    "                                     cv=3, optType='gridSearch',\n",
    "                                     param_grid=None,\n",
    "                                     n_jobs=40, random_state=1, refit=True)\n",
    "\n",
    "# scores, report, cm, cm2 = ml.score_testset(classifier=best_model)\n",
    "# print(report)\n",
    "# print(cm)\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.69      0.68      7494\n",
      "           1       0.88      0.33      0.49      6053\n",
      "           2       0.54      0.87      0.66     17120\n",
      "           3       0.66      0.56      0.61     10870\n",
      "           4       0.97      0.40      0.57      4171\n",
      "           5       0.98      0.26      0.41      2642\n",
      "           6       0.73      0.69      0.71      5332\n",
      "           7       0.95      0.63      0.76      2253\n",
      "\n",
      "    accuracy                           0.64     55935\n",
      "   macro avg       0.80      0.56      0.61     55935\n",
      "weighted avg       0.71      0.64      0.63     55935\n",
      "\n",
      "[[ 5186    22  1381   817     5     4    41    38]\n",
      " [  190  2024  2916   630    12     0   271    10]\n",
      " [ 1021    71 14883   756     8     1   359    21]\n",
      " [  922    56  3463  6137     6     2   277     7]\n",
      " [  121    46  1755   433  1660     4   152     0]\n",
      " [   83    33  1342   267    11   684   222     0]\n",
      " [   65    37  1383   170     0     0  3677     0]\n",
      " [   72     2   594   113     1     0    44  1427]]\n",
      "{'Accuracy': 0.6378475015643157, 'MCC': 0.5536812403909718, 'log_loss': 1.1222387160469567, 'f1 score weighted': 0.6251951265065739, 'f1 score macro': 0.6106382942845954, 'f1 score micro': 0.6378475015643157, 'roc_auc ovr': 0.9074540562357899, 'roc_auc ovo': 0.9181413053674828, 'precision': 0.7054971215268765, 'recall': 0.6378475015643157}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores, report, cm, cm2 = ml.score_testset(classifier=best_model)\n",
    "print(report)\n",
    "print(cm)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dizer que modelos dá para usar o propylixia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit learn instead of Propythia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7664658695164139\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(C=10,gamma=0.001,kernel='rbf')\n",
    "\n",
    "best_model = clf.fit(df_x_train, y_train)\n",
    "y_pred = clf.predict(df_x_test)\n",
    "score = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # SVC\n",
    "# param_grid = {'clf__C': [0.1, 1.0, 10],\n",
    "#                         'clf__kernel': ['rbf'],\n",
    "#                         'clf__gamma': [0.001,0.0001]}\n",
    "# # TRAIN BEST MODEL\n",
    "\n",
    "# # we will use as score the MCC\n",
    "# best_model = ml.train_best_model(model_name= None , model = 'svc', scaler=None,\n",
    "#                                      score=make_scorer(matthews_corrcoef),\n",
    "#                                      cv=3, optType='gridSearch',\n",
    "#                                      param_grid=param_grid,\n",
    "#                                      # podes ver as param grids em e podes subsitituir por uma mais pequena propythia/src/propythia/adjuv_functions/ml_deep/parameters_shallow.py\n",
    "#                                      n_jobs=40, random_state=1, n_iter=15, refit=True)\n",
    "\n",
    "# ##########################################\n",
    "# # scores = ml.cross_val_score_model(model_name = None,model='svm',\n",
    "# #                               score='accuracy',\n",
    "# #                               cv=3,\n",
    "# #                               n_jobs=10,\n",
    "# #                               random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "# scores, report, cm, cm2 = ml.score_testset(classifier=best_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SVC\n",
    "# param_grid = {'clf__C': [0.1, 1.0],\n",
    "#                         'clf__kernel': ['rbf'],\n",
    "#                         'clf__gamma': [0.001,0.0001]}\n",
    "# # TRAIN BEST MODEL\n",
    "# best_model = ml.train_best_model(model = 'svc', scaler=None,\n",
    "#                                      score=make_scorer(matthews_corrcoef),\n",
    "#                                      cv=3, optType='gridSearch',\n",
    "#                                      param_grid=param_grid,\n",
    "#                                      n_jobs=40, random_state=1, refit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run SVC algorithm. \n",
    "\n",
    "    We will run an hyperparameter search with a defined param grid. \n",
    "    We will use a cross validation with 3 folds (for simplicity). \n",
    "    Use MCC as the score for search for best model\n",
    "    Calculate test scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with method 1. Each sequence will be represented by a vector of 498 trigrams with 100 len, this is, 498 * 100 size. \n",
    "This vector can be then flat if necessary. \n",
    "We will use just 200 seuences to simplicity purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 498, 100)\n"
     ]
    }
   ],
   "source": [
    "seqs_to_consider = seqs_new[:200]\n",
    "\n",
    "# Initialize an empty numpy array\n",
    "num_sequences = len(seqs_to_consider)  # Number of sequences\n",
    "result_array = np.zeros((num_sequences, 498, 100))\n",
    "\n",
    "# Loop through the sequences and append vectors to the array\n",
    "for idx, i in enumerate(seqs_new[:200]):\n",
    "    vector = w2v.convert_seq2vec(method=1, sequence=i, padding = True)\n",
    "    result_array[idx] = vector\n",
    "\n",
    "print(result_array.shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}