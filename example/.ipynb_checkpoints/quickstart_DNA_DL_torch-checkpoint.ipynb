{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'/home/martinha/propythia/propythia/src/propythia/')\n",
    "sys.path.append(r'/home/martinha/propythia/propythia/src/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProPythia DNA Deep Learning module quick start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a notebook that explains how to perform every step of the developed Deep Learning modules. They include all the necessary steps to complete an entire Deep Learning pipeline. The steps are:\n",
    "\n",
    "- Data reading and validation\n",
    "- Encoders\n",
    "- DNA Descriptors\n",
    "- Data splitting\n",
    "- Model building and training\n",
    "- Hyperparameter tuning\n",
    "\n",
    "\n",
    "\n",
    "All the DNA notebooks will use the dataset from the tutorial linked to the manuscript, A Primer on Deep Learning in Genomics (Nature Genetics, 2018) by James Zou, Mikael Huss, Abubakar Abid, Pejman Mohammadi, Ali Torkamani & Amalio Telentil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data reading and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The machine learning pipeline uses the same module to read and validate the sequences.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module comprehends functions to read and to validate DNA sequences. First is necessary to create the object ReadDNA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from propythia.dna.sequence import ReadDNA\n",
    "reader = ReadDNA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to create sequence objects using a single DNA sequence, a *CSV* and a *FASTA* file. The single sequence is going to be validated (check if all letters belong to the DNA alphabet) and the output will be the sequence in upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACGTACGAGCATGCAT\n"
     ]
    }
   ],
   "source": [
    "data = reader.read_sequence(\"ACGTACGAGCATGCAT\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With *CSV* there must be at least a column named 'sequence' in the file. The labels may also be retrieved and validated if the user wants them, but he must specify the `with_label` parameter as **True** and the column with the labels must be named 'label'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sequence\n",
      "0  CCGAGGGCTATGGTTTGGAAGTTAGAACCCTGGGGCTTCTCGCGGA...\n",
      "1  GAGTTTATATGGCGCGAGCCTAGTGGTTTTTGTACTTGTTTGTCGC...\n",
      "2  GATCAGTAGGGAAACAAACAGAGGGCCCAGCCACATCTAGCAGGTA...\n",
      "3  GTCCACGACCGAACTCCCACCTTGACCGCAGAGGTACCACCAGAGC...\n",
      "4  GGCGACCGAACTCCAACTAGAACCTGCATAACTGGCCTGGGAGATA...\n",
      "(2000, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                                            sequence  label\n",
      "0  CCGAGGGCTATGGTTTGGAAGTTAGAACCCTGGGGCTTCTCGCGGA...      0\n",
      "1  GAGTTTATATGGCGCGAGCCTAGTGGTTTTTGTACTTGTTTGTCGC...      0\n",
      "2  GATCAGTAGGGAAACAAACAGAGGGCCCAGCCACATCTAGCAGGTA...      0\n",
      "3  GTCCACGACCGAACTCCCACCTTGACCGCAGAGGTACCACCAGAGC...      1\n",
      "4  GGCGACCGAACTCCAACTAGAACCTGCATAACTGGCCTGGGAGATA...      1\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "filename = \"./primer/dataset.csv\"\n",
    "data = reader.read_csv(filename, with_labels=False)\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "data = reader.read_csv(filename, with_labels=True)\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *FASTA* format is similar to the *CSV* format. It always reads the sequence, and the labels only if the user wants them. The *FASTA* format must be one of the following examples:\n",
    "\n",
    "```\n",
    ">sequence_id1\n",
    "ACTGACTGACTGACTGACTGACTGACTGACTGACTGACTG...\n",
    ">sequence_id2\n",
    "ACTGACTGACTGACTGACTGACTGACTGACTGACTGACTG...\n",
    "``` \n",
    "\n",
    "```\n",
    ">sequence_id1,label1\n",
    "ACTGACTGACTGACTGACTGACTGACTGACTGACTGACTG...\n",
    ">sequence_id2,label2\n",
    "ACTGACTGACTGACTGACTGACTGACTGACTGACTGACTG...\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sequence\n",
      "0  CCGAGGGCTATGGTTTGGAAGTTAGAACCCTGGGGCTTCTCGCGGA...\n",
      "1  GAGTTTATATGGCGCGAGCCTAGTGGTTTTTGTACTTGTTTGTCGC...\n",
      "2  GATCAGTAGGGAAACAAACAGAGGGCCCAGCCACATCTAGCAGGTA...\n",
      "3  GTCCACGACCGAACTCCCACCTTGACCGCAGAGGTACCACCAGAGC...\n",
      "4  GGCGACCGAACTCCAACTAGAACCTGCATAACTGGCCTGGGAGATA...\n",
      "(19, 1)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                                            sequence  label\n",
      "0  CCGAGGGCTATGGTTTGGAAGTTAGAACCCTGGGGCTTCTCGCGGA...      0\n",
      "1  GAGTTTATATGGCGCGAGCCTAGTGGTTTTTGTACTTGTTTGTCGC...      0\n",
      "2  GATCAGTAGGGAAACAAACAGAGGGCCCAGCCACATCTAGCAGGTA...      0\n",
      "3  GTCCACGACCGAACTCCCACCTTGACCGCAGAGGTACCACCAGAGC...      1\n",
      "4  GGCGACCGAACTCCAACTAGAACCTGCATAACTGGCCTGGGAGATA...      1\n",
      "(19, 2)\n"
     ]
    }
   ],
   "source": [
    "filename = \"./primer/example.fasta\"\n",
    "data = reader.read_fasta(filename, with_labels=False)\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "print(\"-\" * 100)\n",
    "\n",
    "data = reader.read_fasta(filename, with_labels=True)\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning models automatically extract features from the sequences, but it is necessary to build a representation of the sequences first due to the fact that models can't handle anything other than numerical values. Encoders are easily calculated and can serve as numerical representations of sequences, which can subsequently be used as model input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module comprehends functions to encode the DNA sequences. The encoding step is important because sequences need to be converted into a numerical value in order to create an input matrix for the model. The encoders that have been implemented are:\n",
    "\n",
    "- One-hot encoding\n",
    "- Chemical encoding\n",
    "- K-mer One-hot encoding\n",
    "\n",
    "Below there's an example for each of them.\n",
    "\n",
    "| Encoder             | Sequence | Encoded sequence                             |\n",
    "| ------------------- | -------- | -------------------------------------------- |\n",
    "| One-Hot             | ACGT     | [[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]] |\n",
    "| Chemical            | ACGT     | [[1,1,1], [0,1,0], [1,0,0], [0,0,1]]         |\n",
    "| K-mer One-Hot (k=2) | ACGT     | [[0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0]] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding is extensively used in deep learning models and is well suited for most models. It is a simple encoding that converts the DNA alphabet into a binary vector. \n",
    "\n",
    "- A -> [1,0,0,0]\n",
    "- C -> [0,1,0,0]\n",
    "- G -> [0,0,1,0]\n",
    "- T -> [0,0,0,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode a sequence, we need first to create the object DNAEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from propythia.dna.encoding import DNAEncoder\n",
    "encoder = DNAEncoder('ACGTACGAGCATGCAT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we only need to specify the encoder method (one-hot, chemical, k-mer one-hot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "encoded_sequence = encoder.one_hot_encode()\n",
    "print(encoded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Chemical encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chemical encoding is a more complex encoding that uses the chemical properties of the DNA alphabet. Each letter is assigned a chemical property and the chemical properties are combined to create a vector. In a nutshell, the chemical properties are:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Chemical property</th>\n",
    "      <th>Class</th>\n",
    "      <th>Nucleotides</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td rowspan=\"2\">Ring structure</td>\n",
    "      <td>Purine</td>\n",
    "      <td>A, G</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Pyrimidine</td>\n",
    "      <td>C, T</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"2\">Hydrogen bond</td>\n",
    "      <td>Weak</td>\n",
    "      <td>A, T</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Strong</td>\n",
    "      <td>C, G</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td rowspan=\"2\">Functional group</td>\n",
    "      <td>Amino</td>\n",
    "      <td>A, C</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Keto</td>\n",
    "      <td>G, T</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "If the letter is in the list of the first nucleotides, it is assigned the value 1 and if it is in the list of the second nucleotides, it is assigned the value 0. \n",
    "\n",
    "- A -> [1, 1, 1]\n",
    "- C -> [0, 0, 1]\n",
    "- G -> [1, 0, 0]\n",
    "- T -> [0, 1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder object is already created so we just need to specify the encoder method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 1 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 1 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [1 1 1]\n",
      " [0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_sequence = encoder.chemical_encode()\n",
    "print(encoded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. K-mer One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using one-hot encoding on DNA sequences solely preserves the positional information of each nucleotide. Recent investigations, however, have shown that including high-order dependencies among nucleotides may enhance the efficacy of DNA models. The K-mer One-hot encoding is a method that aims to overcome this problem.\n",
    "\n",
    "If k = 1,the encoder will create the same vector as the one-hot encoding.\n",
    "\n",
    "If k = 2, 16 dinucleotides will be created, and the encoder will create a vector with the following values:\n",
    "\n",
    "- AA = [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "- AC = [0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "- AG = [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "- ...\n",
    "- TT = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1]\n",
    "\n",
    "If k = 3, 64 trinucleotides will be created, and the encoder will create a vector with the following values:\n",
    "\n",
    "- AAA = [1,0,0,0,...,0,0,0,0]\n",
    "- AAC = [0,1,0,0,...,0,0,0,0]\n",
    "- ...\n",
    "- TTT = [0,0,0,0,...,0,0,0,1]\n",
    "\n",
    "The value of K can be any integer greater than 1 and less than or equal to the length of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0.]]\n"
     ]
    }
   ],
   "source": [
    "encoded_sequence = encoder.kmer_one_hot_encode(k=2)\n",
    "print(encoded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This module also allows the user to encode multiple sequences at once. The encoder can receive a column of a dataframe full of sequences and return an array of all encoded sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 1 0 0]\n",
      "  [0 0 1 0]\n",
      "  [1 0 0 0]\n",
      "  [0 1 0 0]\n",
      "  [0 0 1 0]\n",
      "  [1 0 0 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 1 0]\n",
      "  [0 1 0 0]\n",
      "  [1 0 0 0]\n",
      "  [0 0 0 1]]\n",
      "\n",
      " [[0 1 0 0]\n",
      "  [0 0 1 0]\n",
      "  [1 0 0 0]\n",
      "  [1 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 1]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 1]\n",
      "  [1 0 0 0]\n",
      "  [0 1 0 0]]\n",
      "\n",
      " [[1 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 1]\n",
      "  [1 0 0 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 1 0]\n",
      "  [0 0 0 1]\n",
      "  [1 0 0 0]\n",
      "  [1 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    [\n",
    "        ['CGACGATGCAT', 1], \n",
    "        ['CGAAGGTGTAC', 0], \n",
    "        ['AGTAGGGGTAA', 1]\n",
    "    ], \n",
    "    columns=['sequence', 'labels']\n",
    ")\n",
    "\n",
    "column = df['sequence'].values\n",
    "encoder = DNAEncoder(column)\n",
    "encoded_sequences = encoder.one_hot_encode()\n",
    "print(encoded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DNA Descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the `quick-start-DL.ipynb` notebook, descriptors are manually calculated and are an attempt to serve as features for the classification model. However, deep learning models cannot use descriptors as features because their purpose is to extract features on their own instead of manually calculating beforehand. The DNA descriptors are being mentioned here because there are some deep learning models that can use them as features, such as deep neural networks, but models like CNNs and RNNs are not able to use them as features.\n",
    "\n",
    "So, at this point, the user can either choose to use encoders or descriptors to proceed to the next step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using descriptors it would be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 2000\n",
      "100 / 2000\n",
      "200 / 2000\n",
      "300 / 2000\n",
      "400 / 2000\n",
      "500 / 2000\n",
      "600 / 2000\n",
      "700 / 2000\n",
      "800 / 2000\n",
      "900 / 2000\n",
      "1000 / 2000\n",
      "1100 / 2000\n",
      "1200 / 2000\n",
      "1300 / 2000\n",
      "1400 / 2000\n",
      "1500 / 2000\n",
      "1600 / 2000\n",
      "1700 / 2000\n",
      "1800 / 2000\n",
      "1900 / 2000\n",
      "Done!\n",
      "(2000, 247)\n"
     ]
    }
   ],
   "source": [
    "reader = ReadDNA()\n",
    "data = reader.read_csv(filename='./primer/dataset.csv', with_labels=True)\n",
    "\n",
    "from propythia.dna.calculate_features import calculate_and_normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "fps_x, fps_y = calculate_and_normalize(data)\n",
    "\n",
    "scaler = StandardScaler().fit(fps_x)\n",
    "fps_x = scaler.transform(fps_x)\n",
    "fps_y = fps_y.to_numpy()\n",
    "print(fps_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using encodings it would be something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 50, 4)\n"
     ]
    }
   ],
   "source": [
    "reader = ReadDNA()\n",
    "data = reader.read_csv(filename='./primer/dataset.csv', with_labels=True)\n",
    "\n",
    "fps_x = data['sequence'].values\n",
    "fps_y = data['label'].values\n",
    "\n",
    "# choosing one hot encoding\n",
    "encoder = DNAEncoder(fps_x)\n",
    "fps_x = encoder.one_hot_encode()\n",
    "print(fps_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequences are at this point converted into numerical representations and are ready to be split into training, validation, and test sets. After that, each set needs also to be represented as the *PyTorch* object called *DataLoader*, which is a *Python* iterable over a dataset. All of this can be achieved using the function `data_splitting` from the `prepare_data.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from propythia.ml.torch.prepare_data import data_splitting\n",
    "batch_size = 32\n",
    "train_size = 0.6\n",
    "validation_size = 0.2\n",
    "test_size = 0.2\n",
    "\n",
    "trainloader, testloader, validloader, _ = data_splitting(fps_x, fps_y, batch_size, train_size, test_size, validation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model building and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note:** Before continuing, it is worth noting that all of the previous steps, from the data reading, calculation of encoder/descriptors, and even the data splitting step, were compiled into a single function called `prepare_data` that can be called from the `prepare_data.py` file. An example of how to use this function will be shown later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the data is now ready to be used by a model. The user can choose to use one of the 6 implemented *PyTorch* models. They are:\n",
    "\n",
    "| Models                | Features    |\n",
    "| --------------------- | ----------- |\n",
    "| MLP                   | Descriptors |\n",
    "| CNN                   | Encoders    |\n",
    "| LSTM / BiLSTM         | Encoders    |\n",
    "| GRU / BiGRU           | Encoders    |\n",
    "| CNN-LSTM / CNN-BiLSTM | Encoders    |\n",
    "| CNN-GRU / CNN-BiGRU   | Encoders    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some models require the use of encoders and some require descriptors. Also, some models have the bidirectional option, resulting in 2 + 4*2 = 10 different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagining the scenario that we want to use descriptors as features, we need to choose the *MLP* model. We also need to specify some parameters for the training function. To make it easier for the user, a config file was created to provide an overview of all the parameters that will be used from now on. An example of a `config.json` file is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import torch\n",
    "\n",
    "numpy.random.seed(2022)\n",
    "torch.manual_seed(2022)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1,2,3,4,5'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"combination\":{\n",
    "        \"model_label\": \"mlp\",\n",
    "        \"mode\": \"descriptor\",\n",
    "        \"data_dir\": \"primer\"\n",
    "    },\n",
    "    \"do_tuning\": false,\n",
    "    \"fixed_vals\":{\n",
    "        \"epochs\": 500,\n",
    "        \"optimizer_label\": \"adam\",\n",
    "        \"loss_function\": \"cross_entropy\",\n",
    "        \"patience\": 8,\n",
    "        \"output_size\": 2,\n",
    "        \"cpus_per_trial\":1, \n",
    "        \"gpus_per_trial\":0,\n",
    "        \"num_samples\": 15,\n",
    "        \"num_layers\": 2,\n",
    "        \"kmer_one_hot\": 3\n",
    "    },\n",
    "    \"hyperparameters\": {\n",
    "        \"hidden_size\": 32,\n",
    "        \"lr\": 1e-3,\n",
    "        \"batch_size\": 32,\n",
    "        \"dropout\": 0.35\n",
    "    },\n",
    "    \"hyperparameter_search_space\": {\n",
    "        \"hidden_size\": [32, 64, 128, 256],\n",
    "        \"lr\": [1e-5, 1e-2],\n",
    "        \"batch_size\": [8, 16, 32],\n",
    "        \"dropout\": [0.3, 0.5]\n",
    "    },\n",
    "    \"train_all_combinations\": false\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the values from the configuraton file, we can use the function `read_config` from the `deep_ml.py` file. This functions also validates the configuration file and returns a dictionary with the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combination {\n",
      "\t model_label : bi_lstm\n",
      "\t mode : one_hot\n",
      "\t data_dir : /home/martinha/propythia/propythia/example/primer\n",
      "\t class_weights : [1.0, 1.0]\n",
      "}\n",
      "do_tuning : False\n",
      "fixed_vals {\n",
      "\t epochs : 500\n",
      "\t optimizer_label : adam\n",
      "\t loss_function : CrossEntropyLoss()\n",
      "\t patience : 3\n",
      "\t output_size : 2\n",
      "\t cpus_per_trial : 2\n",
      "\t gpus_per_trial : 2\n",
      "\t num_samples : 5\n",
      "\t kmer_one_hot : 1\n",
      "}\n",
      "hyperparameters {\n",
      "\t hidden_size : 32\n",
      "\t lr : 0.001\n",
      "\t batch_size : 32\n",
      "\t dropout : 0.35\n",
      "\t num_layers : 1\n",
      "}\n",
      "hyperparameter_search_space {\n",
      "\t hidden_size : <ray.tune.search.sample.Categorical object at 0x7f3c0e355ca0>\n",
      "\t lr : <ray.tune.search.sample.Categorical object at 0x7f3c0e361d00>\n",
      "\t batch_size : <ray.tune.search.sample.Categorical object at 0x7f3c4779b370>\n",
      "\t dropout : <ray.tune.search.sample.Categorical object at 0x7f3c4779b490>\n",
      "\t num_layers : <ray.tune.search.sample.Categorical object at 0x7f3c4779b580>\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from propythia.ml.torch.deep_ml import read_config\n",
    "config = read_config(device,filename='./configDNAtorch.json')\n",
    "\n",
    "for key, val in config.items():\n",
    "    if(key == \"do_tuning\" or key == 'train_all_combinations'):\n",
    "        print(key, \":\", val)\n",
    "    else:\n",
    "        print(key, \"{\")\n",
    "        for k, v in val.items():\n",
    "            print(\"\\t\", k,\":\", v)\n",
    "        print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is a dict called 'hyperparameters' for the training. These values were arbitrarily chosen, which can lead to poor performance, and that's why we need hyperparameter tuning to find the best values. But so far let's keep it simple and use the default values. Hyperparameter tuning will be discussed later in the tutorial (the dict called 'hyperparameter_search_space' will be used later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to call the training function with all of these values and we will obtain a trained model. But before this, it important to specify which device we want the model to be trained on. Generally, it is a good idea to use the GPU if it is available. It is also a good practice to set a seed to ensure that the results are reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to call the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 50, 4)\n",
      "[1/500, 0/38] loss: 0.69556516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/torch/nn/modules/rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.35 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Current Loss: 0.6328604358893174\n",
      "trigger times: 0\n",
      "[2/500, 0/38] loss: 0.56666702\n",
      "The Current Loss: 0.5569665088103368\n",
      "trigger times: 0\n",
      "[3/500, 0/38] loss: 0.37936792\n",
      "The Current Loss: 0.6042217383017907\n",
      "trigger Times: 1\n",
      "[4/500, 0/38] loss: 0.69421768\n",
      "The Current Loss: 0.5318701886213743\n",
      "trigger times: 0\n",
      "[5/500, 0/38] loss: 0.46688917\n",
      "The Current Loss: 0.525668414739462\n",
      "trigger times: 0\n",
      "[6/500, 0/38] loss: 0.49461269\n",
      "The Current Loss: 0.5204136967658997\n",
      "trigger times: 0\n",
      "[7/500, 0/38] loss: 0.30128571\n",
      "The Current Loss: 0.5196680495372186\n",
      "trigger times: 0\n",
      "[8/500, 0/38] loss: 0.39531812\n",
      "The Current Loss: 0.5120856555608603\n",
      "trigger times: 0\n",
      "[9/500, 0/38] loss: 0.40634978\n",
      "The Current Loss: 0.4748935240965623\n",
      "trigger times: 0\n",
      "[10/500, 0/38] loss: 0.45036387\n",
      "The Current Loss: 0.42444491157164943\n",
      "trigger times: 0\n",
      "[11/500, 0/38] loss: 0.29471993\n",
      "The Current Loss: 0.24339806746978027\n",
      "trigger times: 0\n",
      "[12/500, 0/38] loss: 0.080889851\n",
      "The Current Loss: 0.15395008228146112\n",
      "trigger times: 0\n",
      "[13/500, 0/38] loss: 0.13156871\n",
      "The Current Loss: 0.06629659070704992\n",
      "trigger times: 0\n",
      "[14/500, 0/38] loss: 0.012232898\n",
      "The Current Loss: 0.04673452123713035\n",
      "trigger times: 0\n",
      "[15/500, 0/38] loss: 0.0059354659\n",
      "The Current Loss: 0.040643348931693114\n",
      "trigger times: 0\n",
      "[16/500, 0/38] loss: 0.016308673\n",
      "The Current Loss: 0.02425527395322346\n",
      "trigger times: 0\n",
      "[17/500, 0/38] loss: 0.012713271\n",
      "The Current Loss: 0.0257307452803406\n",
      "trigger Times: 1\n",
      "[18/500, 0/38] loss: 0.0024749122\n",
      "The Current Loss: 0.019187076268001244\n",
      "trigger times: 0\n",
      "[19/500, 0/38] loss: 0.0031093974\n",
      "The Current Loss: 0.022832355309779253\n",
      "trigger Times: 1\n",
      "[20/500, 0/38] loss: 0.0010009129\n",
      "The Current Loss: 0.11902043380093975\n",
      "trigger Times: 2\n",
      "[21/500, 0/38] loss: 0.024148349\n",
      "The Current Loss: 0.016942429940592356\n",
      "trigger times: 0\n",
      "[22/500, 0/38] loss: 0.0018856352\n",
      "The Current Loss: 0.023299388652398754\n",
      "trigger Times: 1\n",
      "[23/500, 0/38] loss: 0.0016454989\n",
      "The Current Loss: 0.021461806282436904\n",
      "trigger times: 0\n",
      "[24/500, 0/38] loss: 0.00032060512\n",
      "The Current Loss: 0.02151803270456954\n",
      "trigger Times: 1\n",
      "[25/500, 0/38] loss: 0.0011836172\n",
      "The Current Loss: 0.027618908524835624\n",
      "trigger Times: 2\n",
      "[26/500, 0/38] loss: 0.0010730601\n",
      "The Current Loss: 0.025173655173812922\n",
      "trigger times: 0\n",
      "[27/500, 0/38] loss: 0.00025699838\n",
      "The Current Loss: 0.022844885921446033\n",
      "trigger times: 0\n",
      "[28/500, 0/38] loss: 0.00041086198\n",
      "The Current Loss: 0.021121739340462506\n",
      "trigger times: 0\n",
      "[29/500, 0/38] loss: 0.00017851524\n",
      "The Current Loss: 0.021821449240983035\n",
      "trigger Times: 1\n",
      "[30/500, 0/38] loss: 0.0013374861\n",
      "The Current Loss: 0.0261280539427669\n",
      "trigger Times: 2\n",
      "[31/500, 0/38] loss: 5.7581081e-05\n",
      "The Current Loss: 0.03207739815451742\n",
      "trigger Times: 3\n",
      "Early stopping!\n",
      "Start to test process.\n"
     ]
    }
   ],
   "source": [
    "from propythia.ml.torch.train import traindata\n",
    "hyperparameters = config['hyperparameters']\n",
    "print(fps_x.shape)\n",
    "model = traindata(hyperparameters, device, config, trainloader, validloader, input_size=fps_x.shape[2], sequence_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we didn't need to read any data or calculate the descriptors. This is because the training function already did all of those steps using the `prepare_data` function mentioned in the introduction of this chapter's important note. However, we will need to do it again now to obtain the test set to see if the model is working properly. This is inconvenient because we are reading and splitting the data twice, but this is required because later we will use 'batch_size' (which is used to read the data) as a varying hyperparameter. Because we can only vary the hyperparameters inside the train function, we have to read the data in that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from propythia.ml.torch.prepare_data import prepare_data\n",
    "mode = config['combination']['mode']\n",
    "data_dir = config['combination']['data_dir']\n",
    "kmer_one_hot = config['fixed_vals']['kmer_one_hot']\n",
    "model_label = config['combination']['model_label'] \n",
    "batch_size = config['hyperparameters']['batch_size']\n",
    "\n",
    "_, testloader, _, _, _ = prepare_data(\n",
    "    data_dir=data_dir,\n",
    "    mode=mode,\n",
    "    batch_size=batch_size,\n",
    "    k=kmer_one_hot\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well the model performs on the test set. The metrics chosen are the accuracy, the Matthews correlation coefficient, and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results in test set:\n",
      "Results in test set:\n",
      "--------------------\n",
      "- model:   bi_lstm\n",
      "- mode:    one_hot\n",
      "- dataset: primer\n",
      "--------------------\n",
      "Accuracy: 0.998\n",
      "MCC: 0.995\n",
      "{'accuracy': 0.9975, 'mcc': 0.9950116913435413, 'roc_auc': 0.9975369458128079, 'f1': 0.9974683544303797, 'recall': 1.0, 'confusion_matrix': array([[202,   1],\n",
      "       [  0, 197]])}\n"
     ]
    }
   ],
   "source": [
    "from propythia.ml.torch.test import test\n",
    "scores = test(device, model, testloader)\n",
    "print(\"Results in test set:\")\n",
    "\n",
    "acc = scores['accuracy']\n",
    "mcc = scores['mcc']\n",
    "\n",
    "print(\"Results in test set:\")\n",
    "print(\"--------------------\")\n",
    "print(\"- model:  \", model_label)\n",
    "print(\"- mode:   \", mode)\n",
    "print(\"- dataset:\", data_dir.split(\"/\")[-1])\n",
    "print(\"--------------------\")\n",
    "print('Accuracy: %.3f' % acc)\n",
    "print('MCC: %.3f' % mcc)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, there was developed a method to find the best hyperparameters. This method is called *hyperparameter tuning*. It is a process of tuning the hyperparameters of a model to obtain the best performance. A function called `hyperparameter_tuning` was implemented that performs this process. It takes as input the config object (which must have the hyperparameters search space) and the device on which the model will be trained. It will create a scheduler called `ASHAScheduler` that will be used terminate the training if the model does not improve for a certain number of epochs. There will be created also a `CLIReporter` object that will report the metrics on the console (accuracy, Matthews correlation coefficient, and loss). Then, `num_samples` samples will be drawn from the hyperparameter search space and the model will be trained on each of them. The best model will be the one that has the highest Matthews correlation coefficient and will be then tested on the test set, outputting the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 21:16:38,166\tERROR services.py:1169 -- Failed to start the dashboard , return code 0\n",
      "2023-08-14 21:16:38,167\tERROR services.py:1194 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.\n",
      "2023-08-14 21:16:38,167\tERROR services.py:1238 -- \n",
      "The last 20 lines of /tmp/ray/session_2023-08-14_21-16-36_489342_793639/logs/dashboard.log (it contains the error message from the dashboard): \n",
      "    loop.run_until_complete(dashboard.run())\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/asyncio/base_events.py\", line 647, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/dashboard/dashboard.py\", line 70, in run\n",
      "    await self.dashboard_head.run()\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/dashboard/head.py\", line 302, in run\n",
      "    self.http_server = await self._configure_http_server(modules)\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/dashboard/head.py\", line 143, in _configure_http_server\n",
      "    http_server = HttpServerDashboardHead(\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/dashboard/http_server_head.py\", line 93, in __init__\n",
      "    raise ex\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/dashboard/http_server_head.py\", line 84, in __init__\n",
      "    build_dir = setup_static_dir()\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/dashboard/http_server_head.py\", line 42, in setup_static_dir\n",
      "    raise dashboard_utils.FrontendNotFoundError(\n",
      "ray.dashboard.utils.FrontendNotFoundError: [Errno 2] Dashboard build directory not found. If installing from source, please follow the additional steps required to build the dashboard(cd python/ray/dashboard/client && npm ci && npm run build): '/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/dashboard/client/build'\n",
      "\n",
      "2023-08-14 21:16:38,085\tERROR base_events.py:1753 -- Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x7fc1b1debd60>\n",
      "2023-08-14 21:16:38,085\tERROR base_events.py:1753 -- Unclosed client session\n",
      "2023-08-14 21:16:38,229\tINFO worker.py:1553 -- Started a local Ray instance.\n",
      "2023-08-14 21:16:38,934\tERROR trial_runner.py:1062 -- Trial prepare_and_train_799d7_00000: Error processing event.\n",
      "ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1276, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/_private/worker.py\", line 2382, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 7e670d3f80872ebf7cf13cd901000000\n",
      "\tnamespace: 36ec617e-c386-4689-9171-1d3b9f81fcdc\n",
      "The actor is dead because its owner has died. Owner Id: 01000000ffffffffffffffffffffffffffffffffffffffffffffffff Owner Ip address: 192.168.92.201 Owner worker exit type: SYSTEM_ERROR Worker exit detail: Owner's node has crashed.\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2023-08-14 21:16:38 (running for 00:00:00.07)\n",
      "Memory usage on this node: 2.2/62.6 GiB \n",
      "Using HyperBand: num_stopped=0 total_brackets=1\n",
      "Round #0:\n",
      "  Bracket(Max Size (n)=10, Milestone (r)=500, completed=0.0%): {PENDING: 4, RUNNING: 1} \n",
      "Resources requested: 2.0/8 CPUs, 2.0/2 GPUs, 0.0/35.67 GiB heap, 0.0/17.83 GiB objects (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /home/martinha/ray_results/prepare_and_train_2023-08-14_21-16-38\n",
      "Number of trials: 5/5 (4 PENDING, 1 RUNNING)\n",
      "+-------------------------------+----------+-------+--------------+-----------+---------------+--------+--------------+\n",
      "| Trial name                    | status   | loc   |   batch_size |   dropout |   hidden_size |     lr |   num_layers |\n",
      "|-------------------------------+----------+-------+--------------+-----------+---------------+--------+--------------|\n",
      "| prepare_and_train_799d7_00000 | RUNNING  |       |           64 |       0.4 |           128 | 0.0001 |            1 |\n",
      "| prepare_and_train_799d7_00001 | PENDING  |       |           32 |       0.4 |            32 | 0.01   |            3 |\n",
      "| prepare_and_train_799d7_00002 | PENDING  |       |           16 |       0.5 |           128 | 0.01   |            3 |\n",
      "| prepare_and_train_799d7_00003 | PENDING  |       |           32 |       0.5 |            64 | 0.0001 |            2 |\n",
      "| prepare_and_train_799d7_00004 | PENDING  |       |           16 |       0.2 |            64 | 0.001  |            2 |\n",
      "+-------------------------------+----------+-------+--------------+-----------+---------------+--------+--------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                   </th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>prepare_and_train_799d7_00000</td><td>799d7_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 21:16:38,938\tERROR ray_trial_executor.py:930 -- An exception occurred when trying to stop the Ray actor:Traceback (most recent call last):\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/tune/execution/ray_trial_executor.py\", line 921, in _resolve_stop_event\n",
      "    ray.get(future, timeout=timeout)\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/martinha/miniconda3/envs/propythia/lib/python3.9/site-packages/ray/_private/worker.py\", line 2382, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: ImplicitFunc\n",
      "\tactor_id: 7e670d3f80872ebf7cf13cd901000000\n",
      "\tnamespace: 36ec617e-c386-4689-9171-1d3b9f81fcdc\n",
      "The actor is dead because its owner has died. Owner Id: 01000000ffffffffffffffffffffffffffffffffffffffffffffffff Owner Ip address: 192.168.92.201 Owner worker exit type: SYSTEM_ERROR Worker exit detail: Owner's node has crashed.\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-14 21:16:38,930 E 793716 793742] (raylet) agent_manager.cc:135: The raylet exited immediately because the Ray agent failed. The raylet fate shares with the agent. This can happen because the Ray agent was unexpectedly killed or failed. Agent can fail when\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m - The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m - The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/dashboard_agent.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m - The agent is killed by the OS (e.g., out of memory).\n"
     ]
    }
   ],
   "source": [
    "os.chdir('../')\n",
    "sys.path.append(os.getcwd())\n",
    "from propythia.ml.torch.hyperparameter_tuning import hyperparameter_tuning\n",
    "config['do_tuning'] = True\n",
    "hyperparameter_tuning(device, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've reached the end of the deep learning pipeline. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba449ea13c29f64a91968d8f927cecceedd6e605eda30388903386e6cd94168d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
