{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification of human sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll try to replicate the [Classifying human DNA sequence and random ATCG sequences, using keras CNN](https://github.com/onceupon/deep_learning_DNA) problem, but using PyTorch instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data_utils\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import DataParallel\n",
    "from torch import Tensor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CTACTCGGGAGGCTGAGGCAGGAGAATCACTTGAACCAGGGAGTCA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CACCTTATCCAGAGAAGCTTCTTCTTTTAGAAAATCAAGCAAAACA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAGGGGCTGATAGAAAAATAAAGAGATTTGGCCAGGTACGGTGGC...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAGTGGATATTCAGACCTCCTTGAGGCCTTCGTTGGAAACGGGATT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATACCATGACAAAGATATTATTAGCCAATTTTTAGAGAGAAGGAAA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>AAGACGAGTGGTTACCCCCTGTGAGACTGCCGCGCGTGGTGGTCGG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>ACGAGGCCATCTAGAGCGCAACGAAACTGCAAGAAAATACGTCCGA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>CATCTTTACCCAATTTGTGTGTGGAAAGGTGTCAGCGATGCTTTGG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>ATAAGTAACTCCAGTCGAGCGGAAGGTATAGTGGCACGGGGTTTAG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>CTTGTAACGCCATAGCCGAATTTGCTCTATTTTCTTATGTCCCTCT...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sequence  label\n",
       "0      CTACTCGGGAGGCTGAGGCAGGAGAATCACTTGAACCAGGGAGTCA...      1\n",
       "1      CACCTTATCCAGAGAAGCTTCTTCTTTTAGAAAATCAAGCAAAACA...      1\n",
       "2      AAAGGGGCTGATAGAAAAATAAAGAGATTTGGCCAGGTACGGTGGC...      1\n",
       "3      AAGTGGATATTCAGACCTCCTTGAGGCCTTCGTTGGAAACGGGATT...      1\n",
       "4      ATACCATGACAAAGATATTATTAGCCAATTTTTAGAGAGAAGGAAA...      1\n",
       "...                                                  ...    ...\n",
       "19995  AAGACGAGTGGTTACCCCCTGTGAGACTGCCGCGCGTGGTGGTCGG...      0\n",
       "19996  ACGAGGCCATCTAGAGCGCAACGAAACTGCAAGAAAATACGTCCGA...      0\n",
       "19997  CATCTTTACCCAATTTGTGTGTGGAAAGGTGTCAGCGATGCTTTGG...      0\n",
       "19998  ATAAGTAACTCCAGTCGAGCGGAAGGTATAGTGGCACGGGGTTTAG...      0\n",
       "19999  CTTGTAACGCCATAGCCGAATTTGCTCTATTTTCTTATGTCCCTCT...      0\n",
       "\n",
       "[20000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"datasets/human-exercise/dataset.csv\")\n",
    "print(dataset.shape)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this dataset contains the sequence and the corresponding positive/negative class labels, with positive class labels corresponding to the human DNA. The amount of positive and negative examples is evenly distributed across the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    10000\n",
      "0    10000\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of each class\n",
    "dataset.groupby('label').size().plot(kind='bar')\n",
    "\n",
    "print(dataset['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need now to split the dataset into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_x = dataset['sequence'].values\n",
    "fps_y = dataset['label'].values\n",
    "\n",
    "# split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(fps_x, fps_y, stratify=fps_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to one hot encode the sequences and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 250, 4)\n",
      "(15000, 2)\n",
      "(5000, 250, 4)\n",
      "(5000, 2)\n"
     ]
    }
   ],
   "source": [
    "import encoding as enc\n",
    "X_train_enc = enc.DNAEncoding(X_train)\n",
    "X_train = X_train_enc.one_hot_encode()\n",
    "\n",
    "y_train_enc = enc.DNAEncoding(y_train)\n",
    "y_train = y_train_enc.one_hot_encode()\n",
    "\n",
    "X_test_enc = enc.DNAEncoding(X_test)\n",
    "X_test = X_test_enc.one_hot_encode()\n",
    "\n",
    "y_test_enc = enc.DNAEncoding(y_test)\n",
    "y_test = y_test_enc.one_hot_encode()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until this point, everything is working fine because the Keras model achieved the same accuracy as comparative study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_utils.TensorDataset(torch.tensor(X_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.long))\n",
    "train_loader = data_utils.DataLoader(train_data, shuffle=True, batch_size=16)\n",
    "test_data = data_utils.TensorDataset(torch.tensor(X_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.long))\n",
    "test_loader = data_utils.DataLoader(test_data, shuffle=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(4, 20, 10, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(47, 10)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "\n",
    "        self.max_pool = nn.MaxPool1d(10, stride=5)\n",
    "    \n",
    "        self.act1 = nn.ReLU()\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.act3 = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act3(x)\n",
    "        print(\"x:\", x)\n",
    "        print(\"x.shape:\", x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, device, train_loader, optimizer, epoch):\n",
    "#     model.train()\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data)\n",
    "#         loss = F.nll_loss(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         if batch_idx % 1000 == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "\n",
    "def train_model(train_loader, model, epochs, device):\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    for epoch in range(epochs):\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            yhat = model(inputs)\n",
    "            # print(\"yhat: \", yhat)\n",
    "            # print(\"yhat.shape: \", yhat.shape)\n",
    "            loss = criterion(yhat, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch: {epoch}, Loss: {loss}')\n",
    "            \n",
    "\n",
    "# def test(model, device, test_loader):\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "#             pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability            \n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "#         test_loss, correct, len(test_loader.dataset),\n",
    "#         100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "from numpy import vstack\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "def evaluate_model(model, device, test_loader):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        yhat = model(inputs)\n",
    "        yhat = yhat.cpu().detach().numpy()\n",
    "        actual = targets.cpu().numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        yhat = yhat.reshape((len(yhat), 1))\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    mcc = matthews_corrcoef(actuals, predictions)\n",
    "    report = confusion_matrix(actuals, predictions)\n",
    "    return acc, mcc, report\n",
    "\n",
    "def predict(row, model):\n",
    "    row = Tensor([row])\n",
    "    yhat = model(row)\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[0.2725, 0.0000],\n",
      "         [0.3186, 0.0000],\n",
      "         [0.1022, 0.0000],\n",
      "         [0.1066, 0.0198],\n",
      "         [0.0932, 0.0000],\n",
      "         [0.3060, 0.0000],\n",
      "         [0.0300, 0.0000],\n",
      "         [0.2565, 0.0000],\n",
      "         [0.2864, 0.0000],\n",
      "         [0.1867, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0576, 0.0000],\n",
      "         [0.2308, 0.0000],\n",
      "         [0.0058, 0.0000],\n",
      "         [0.4234, 0.0000],\n",
      "         [0.2025, 0.0000],\n",
      "         [0.3265, 0.0000],\n",
      "         [0.0958, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2243, 0.0000],\n",
      "         [0.4616, 0.0000],\n",
      "         [0.0000, 0.0200],\n",
      "         [0.1394, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2419, 0.0000],\n",
      "         [0.4689, 0.0000],\n",
      "         [0.1306, 0.0000],\n",
      "         [0.3085, 0.0000],\n",
      "         [0.0115, 0.0000],\n",
      "         [0.0263, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.0000, 0.0007],\n",
      "         [0.3483, 0.0000],\n",
      "         [0.2265, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.0394, 0.0000],\n",
      "         [0.0823, 0.0000],\n",
      "         [0.2240, 0.0000]],\n",
      "\n",
      "        [[0.1249, 0.0000],\n",
      "         [0.0046, 0.0000],\n",
      "         [0.0000, 0.0510],\n",
      "         [0.0224, 0.0000],\n",
      "         [0.2621, 0.0000],\n",
      "         [0.0453, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.0693, 0.1135],\n",
      "         [0.1325, 0.0782],\n",
      "         [0.0449, 0.0768],\n",
      "         [0.0648, 0.0000],\n",
      "         [0.0371, 0.0000],\n",
      "         [0.2552, 0.0000],\n",
      "         [0.1166, 0.0000],\n",
      "         [0.2220, 0.0000],\n",
      "         [0.2650, 0.0000],\n",
      "         [0.1238, 0.0000],\n",
      "         [0.1954, 0.0000],\n",
      "         [0.0177, 0.0000],\n",
      "         [0.0253, 0.0811]],\n",
      "\n",
      "        [[0.2763, 0.0000],\n",
      "         [0.0466, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0493, 0.0000],\n",
      "         [0.0000, 0.0584],\n",
      "         [0.1292, 0.0000],\n",
      "         [0.4590, 0.0000],\n",
      "         [0.1860, 0.0000],\n",
      "         [0.3603, 0.0000],\n",
      "         [0.1031, 0.0000],\n",
      "         [0.1200, 0.1132],\n",
      "         [0.3065, 0.0000],\n",
      "         [0.0486, 0.0000],\n",
      "         [0.1601, 0.0000],\n",
      "         [0.2552, 0.0000],\n",
      "         [0.3007, 0.0000],\n",
      "         [0.1530, 0.0000],\n",
      "         [0.2177, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.0850, 0.0000]],\n",
      "\n",
      "        [[0.3300, 0.0000],\n",
      "         [0.2346, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2439, 0.0000],\n",
      "         [0.0744, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0124, 0.0000],\n",
      "         [0.3480, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1095, 0.1602],\n",
      "         [0.0000, 0.1072],\n",
      "         [0.0405, 0.0064],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0122, 0.0000],\n",
      "         [0.2309, 0.0000],\n",
      "         [0.0502, 0.0000],\n",
      "         [0.0000, 0.2031],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2096, 0.0000]],\n",
      "\n",
      "        [[0.5160, 0.0000],\n",
      "         [0.2584, 0.0000],\n",
      "         [0.0000, 0.1268],\n",
      "         [0.1150, 0.0000],\n",
      "         [0.1905, 0.0000],\n",
      "         [0.0662, 0.0000],\n",
      "         [0.1079, 0.1832],\n",
      "         [0.1359, 0.0000],\n",
      "         [0.3361, 0.0000],\n",
      "         [0.2196, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.2205, 0.0239],\n",
      "         [0.2849, 0.0000],\n",
      "         [0.1799, 0.0000],\n",
      "         [0.1840, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.0489, 0.0000],\n",
      "         [0.1429, 0.0760],\n",
      "         [0.3031, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.2182, 0.0000],\n",
      "         [0.1055, 0.0590],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0061, 0.0000],\n",
      "         [0.1526, 0.0000],\n",
      "         [0.2489, 0.0000],\n",
      "         [0.0515, 0.0426],\n",
      "         [0.2610, 0.0000],\n",
      "         [0.0286, 0.0000],\n",
      "         [0.0849, 0.0000],\n",
      "         [0.2112, 0.0000],\n",
      "         [0.0904, 0.0668],\n",
      "         [0.0959, 0.0000],\n",
      "         [0.2327, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.5493, 0.0000],\n",
      "         [0.1175, 0.0000],\n",
      "         [0.2308, 0.0339],\n",
      "         [0.1085, 0.0000],\n",
      "         [0.0403, 0.0000]],\n",
      "\n",
      "        [[0.3810, 0.0000],\n",
      "         [0.1117, 0.1605],\n",
      "         [0.2724, 0.0000],\n",
      "         [0.1093, 0.0000],\n",
      "         [0.0000, 0.0166],\n",
      "         [0.1047, 0.0000],\n",
      "         [0.2107, 0.0000],\n",
      "         [0.1761, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0771, 0.0000],\n",
      "         [0.1256, 0.0000],\n",
      "         [0.1163, 0.0000],\n",
      "         [0.2042, 0.0000],\n",
      "         [0.3063, 0.0000],\n",
      "         [0.3253, 0.0000],\n",
      "         [0.2742, 0.0762],\n",
      "         [0.1157, 0.0000],\n",
      "         [0.0472, 0.0000],\n",
      "         [0.1340, 0.0000],\n",
      "         [0.0000, 0.1351]]], device='cuda:0', grad_fn=<ReluBackward0>)\n",
      "x.shape: torch.Size([8, 20, 2])\n",
      "x: tensor([[[0.3378, 0.0000],\n",
      "         [0.4023, 0.0000],\n",
      "         [0.0855, 0.0000],\n",
      "         [0.1914, 0.0000],\n",
      "         [0.1694, 0.0000],\n",
      "         [0.1508, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3284, 0.0000],\n",
      "         [0.3179, 0.0000],\n",
      "         [0.1732, 0.0000],\n",
      "         [0.0560, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0602, 0.0000],\n",
      "         [0.2292, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.3085, 0.0155],\n",
      "         [0.1925, 0.0000],\n",
      "         [0.2604, 0.0000],\n",
      "         [0.0229, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.0851, 0.0000],\n",
      "         [0.1741, 0.0000],\n",
      "         [0.0000, 0.0011],\n",
      "         [0.0872, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1299, 0.0000],\n",
      "         [0.4129, 0.0000],\n",
      "         [0.1314, 0.0000],\n",
      "         [0.4097, 0.0000],\n",
      "         [0.0240, 0.0797],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2385, 0.0000],\n",
      "         [0.1779, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2699, 0.0000],\n",
      "         [0.0050, 0.0000]],\n",
      "\n",
      "        [[0.1120, 0.0077],\n",
      "         [0.2060, 0.0000],\n",
      "         [0.0723, 0.0000],\n",
      "         [0.0717, 0.0000],\n",
      "         [0.1954, 0.0000],\n",
      "         [0.0838, 0.0479],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.0764, 0.1271],\n",
      "         [0.1191, 0.0000],\n",
      "         [0.0573, 0.0150],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2425, 0.0000],\n",
      "         [0.1147, 0.0000],\n",
      "         [0.2011, 0.0000],\n",
      "         [0.3539, 0.0000],\n",
      "         [0.0984, 0.0000],\n",
      "         [0.1653, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0645, 0.1352]],\n",
      "\n",
      "        [[0.3217, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0174, 0.0000],\n",
      "         [0.1218, 0.0000],\n",
      "         [0.1302, 0.0332],\n",
      "         [0.0850, 0.0429],\n",
      "         [0.4298, 0.0000],\n",
      "         [0.1937, 0.0000],\n",
      "         [0.1555, 0.0000],\n",
      "         [0.1784, 0.0000],\n",
      "         [0.1198, 0.0903],\n",
      "         [0.3547, 0.0000],\n",
      "         [0.0502, 0.0269],\n",
      "         [0.2077, 0.0000],\n",
      "         [0.1945, 0.0000],\n",
      "         [0.3042, 0.0000],\n",
      "         [0.1372, 0.0000],\n",
      "         [0.3062, 0.0000],\n",
      "         [0.0813, 0.0000],\n",
      "         [0.0850, 0.0000]],\n",
      "\n",
      "        [[0.3104, 0.0000],\n",
      "         [0.1941, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2075, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.0131, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1935, 0.0455],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0855, 0.1136],\n",
      "         [0.0468, 0.1288],\n",
      "         [0.0435, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.1157, 0.0000],\n",
      "         [0.1421, 0.0000],\n",
      "         [0.1091, 0.0000],\n",
      "         [0.0000, 0.1694],\n",
      "         [0.0307, 0.0000],\n",
      "         [0.2735, 0.0000]],\n",
      "\n",
      "        [[0.4632, 0.0000],\n",
      "         [0.4342, 0.0000],\n",
      "         [0.1492, 0.1606],\n",
      "         [0.2780, 0.0000],\n",
      "         [0.1094, 0.0000],\n",
      "         [0.1830, 0.0000],\n",
      "         [0.0674, 0.1105],\n",
      "         [0.1119, 0.0000],\n",
      "         [0.2359, 0.0000],\n",
      "         [0.1467, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.1861, 0.0000],\n",
      "         [0.1702, 0.0000],\n",
      "         [0.1057, 0.0000],\n",
      "         [0.2255, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.0718, 0.0000],\n",
      "         [0.1366, 0.0124],\n",
      "         [0.2093, 0.0000],\n",
      "         [0.0000, 0.0231]],\n",
      "\n",
      "        [[0.2109, 0.0000],\n",
      "         [0.0671, 0.0741],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0998, 0.0000],\n",
      "         [0.2338, 0.0000],\n",
      "         [0.0000, 0.1188],\n",
      "         [0.0133, 0.1139],\n",
      "         [0.2110, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.2599, 0.0000],\n",
      "         [0.0000, 0.1260],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0785, 0.0000],\n",
      "         [0.0850, 0.0000],\n",
      "         [0.5249, 0.0000],\n",
      "         [0.1089, 0.0000],\n",
      "         [0.2605, 0.0629],\n",
      "         [0.0992, 0.0000],\n",
      "         [0.0000, 0.0000]],\n",
      "\n",
      "        [[0.5053, 0.0000],\n",
      "         [0.0927, 0.1230],\n",
      "         [0.2039, 0.0000],\n",
      "         [0.1363, 0.0000],\n",
      "         [0.0000, 0.0000],\n",
      "         [0.0434, 0.0000],\n",
      "         [0.2299, 0.0000],\n",
      "         [0.2810, 0.0000],\n",
      "         [0.0000, 0.0488],\n",
      "         [0.0979, 0.0000],\n",
      "         [0.1218, 0.0000],\n",
      "         [0.3482, 0.0000],\n",
      "         [0.1017, 0.0000],\n",
      "         [0.2942, 0.0000],\n",
      "         [0.3805, 0.0000],\n",
      "         [0.2783, 0.0802],\n",
      "         [0.1453, 0.0000],\n",
      "         [0.0650, 0.0000],\n",
      "         [0.2805, 0.0000],\n",
      "         [0.0546, 0.0119]]], device='cuda:1', grad_fn=<ReluBackward0>)\n",
      "x.shape: torch.Size([8, 20, 2])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([16, 2])) that is different to the input size (torch.Size([16, 20, 2])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000013vscode-remote?line=7'>8</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000013vscode-remote?line=9'>10</a>\u001b[0m \u001b[39m# optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000013vscode-remote?line=10'>11</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000013vscode-remote?line=11'>12</a>\u001b[0m \u001b[39m# scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000013vscode-remote?line=14'>15</a>\u001b[0m \u001b[39m#     test(model, device, test_loader)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000013vscode-remote?line=15'>16</a>\u001b[0m \u001b[39m#     scheduler.step()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000013vscode-remote?line=18'>19</a>\u001b[0m train_model(train_loader, model, epochs, device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000013vscode-remote?line=19'>20</a>\u001b[0m acc, mcc, report \u001b[39m=\u001b[39m evaluate_model(test_loader, model, device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000013vscode-remote?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m acc)\n",
      "\u001b[1;32m/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb Cell 14'\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, model, epochs, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000012vscode-remote?line=22'>23</a>\u001b[0m yhat \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000012vscode-remote?line=23'>24</a>\u001b[0m \u001b[39m# print(\"yhat: \", yhat)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000012vscode-remote?line=24'>25</a>\u001b[0m \u001b[39m# print(\"yhat.shape: \", yhat.shape)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000012vscode-remote?line=25'>26</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(yhat, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000012vscode-remote?line=26'>27</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bturing.di.uminho.pt/home/jabreu/propythia/src/propythia/DNA/deep_ml/testing.ipynb#ch0000012vscode-remote?line=27'>28</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/modules/loss.py:612\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/modules/loss.py?line=610'>611</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/modules/loss.py?line=611'>612</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/functional.py:3056\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/functional.py?line=3053'>3054</a>\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/functional.py?line=3054'>3055</a>\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[0;32m-> <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/functional.py?line=3055'>3056</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/functional.py?line=3056'>3057</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/functional.py?line=3057'>3058</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/functional.py?line=3058'>3059</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/functional.py?line=3060'>3061</a>\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/jabreu/miniconda3/envs/dna-conda/lib/python3.8/site-packages/torch/nn/functional.py?line=3061'>3062</a>\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([16, 2])) that is different to the input size (torch.Size([16, 20, 2])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2022)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4,5'\n",
    "device = torch.device('cuda:0')\n",
    "epochs = 5\n",
    "\n",
    "model = Net()\n",
    "model = DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "# for epoch in range(1,epochs+1):\n",
    "#     train(model, device, train_loader, optimizer, epoch)\n",
    "#     test(model, device, test_loader)\n",
    "#     scheduler.step()\n",
    "\n",
    "\n",
    "train_model(train_loader, model, epochs, device)\n",
    "acc, mcc, report = evaluate_model(test_loader, model, device)\n",
    "\n",
    "print('Accuracy: %.3f' % acc)\n",
    "print('MCC: %.3f' % mcc)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba449ea13c29f64a91968d8f927cecceedd6e605eda30388903386e6cd94168d"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
