{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accurately predicting human essential genes based on deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will present a comparative analysis to demonstrate the application and performance of PyTorch models for addressing sequence-based prediction problems.\n",
    "\n",
    "We'll try to replicate the [DeepHE: Accurately predicting human essential genes based on deep learning](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008229) deep learning model and evaluate its performance. Other models will be compared to the DeepHE model.\n",
    "\n",
    "DeepHE's model is based on the multilayer perceptron structure. It includes one input layer, three hidden layers, and one output layer. All the hidden layers utilize the ReLU activation function. The output layer uses sigmoid activation function to perform discrete classification. The loss function in DeepHE is binary cross-entropy. A dropout layer is used after each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, there is already 2 cleaned datasets which will be used in this notebook.\n",
    "- `essential_genes.csv`: 16 datasets grouped that contain essential genes of the human genome. Each sequence has an EMBL id associated, the original dataset it came from, among other information. [Link](http://origin.tubic.org/deg/public/index.php)\n",
    "- `essential_genes_negative.csv`: contains the genome DNA sequences of humans for all annotated genes from Ensembl. Each sequence has an EMBL id associated. [Link](http://www.ensembl.org/Homo_sapiens/Info/Index)\n",
    "\n",
    "The process of cleaning each dataset is described below:\n",
    "- `essential_genes.csv`:\n",
    "    - removed rows with unavailable sequences.\n",
    "- `essential_genes_negative.csv`:\n",
    "    - removed all rows which sequences belonged to the `essential_genes.csv` dataset.\n",
    "    - removed all rows which EMBL id was in the `essential_genes.csv` dataset.\n",
    "    - grouped all sequences with the same EMBL id, and kept only the first two.\n",
    "\n",
    "One of the tasks in this notebook is to also build the positive dataset (`essential_genes_positve.csv`), which will contain only the sequences that are in at least 5 different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26791, 15)\n",
      "(27684, 2)\n"
     ]
    }
   ],
   "source": [
    "deg_dataset = pd.read_csv(\"datasets/essential_genes.csv\", sep=';')\n",
    "print(deg_dataset.shape)\n",
    "\n",
    "eg_negative = pd.read_csv(\"datasets/essential_genes_negative.csv\", sep=',')\n",
    "print(eg_negative.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating positive essential genes dataset. Each sequence needs to be in at least 5 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                                           sequence\n",
      "0  GI:-  ATGGTGCTGTCCCAGAGACAACGAGATGAACTAAATCGAGCTATAG...\n",
      "1  GI:-  ATGGCTGCAGCTTCATATGATCAGTTGTTAAAGCAAGTTGAGGCAC...\n",
      "2  GI:-  ATGAGCCGCCTGCTCTGGAGGAAGGTGGCCGGCGCCACCGTCGGGC...\n",
      "3  GI:-  ATGCAGAGCTGGAGTCGTGTGTACTGCTCCTTGGCCAAGAGAGGCC...\n",
      "4  GI:-  ATGGTTGGCTATGACCCCAAACCAGATGGCAGGAATAACACCAAGT...\n",
      "(2010, 2)\n"
     ]
    }
   ],
   "source": [
    "# for each sequence, get all the datasets that contain it\n",
    "d = {}\n",
    "for _, row in deg_dataset.iterrows():\n",
    "    if(row[\"sequence\"] in d):\n",
    "        d[row[\"sequence\"]].append((row[\"id1\"], row[\"id4\"]))\n",
    "    else:\n",
    "        d[row[\"sequence\"]] = [(row[\"id1\"], row[\"id4\"])]\n",
    "\n",
    "\n",
    "# get a list of sequences that are in more than 5 datasets\n",
    "essential_sequences = []\n",
    "for key, val in d.items():\n",
    "    if(len(val) >= 5):\n",
    "        essential_sequences.append((val[0][1], key))\n",
    "        \n",
    "# create dataframe with essential sequences\n",
    "eg_positive = pd.DataFrame(essential_sequences, columns=[\"id\", \"sequence\"])\n",
    "print(eg_positive.head())\n",
    "print(eg_positive.shape)\n",
    "\n",
    "# write to csv\n",
    "eg_positive.to_csv(\"datasets/essential_genes_positive.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique positive sequences: 2010\n",
      "unique negative sequences: 23443\n"
     ]
    }
   ],
   "source": [
    "print(\"unique positive sequences:\", len(set(eg_positive[\"sequence\"])))\n",
    "print(\"unique negative sequences:\", len(set(eg_negative[\"sequence\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the negative dataset has 27684 sequences, not all of them are unique. So, we need to remove the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23443, 2)\n"
     ]
    }
   ],
   "source": [
    "eg_negative = eg_negative.drop_duplicates(subset=\"sequence\")\n",
    "print(eg_negative.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have both positive and negative datasets:\n",
    "* eg_positive (2010,  2) -> positive dataset with essential genes \n",
    "* eg_negative (23443, 2) -> negative dataset with non essential genes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating length of each sequence and cleaning up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_positive[\"length\"] = eg_positive[\"sequence\"].str.len()\n",
    "eg_negative[\"length\"] = eg_negative[\"sequence\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before removing\n",
      "(2010, 3)\n",
      "(23443, 3)\n",
      "after removing\n",
      "(2010, 3) 0 were removed\n",
      "(23439, 3) 4 were removed\n"
     ]
    }
   ],
   "source": [
    "print(\"before removing\")\n",
    "print(eg_positive.shape)\n",
    "print(eg_negative.shape)\n",
    "\n",
    "old_eg_positive = eg_positive.shape[0]\n",
    "old_eg_negative = eg_negative.shape[0]\n",
    "\n",
    "eg_positive = eg_positive[eg_positive[\"length\"] <= 20000]\n",
    "eg_negative = eg_negative[eg_negative[\"length\"] <= 20000]\n",
    "print(\"after removing\")\n",
    "print(eg_positive.shape, old_eg_positive - eg_positive.shape[0], \"were removed\")\n",
    "print(eg_negative.shape, old_eg_negative - eg_negative.shape[0], \"were removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics about the data, including:\n",
    "* Total sequences\n",
    "* Top 5 longest and shortest sequences\n",
    "* Average length of sequences\n",
    "* Top 5 most and least common sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(dataset):\n",
    "    dataset[\"length\"].hist(bins=100)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"Total sequences:\", dataset.shape[0])\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"Top 5 longest sequences:\")\n",
    "    print(\"id       length\")\n",
    "    print(dataset[\"length\"].nlargest(5).to_string())\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"Top 5 shortest sequences:\")\n",
    "    print(\"id       length\")\n",
    "    print(dataset[\"length\"].nsmallest(5).to_string())\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    average_length = dataset[\"length\"].mean()\n",
    "    print(\"Average length:\", average_length)\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"Top 5 most common lengths:\")\n",
    "    print(\"length   count\")\n",
    "    print(dataset[\"length\"].value_counts().nlargest(5).to_string())\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    print(\"Top 5 least common lengths:\")\n",
    "    print(\"length   count\")\n",
    "    print(dataset[\"length\"].value_counts().nsmallest(5).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Total sequences: 2010\n",
      "----------------------------------------\n",
      "Top 5 longest sequences:\n",
      "id       length\n",
      "676    16791\n",
      "248    15615\n",
      "204    14574\n",
      "491    13941\n",
      "31     13167\n",
      "----------------------------------------\n",
      "Top 5 shortest sequences:\n",
      "id       length\n",
      "1112    192\n",
      "1427    195\n",
      "1500    204\n",
      "1611    204\n",
      "1346    210\n",
      "----------------------------------------\n",
      "Average length: 1903.6034825870647\n",
      "----------------------------------------\n",
      "Top 5 most common lengths:\n",
      "length   count\n",
      "1083    9\n",
      "1041    8\n",
      "618     7\n",
      "585     7\n",
      "1314    7\n",
      "----------------------------------------\n",
      "Top 5 least common lengths:\n",
      "length   count\n",
      "4311    1\n",
      "2289    1\n",
      "2958    1\n",
      "513     1\n",
      "2799    1\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Total sequences: 23439\n",
      "----------------------------------------\n",
      "Top 5 longest sequences:\n",
      "id       length\n",
      "20105    18921\n",
      "2852     17673\n",
      "12774    17388\n",
      "23013    17289\n",
      "25523    16965\n",
      "----------------------------------------\n",
      "Top 5 shortest sequences:\n",
      "id       length\n",
      "6728      3\n",
      "14762     5\n",
      "13586    10\n",
      "23228    10\n",
      "25635    12\n",
      "----------------------------------------\n",
      "Average length: 1351.6297196979394\n",
      "----------------------------------------\n",
      "Top 5 most common lengths:\n",
      "length   count\n",
      "939    98\n",
      "945    76\n",
      "930    72\n",
      "444    66\n",
      "948    63\n",
      "----------------------------------------\n",
      "Top 5 least common lengths:\n",
      "length   count\n",
      "644      1\n",
      "997      1\n",
      "11724    1\n",
      "607      1\n",
      "10896    1\n"
     ]
    }
   ],
   "source": [
    "stats(eg_positive)\n",
    "print(\"*\" * 100)\n",
    "stats(eg_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the positive and negative datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25449, 3)\n",
      "     id                                           sequence  label\n",
      "0  GI:-  ATGGTGCTGTCCCAGAGACAACGAGATGAACTAAATCGAGCTATAG...      1\n",
      "1  GI:-  ATGGCTGCAGCTTCATATGATCAGTTGTTAAAGCAAGTTGAGGCAC...      1\n",
      "2  GI:-  ATGAGCCGCCTGCTCTGGAGGAAGGTGGCCGGCGCCACCGTCGGGC...      1\n",
      "3  GI:-  ATGCAGAGCTGGAGTCGTGTGTACTGCTCCTTGGCCAAGAGAGGCC...      1\n",
      "4  GI:-  ATGGTTGGCTATGACCCCAAACCAGATGGCAGGAATAACACCAAGT...      1\n"
     ]
    }
   ],
   "source": [
    "# adding labels to the dataset\n",
    "eg_positive[\"label\"] = 1\n",
    "eg_negative[\"label\"] = 0\n",
    "\n",
    "# removing length column\n",
    "eg_positive = eg_positive.drop(columns=[\"length\"])\n",
    "eg_negative = eg_negative.drop(columns=[\"length\"])\n",
    "\n",
    "# joining the two datasets\n",
    "dataset = pd.concat([eg_positive, eg_negative])\n",
    "print(dataset.shape)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25449, 3)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label'>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of each class\n",
    "dataset.groupby('label').size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using DNA descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "\n",
    "sys.path.append('../../../../src/')\n",
    "from propythia.DNA.descriptors.descriptors import DNADescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature(data):\n",
    "    list_feature = []\n",
    "    count = 0\n",
    "    for seq in data['sequence']:\n",
    "        res = {'sequence': seq}\n",
    "        dna = DNADescriptor(seq)\n",
    "        feature = dna.get_descriptors()\n",
    "        res.update(feature)\n",
    "        list_feature.append(res)\n",
    "        # print progress every 100 sequences\n",
    "        if count % 100 == 0:\n",
    "            print(count, '/', len(data))\n",
    "\n",
    "        count += 1\n",
    "    print(\"Done!\")\n",
    "    df = pd.DataFrame(list_feature)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip the calculation of descriptors if `essential_genes_features.pkl` exists which already has them calculated. Skip all of this if `fps_x.pkl` exists because it already has the features calculated and **normalized**. The need of data normalization is explained in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features already calculated and normalized\n"
     ]
    }
   ],
   "source": [
    "if exists(\"datasets/fps_x.pkl\") == False:\n",
    "    if exists(\"datasets/essential_genes_features.pkl\"):\n",
    "        with open(\"datasets/essential_genes_features.pkl\", \"rb\") as f:\n",
    "            features = pickle.load(f)\n",
    "        print(\"Features loaded from pickle file\")\n",
    "    else:\n",
    "        features = calculate_feature(dataset)\n",
    "        with open(\"datasets/essential_genes_features.pkl\", \"wb\") as f:\n",
    "            pickle.dump(features, f)\n",
    "else:\n",
    "    print(\"Features already calculated and normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to help normalize the data.\n",
    "\n",
    "Without being normalized, we have a dataset with 17 columns. Each column is a result of a DNA descriptor, and some of these columns are numbers, dicts and even lists.\n",
    "\n",
    "We still need to normalize those who have dictionaries and lists because the model can't handle data in these types.\n",
    "\n",
    "To normalize the data, dicts and lists need to \"explode\" into more columns. \n",
    "\n",
    "E.g. dicts:\n",
    "\n",
    "| descriptor_hello |\n",
    "| ---------------- |\n",
    "| {'a': 1, 'b': 2} |\n",
    "\n",
    "will be transformed into:\n",
    "\n",
    "| descriptor_hello_a | descriptor_hello_b |\n",
    "| ------------------ | ------------------ |\n",
    "| 1                  | 2                  |\n",
    "\n",
    "E.g. lists:\n",
    "\n",
    "| descriptor_hello |\n",
    "| ---------------- |\n",
    "| [1, 2, 3]        |\n",
    "\n",
    "will be transformed into:\n",
    "\n",
    "| descriptor_hello_0 | descriptor_hello_1 | descriptor_hello_2 |\n",
    "| ------------------ | ------------------ | ------------------ |\n",
    "| 1                  | 2                  | 3                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lists(fps_x, field):\n",
    "    l = fps_x[field].to_list()\n",
    "    new_df = pd.DataFrame(l)\n",
    "    new_df.columns = [str(field) + \"_\" + str(i) for i in new_df.columns]\n",
    "    fps_x.drop(field, axis=1, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "def process_lists_of_lists(fps_x, field):\n",
    "    l = fps_x[field].to_list()\n",
    "    new_df = pd.DataFrame(l)\n",
    "    new_df.columns = [str(field) + \"_\" + str(i) for i in new_df.columns]\n",
    "    empty_val = {} if field == \"enhanced_nucleic_acid_composition\" else []\n",
    "    small_processed = []\n",
    "    for f in new_df.columns:\n",
    "        col = [empty_val if i is None else i for i in new_df[f].to_list()]\n",
    "        sub = pd.DataFrame(col)\n",
    "        sub.columns = [str(f) + \"_\" + str(i) for i in sub.columns]\n",
    "        small_processed.append(sub)\n",
    "    fps_x.drop(field, axis=1, inplace=True)\n",
    "    return small_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features():\n",
    "    fps_y = dataset['label']\n",
    "    fps_x = features.loc[:, features.columns != 'label']\n",
    "    fps_x = fps_x.loc[:, fps_x.columns != 'sequence']\n",
    "    print(fps_x.shape)\n",
    "    \n",
    "    lists = [\"nucleic_acid_composition\",\"dinucleotide_composition\",\"trinucleotide_composition\",\"k_spaced_nucleic_acid_pairs\",\"kmer\",\"PseDNC\", \"PseKNC\", \"DAC\", \"DCC\", \"DACC\", \"TAC\",\"TCC\",\"TACC\"]\n",
    "    lists_of_lists = [\n",
    "        \"accumulated_nucleotide_frequency\"\n",
    "    ]\n",
    "\n",
    "    small_processed = []\n",
    "    for i in lists:\n",
    "        print(\"Starting:\", i)\n",
    "        new_df = process_lists(fps_x, i)\n",
    "        small_processed.append(new_df)\n",
    "        \n",
    "    for i in lists_of_lists:\n",
    "        print(\"Starting:\", i)\n",
    "        smaller_processed = process_lists_of_lists(fps_x, i)\n",
    "        small_processed += smaller_processed\n",
    "\n",
    "    # concat final with original\n",
    "    fps_x = pd.concat([fps_x, *small_processed], axis=1)\n",
    "\n",
    "    with open(\"datasets/fps_x.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fps_x, f)\n",
    "        \n",
    "    with open(\"datasets/fps_y.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fps_y, f)\n",
    "    \n",
    "    return fps_x, fps_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip the data normalization if it was already performed (`fps_x.pkl` exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded from pickle file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>gc_content</th>\n",
       "      <th>at_content</th>\n",
       "      <th>nucleic_acid_composition_A</th>\n",
       "      <th>nucleic_acid_composition_C</th>\n",
       "      <th>nucleic_acid_composition_G</th>\n",
       "      <th>nucleic_acid_composition_T</th>\n",
       "      <th>dinucleotide_composition_AA</th>\n",
       "      <th>dinucleotide_composition_AC</th>\n",
       "      <th>dinucleotide_composition_AG</th>\n",
       "      <th>...</th>\n",
       "      <th>accumulated_nucleotide_frequency_0_G</th>\n",
       "      <th>accumulated_nucleotide_frequency_0_T</th>\n",
       "      <th>accumulated_nucleotide_frequency_1_A</th>\n",
       "      <th>accumulated_nucleotide_frequency_1_C</th>\n",
       "      <th>accumulated_nucleotide_frequency_1_G</th>\n",
       "      <th>accumulated_nucleotide_frequency_1_T</th>\n",
       "      <th>accumulated_nucleotide_frequency_2_A</th>\n",
       "      <th>accumulated_nucleotide_frequency_2_C</th>\n",
       "      <th>accumulated_nucleotide_frequency_2_G</th>\n",
       "      <th>accumulated_nucleotide_frequency_2_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1233</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8532</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3720</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1530</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>963</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25444</th>\n",
       "      <td>576</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25445</th>\n",
       "      <td>576</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25446</th>\n",
       "      <td>3363</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25447</th>\n",
       "      <td>1101</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25448</th>\n",
       "      <td>96</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25449 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       length  gc_content  at_content  nucleic_acid_composition_A  \\\n",
       "0        1233       0.440       0.560                       0.303   \n",
       "1        8532       0.412       0.588                       0.344   \n",
       "2        3720       0.604       0.396                       0.215   \n",
       "3        1530       0.414       0.586                       0.303   \n",
       "4         963       0.559       0.441                       0.212   \n",
       "...       ...         ...         ...                         ...   \n",
       "25444     576       0.439       0.561                       0.288   \n",
       "25445     576       0.434       0.566                       0.286   \n",
       "25446    3363       0.394       0.606                       0.338   \n",
       "25447    1101       0.633       0.367                       0.227   \n",
       "25448      96       0.635       0.365                       0.146   \n",
       "\n",
       "       nucleic_acid_composition_C  nucleic_acid_composition_G  \\\n",
       "0                           0.193                       0.247   \n",
       "1                           0.208                       0.205   \n",
       "2                           0.290                       0.314   \n",
       "3                           0.169                       0.246   \n",
       "4                           0.286                       0.273   \n",
       "...                           ...                         ...   \n",
       "25444                       0.210                       0.229   \n",
       "25445                       0.212                       0.222   \n",
       "25446                       0.192                       0.202   \n",
       "25447                       0.292                       0.341   \n",
       "25448                       0.302                       0.333   \n",
       "\n",
       "       nucleic_acid_composition_T  dinucleotide_composition_AA  \\\n",
       "0                           0.256                        0.096   \n",
       "1                           0.244                        0.126   \n",
       "2                           0.181                        0.041   \n",
       "3                           0.282                        0.099   \n",
       "4                           0.229                        0.048   \n",
       "...                           ...                          ...   \n",
       "25444                       0.273                        0.089   \n",
       "25445                       0.280                        0.089   \n",
       "25446                       0.268                        0.122   \n",
       "25447                       0.140                        0.037   \n",
       "25448                       0.219                        0.011   \n",
       "\n",
       "       dinucleotide_composition_AC  dinucleotide_composition_AG  ...  \\\n",
       "0                            0.052                        0.075  ...   \n",
       "1                            0.054                        0.089  ...   \n",
       "2                            0.050                        0.089  ...   \n",
       "3                            0.044                        0.080  ...   \n",
       "4                            0.045                        0.076  ...   \n",
       "...                            ...                          ...  ...   \n",
       "25444                        0.068                        0.068  ...   \n",
       "25445                        0.068                        0.066  ...   \n",
       "25446                        0.055                        0.076  ...   \n",
       "25447                        0.056                        0.105  ...   \n",
       "25448                        0.042                        0.042  ...   \n",
       "\n",
       "       accumulated_nucleotide_frequency_0_G  \\\n",
       "0                                     0.224   \n",
       "1                                     0.240   \n",
       "2                                     0.340   \n",
       "3                                     0.248   \n",
       "4                                     0.282   \n",
       "...                                     ...   \n",
       "25444                                 0.229   \n",
       "25445                                 0.229   \n",
       "25446                                 0.193   \n",
       "25447                                 0.295   \n",
       "25448                                 0.333   \n",
       "\n",
       "       accumulated_nucleotide_frequency_0_T  \\\n",
       "0                                     0.247   \n",
       "1                                     0.250   \n",
       "2                                     0.153   \n",
       "3                                     0.290   \n",
       "4                                     0.224   \n",
       "...                                     ...   \n",
       "25444                                 0.312   \n",
       "25445                                 0.312   \n",
       "25446                                 0.262   \n",
       "25447                                 0.175   \n",
       "25448                                 0.333   \n",
       "\n",
       "       accumulated_nucleotide_frequency_1_A  \\\n",
       "0                                     0.324   \n",
       "1                                     0.336   \n",
       "2                                     0.205   \n",
       "3                                     0.298   \n",
       "4                                     0.199   \n",
       "...                                     ...   \n",
       "25444                                 0.292   \n",
       "25445                                 0.292   \n",
       "25446                                 0.353   \n",
       "25447                                 0.227   \n",
       "25448                                 0.125   \n",
       "\n",
       "       accumulated_nucleotide_frequency_1_C  \\\n",
       "0                                     0.182   \n",
       "1                                     0.193   \n",
       "2                                     0.311   \n",
       "3                                     0.163   \n",
       "4                                     0.286   \n",
       "...                                     ...   \n",
       "25444                                 0.181   \n",
       "25445                                 0.181   \n",
       "25446                                 0.190   \n",
       "25447                                 0.292   \n",
       "25448                                 0.271   \n",
       "\n",
       "       accumulated_nucleotide_frequency_1_G  \\\n",
       "0                                     0.227   \n",
       "1                                     0.220   \n",
       "2                                     0.322   \n",
       "3                                     0.247   \n",
       "4                                     0.278   \n",
       "...                                     ...   \n",
       "25444                                 0.215   \n",
       "25445                                 0.215   \n",
       "25446                                 0.196   \n",
       "25447                                 0.323   \n",
       "25448                                 0.375   \n",
       "\n",
       "       accumulated_nucleotide_frequency_1_T  \\\n",
       "0                                     0.267   \n",
       "1                                     0.251   \n",
       "2                                     0.162   \n",
       "3                                     0.292   \n",
       "4                                     0.237   \n",
       "...                                     ...   \n",
       "25444                                 0.312   \n",
       "25445                                 0.312   \n",
       "25446                                 0.262   \n",
       "25447                                 0.158   \n",
       "25448                                 0.229   \n",
       "\n",
       "       accumulated_nucleotide_frequency_2_A  \\\n",
       "0                                     0.321   \n",
       "1                                     0.345   \n",
       "2                                     0.208   \n",
       "3                                     0.301   \n",
       "4                                     0.213   \n",
       "...                                     ...   \n",
       "25444                                 0.296   \n",
       "25445                                 0.296   \n",
       "25446                                 0.351   \n",
       "25447                                 0.225   \n",
       "25448                                 0.125   \n",
       "\n",
       "       accumulated_nucleotide_frequency_2_C  \\\n",
       "0                                     0.188   \n",
       "1                                     0.198   \n",
       "2                                     0.303   \n",
       "3                                     0.164   \n",
       "4                                     0.289   \n",
       "...                                     ...   \n",
       "25444                                 0.206   \n",
       "25445                                 0.206   \n",
       "25446                                 0.187   \n",
       "25447                                 0.293   \n",
       "25448                                 0.292   \n",
       "\n",
       "       accumulated_nucleotide_frequency_2_G  \\\n",
       "0                                     0.240   \n",
       "1                                     0.211   \n",
       "2                                     0.319   \n",
       "3                                     0.244   \n",
       "4                                     0.267   \n",
       "...                                     ...   \n",
       "25444                                 0.208   \n",
       "25445                                 0.208   \n",
       "25446                                 0.198   \n",
       "25447                                 0.340   \n",
       "25448                                 0.361   \n",
       "\n",
       "       accumulated_nucleotide_frequency_2_T  \n",
       "0                                     0.251  \n",
       "1                                     0.246  \n",
       "2                                     0.171  \n",
       "3                                     0.291  \n",
       "4                                     0.230  \n",
       "...                                     ...  \n",
       "25444                                 0.289  \n",
       "25445                                 0.289  \n",
       "25446                                 0.264  \n",
       "25447                                 0.142  \n",
       "25448                                 0.222  \n",
       "\n",
       "[25449 rows x 247 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if exists(\"datasets/fps_x.pkl\"):\n",
    "    with open(\"datasets/fps_x.pkl\", \"rb\") as f:\n",
    "        fps_x = pickle.load(f)\n",
    "    with open(\"datasets/fps_y.pkl\", \"rb\") as f:\n",
    "        fps_y = pickle.load(f)\n",
    "    print(\"Features loaded from pickle file\")\n",
    "else:\n",
    "    fps_x, fps_y = normalize_features()\n",
    "fps_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need now to split the dataset into training, test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15269, 247)\n",
      "(15269,)\n",
      "(5090, 247)\n",
      "(5090,)\n",
      "(5090, 247)\n",
      "(5090,)\n"
     ]
    }
   ],
   "source": [
    "x, x_test, y, y_test = train_test_split(\n",
    "    fps_x, fps_y,\n",
    "    test_size=0.2,\n",
    "    train_size=0.8,\n",
    "    stratify=fps_y\n",
    ")\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(\n",
    "    x, y,\n",
    "    test_size=0.25,\n",
    "    train_size=0.75,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "x_cv = scaler.transform(x_cv)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_cv.shape)\n",
    "print(y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2022)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4,5'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class_weights = torch.tensor([1.0, 4.0]).to(device)\n",
    "\n",
    "paramDict = {\n",
    "    'epoch': 50,\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.2,\n",
    "    'loss': nn.CrossEntropyLoss(weight=class_weights),\n",
    "    'input_size': x_train.shape[1],\n",
    "    'hidden_size': 128,\n",
    "    'output_size': 2,\n",
    "    'patience': 15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch.tensor\n",
    "train_data = data_utils.TensorDataset(\n",
    "    torch.tensor(x_train, dtype=torch.float),\n",
    "    torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    ")\n",
    "test_data = data_utils.TensorDataset(\n",
    "    torch.tensor(x_test, dtype=torch.float),\n",
    "    torch.tensor(y_test.to_numpy(), dtype=torch.long)\n",
    ")\n",
    "valid_data = data_utils.TensorDataset(\n",
    "    torch.tensor(x_cv, dtype=torch.float),\n",
    "    torch.tensor(y_cv.to_numpy(), dtype=torch.long)\n",
    ")\n",
    "\n",
    "# Data loader\n",
    "trainloader = data_utils.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=paramDict['batch_size']\n",
    ")\n",
    "testloader = data_utils.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=paramDict['batch_size']\n",
    ")\n",
    "validloader = data_utils.DataLoader(\n",
    "    valid_data,\n",
    "    shuffle=True,\n",
    "    batch_size=paramDict['batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.models import MLP\n",
    "from src.train import traindata\n",
    "from src.test import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/100, 0/478] loss: 0.68802488\n",
      "[1/100, 100/478] loss: 0.60594463\n",
      "[1/100, 200/478] loss: 0.31341287\n",
      "[1/100, 300/478] loss: 0.52378786\n",
      "[1/100, 400/478] loss: 0.52378803\n",
      "The Current Loss: 0.5543495733290911\n",
      "trigger times: 0\n",
      "[2/100, 0/478] loss: 0.73879373\n",
      "[2/100, 100/478] loss: 0.42754745\n",
      "[2/100, 200/478] loss: 0.60594422\n",
      "[2/100, 300/478] loss: 0.73879361\n",
      "[2/100, 400/478] loss: 0.42754757\n",
      "The Current Loss: 0.5518364679068327\n",
      "trigger times: 0\n",
      "[3/100, 0/478] loss: 0.6059444\n",
      "[3/100, 100/478] loss: 0.69314742\n",
      "[3/100, 200/478] loss: 0.52378786\n",
      "[3/100, 300/478] loss: 0.52378786\n",
      "[3/100, 400/478] loss: 0.52378798\n",
      "The Current Loss: 0.5531560124829411\n",
      "Trigger Times: 1\n",
      "[4/100, 0/478] loss: 0.60594445\n",
      "[4/100, 100/478] loss: 0.5237878\n",
      "[4/100, 200/478] loss: 0.67689788\n",
      "[4/100, 300/478] loss: 0.67689776\n",
      "[4/100, 400/478] loss: 0.52378786\n",
      "The Current Loss: 0.5526303561404348\n",
      "trigger times: 0\n",
      "[5/100, 0/478] loss: 0.5237878\n",
      "[5/100, 100/478] loss: 0.67689776\n",
      "[5/100, 200/478] loss: 0.67689782\n",
      "[5/100, 300/478] loss: 0.52378792\n",
      "[5/100, 400/478] loss: 0.73879355\n",
      "The Current Loss: 0.5547260804101825\n",
      "Trigger Times: 1\n",
      "[6/100, 0/478] loss: 0.67689794\n",
      "[6/100, 100/478] loss: 0.42754737\n",
      "[6/100, 200/478] loss: 0.52378792\n",
      "[6/100, 300/478] loss: 0.6059444\n",
      "[6/100, 400/478] loss: 0.6059444\n",
      "The Current Loss: 0.552276061475277\n",
      "trigger times: 0\n",
      "[7/100, 0/478] loss: 0.73879355\n",
      "[7/100, 100/478] loss: 0.52378786\n",
      "[7/100, 200/478] loss: 0.60594451\n",
      "[7/100, 300/478] loss: 0.73879355\n",
      "[7/100, 400/478] loss: 0.60594451\n",
      "The Current Loss: 0.5547053094953298\n",
      "Trigger Times: 1\n",
      "[8/100, 0/478] loss: 0.6059444\n",
      "[8/100, 100/478] loss: 0.52378798\n",
      "[8/100, 200/478] loss: 0.42754757\n",
      "[8/100, 300/478] loss: 0.67689788\n",
      "[8/100, 400/478] loss: 0.31326169\n",
      "The Current Loss: 0.5517708104103803\n",
      "trigger times: 0\n",
      "[9/100, 0/478] loss: 0.5237878\n",
      "[9/100, 100/478] loss: 0.31326169\n",
      "[9/100, 200/478] loss: 0.5237878\n",
      "[9/100, 300/478] loss: 0.52378798\n",
      "[9/100, 400/478] loss: 0.60594445\n",
      "The Current Loss: 0.5541381472721696\n",
      "Trigger Times: 1\n",
      "[10/100, 0/478] loss: 0.79326171\n",
      "[10/100, 100/478] loss: 0.52378798\n",
      "[10/100, 200/478] loss: 0.67689788\n",
      "[10/100, 300/478] loss: 0.52378815\n",
      "[10/100, 400/478] loss: 0.52378786\n",
      "The Current Loss: 0.5570047194138169\n",
      "Trigger Times: 2\n",
      "[11/100, 0/478] loss: 0.67689782\n",
      "[11/100, 100/478] loss: 0.31326169\n",
      "[11/100, 200/478] loss: 0.60594445\n",
      "[11/100, 300/478] loss: 0.4275474\n",
      "[11/100, 400/478] loss: 0.5237878\n",
      "The Current Loss: 0.5550612933933735\n",
      "trigger times: 0\n",
      "[12/100, 0/478] loss: 0.5237878\n",
      "[12/100, 100/478] loss: 0.52378798\n",
      "[12/100, 200/478] loss: 0.60594457\n",
      "[12/100, 300/478] loss: 0.42754743\n",
      "[12/100, 400/478] loss: 0.52378798\n",
      "The Current Loss: 0.5530101858079434\n",
      "trigger times: 0\n",
      "[13/100, 0/478] loss: 0.73879355\n",
      "[13/100, 100/478] loss: 0.67689782\n",
      "[13/100, 200/478] loss: 0.6059444\n",
      "[13/100, 300/478] loss: 0.42754745\n",
      "[13/100, 400/478] loss: 0.60594445\n",
      "The Current Loss: 0.5528202688321471\n",
      "trigger times: 0\n",
      "[14/100, 0/478] loss: 0.67689794\n",
      "[14/100, 100/478] loss: 0.6059444\n",
      "[14/100, 200/478] loss: 0.6059444\n",
      "[14/100, 300/478] loss: 0.52378798\n",
      "[14/100, 400/478] loss: 0.31326169\n",
      "The Current Loss: 0.5513421844691038\n",
      "trigger times: 0\n",
      "[15/100, 0/478] loss: 0.52378786\n",
      "[15/100, 100/478] loss: 0.42754743\n",
      "[15/100, 200/478] loss: 0.6059444\n",
      "[15/100, 300/478] loss: 0.52378786\n",
      "[15/100, 400/478] loss: 0.52378792\n",
      "The Current Loss: 0.5511712592095137\n",
      "trigger times: 0\n",
      "[16/100, 0/478] loss: 0.79326159\n",
      "[16/100, 100/478] loss: 0.52378792\n",
      "[16/100, 200/478] loss: 0.79326189\n",
      "[16/100, 300/478] loss: 0.42754751\n",
      "[16/100, 400/478] loss: 0.73879373\n",
      "The Current Loss: 0.554052178002894\n",
      "Trigger Times: 1\n",
      "[17/100, 0/478] loss: 0.52378803\n",
      "[17/100, 100/478] loss: 0.67689788\n",
      "[17/100, 200/478] loss: 0.6059444\n",
      "[17/100, 300/478] loss: 0.67689788\n",
      "[17/100, 400/478] loss: 0.42754757\n",
      "The Current Loss: 0.5518799426034093\n",
      "trigger times: 0\n",
      "[18/100, 0/478] loss: 0.42754754\n",
      "[18/100, 100/478] loss: 0.31326169\n",
      "[18/100, 200/478] loss: 0.60594457\n",
      "[18/100, 300/478] loss: 0.60594434\n",
      "[18/100, 400/478] loss: 0.52378786\n",
      "The Current Loss: 0.5581554720178247\n",
      "Trigger Times: 1\n",
      "[19/100, 0/478] loss: 0.52378792\n",
      "[19/100, 100/478] loss: 0.67689794\n",
      "[19/100, 200/478] loss: 0.6059444\n",
      "[19/100, 300/478] loss: 0.52378815\n",
      "[19/100, 400/478] loss: 0.5237878\n",
      "The Current Loss: 0.5512798376381397\n",
      "trigger times: 0\n",
      "[20/100, 0/478] loss: 0.73879361\n",
      "[20/100, 100/478] loss: 0.6059444\n",
      "[20/100, 200/478] loss: 0.60594445\n",
      "[20/100, 300/478] loss: 0.42754757\n",
      "[20/100, 400/478] loss: 0.73879331\n",
      "The Current Loss: 0.5501196702942253\n",
      "trigger times: 0\n",
      "[21/100, 0/478] loss: 0.42754757\n",
      "[21/100, 100/478] loss: 0.8415637\n",
      "[21/100, 200/478] loss: 0.52378792\n",
      "[21/100, 300/478] loss: 0.6059444\n",
      "[21/100, 400/478] loss: 0.60594428\n",
      "The Current Loss: 0.5542517552152276\n",
      "Trigger Times: 1\n",
      "[22/100, 0/478] loss: 0.6059444\n",
      "[22/100, 100/478] loss: 0.52378798\n",
      "[22/100, 200/478] loss: 0.52378792\n",
      "[22/100, 300/478] loss: 0.73879361\n",
      "[22/100, 400/478] loss: 0.60594445\n",
      "The Current Loss: 0.5535619178786874\n",
      "trigger times: 0\n",
      "[23/100, 0/478] loss: 0.5237878\n",
      "[23/100, 100/478] loss: 0.67689776\n",
      "[23/100, 200/478] loss: 0.42754737\n",
      "[23/100, 300/478] loss: 0.60594445\n",
      "[23/100, 400/478] loss: 0.52378815\n",
      "The Current Loss: 0.5548414224758744\n",
      "Trigger Times: 1\n",
      "[24/100, 0/478] loss: 0.73879379\n",
      "[24/100, 100/478] loss: 0.5237878\n",
      "[24/100, 200/478] loss: 0.42754757\n",
      "[24/100, 300/478] loss: 0.67689794\n",
      "[24/100, 400/478] loss: 0.42754757\n",
      "The Current Loss: 0.5542384365573525\n",
      "trigger times: 0\n",
      "[25/100, 0/478] loss: 0.6059444\n",
      "[25/100, 100/478] loss: 0.4275474\n",
      "[25/100, 200/478] loss: 0.73879355\n",
      "[25/100, 300/478] loss: 0.67689794\n",
      "[25/100, 400/478] loss: 0.676898\n",
      "The Current Loss: 0.5551965737715363\n",
      "Trigger Times: 1\n",
      "[26/100, 0/478] loss: 0.67689782\n",
      "[26/100, 100/478] loss: 0.42754757\n",
      "[26/100, 200/478] loss: 0.52378803\n",
      "[26/100, 300/478] loss: 0.52378809\n",
      "[26/100, 400/478] loss: 0.60594434\n",
      "The Current Loss: 0.5536524470895529\n",
      "trigger times: 0\n",
      "[27/100, 0/478] loss: 0.5237878\n",
      "[27/100, 100/478] loss: 0.67689776\n",
      "[27/100, 200/478] loss: 0.6059444\n",
      "[27/100, 300/478] loss: 0.60594434\n",
      "[27/100, 400/478] loss: 0.5237878\n",
      "The Current Loss: 0.55544489081949\n",
      "Trigger Times: 1\n",
      "[28/100, 0/478] loss: 0.6059444\n",
      "[28/100, 100/478] loss: 0.52378809\n",
      "[28/100, 200/478] loss: 0.6059444\n",
      "[28/100, 300/478] loss: 0.42754757\n",
      "[28/100, 400/478] loss: 0.5237878\n",
      "The Current Loss: 0.552142507955432\n",
      "trigger times: 0\n",
      "[29/100, 0/478] loss: 0.5237878\n",
      "[29/100, 100/478] loss: 0.52378786\n",
      "[29/100, 200/478] loss: 0.42754757\n",
      "[29/100, 300/478] loss: 0.42754754\n",
      "[29/100, 400/478] loss: 0.42754751\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 0.004\n",
    "model = MLP(\n",
    "    paramDict['input_size'],\n",
    "    paramDict['hidden_size'],\n",
    "    paramDict['output_size'],\n",
    "    paramDict['dropout']\n",
    ").to(device)\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model = traindata(device, model, epochs, optimizer, paramDict['loss'], trainloader, validloader, paramDict['patience'])\n",
    "\n",
    "# Test\n",
    "acc, mcc, report = test(device, model, testloader)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "print('MCC: %.3f' % mcc)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.utils.data as data_utils\n",
    "# import src.encoding as enc\n",
    "# import os\n",
    "# from torch import nn\n",
    "# from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without using descriptors and using one hot encoding, all sequences need to have the same length. So, some sequences will be truncated or padded with Ns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fps_x = dataset['sequence'].values\n",
    "# fps_y = dataset['label'].values\n",
    "\n",
    "# average_length = int(average_length)\n",
    "\n",
    "# cut sequences to the average length\n",
    "# seqs_dataset = seqs_dataset.str.slice(0, average_length)\n",
    "\n",
    "# fill with \"N\" the sequences that are shorter than average length\n",
    "# seqs_dataset = seqs_dataset.str.pad(average_length, side='right', fillchar='N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need now to split the dataset into training, test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, x_test, y, y_test = train_test_split(\n",
    "#     fps_x, fps_y,\n",
    "#     test_size=0.2,\n",
    "#     train_size=0.8,\n",
    "#     stratify=fps_y\n",
    "# )\n",
    "# x_train, x_cv, y_train, y_cv = train_test_split(\n",
    "#     x, y,\n",
    "#     test_size=0.25,\n",
    "#     train_size=0.75,\n",
    "#     stratify=y\n",
    "# )\n",
    "\n",
    "# print(fps_x.shape)\n",
    "# print(fps_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to one hot encode the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_enc = enc.DNAEncoding(x_train)\n",
    "# x_train = x_train_enc.one_hot_encode()\n",
    "\n",
    "# x_test_enc = enc.DNAEncoding(x_test)\n",
    "# x_test = x_test_enc.one_hot_encode()\n",
    "\n",
    "# x_cv_enc = enc.DNAEncoding(x_cv)\n",
    "# x_cv = x_cv_enc.one_hot_encode()\n",
    "\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(y_test.shape)\n",
    "# print(x_cv.shape)\n",
    "# print(y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert to torch.tensor\n",
    "# train_data = data_utils.TensorDataset(\n",
    "#     torch.tensor(x_train, dtype=torch.float),\n",
    "#     torch.tensor(y_train, dtype=torch.long)\n",
    "# )\n",
    "# test_data = data_utils.TensorDataset(\n",
    "#     torch.tensor(x_test, dtype=torch.float),\n",
    "#     torch.tensor(y_test, dtype=torch.long)\n",
    "# )\n",
    "# valid_data = data_utils.TensorDataset(\n",
    "#     torch.tensor(x_cv, dtype=torch.float),\n",
    "#     torch.tensor(y_cv, dtype=torch.long)\n",
    "# )\n",
    "\n",
    "# batch_size = 16\n",
    "\n",
    "# # Data loader\n",
    "# trainloader = data_utils.DataLoader(\n",
    "#     train_data,\n",
    "#     shuffle=True,\n",
    "#     batch_size=batch_size\n",
    "# )\n",
    "# testloader = data_utils.DataLoader(\n",
    "#     test_data,\n",
    "#     shuffle=True,\n",
    "#     batch_size=batch_size\n",
    "# )\n",
    "# validloader = data_utils.DataLoader(\n",
    "#     valid_data,\n",
    "#     shuffle=True,\n",
    "#     batch_size=batch_size\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model equivalent to the one in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.models import MLP\n",
    "# from src.train import traindata\n",
    "# from src.test import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(2022)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4,5'\n",
    "# device = torch.device('cuda:0')\n",
    "\n",
    "# model = MLP().to(device)\n",
    "\n",
    "# paramDict = {\n",
    "#     'epoch': 200,\n",
    "#     'batchSize': 32,\n",
    "#     'dropOut': 0.2,\n",
    "#     'learning_rate': 0.004,\n",
    "#     'loss': nn.CrossEntropyLoss(),\n",
    "#     'metrics': ['accuracy'],\n",
    "#     'activation1': 'relu',\n",
    "#     'activation2': 'sigmoid',\n",
    "#     'monitor': 'val_accuracy',\n",
    "#     'save_best_only': True,\n",
    "#     'mode': 'max'\n",
    "# }\n",
    "\n",
    "# class_weight = {0: 1.0, 1: 4.0}\n",
    "\n",
    "# optimizerDict = {\n",
    "#     'adam': Adam(model.parameters(), learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # epochs = 100\n",
    "# # lr = 0.004\n",
    "# # loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# #optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# model = traindata(device, model, paramDict['epochs'], optimizerDict['adam'], paramDict['loss'], trainloader, validloader)\n",
    "\n",
    "# acc, mcc, report = test(device, model, testloader)\n",
    "# print('Accuracy: %.3f' % acc)\n",
    "# print('MCC: %.3f' % mcc)\n",
    "# print(report)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba449ea13c29f64a91968d8f927cecceedd6e605eda30388903386e6cd94168d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dna-conda': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
