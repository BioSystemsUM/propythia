{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accurately predicting human essential genes based on deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will present a comparative analysis to demonstrate the application and performance of PyTorch models for addressing sequence-based prediction problems.\n",
    "\n",
    "We'll try to replicate the [DeepHE: Accurately predicting human essential genes based on deep learning](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008229) deep learning model and evaluate its performance. Other models will be compared to the DeepHE model.\n",
    "\n",
    "DeepHE's model is based on the multilayer perceptron structure. It includes one input layer, three hidden layers, and one output layer. All the hidden layers utilize the ReLU activation function. The output layer uses sigmoid activation function to perform discrete classification. The loss function in DeepHE is binary cross-entropy. A dropout layer is used after each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sys.path.append('../../../../src/')\n",
    "from propythia.shallow_ml import ShallowML\n",
    "\n",
    "from descriptors import DNADescriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using DNA descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was already built and preprocessed in the \"essential_genes.ipynb\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_x = pd.read_csv(\"fps_x.csv\")\n",
    "print(fps_x.shape)\n",
    "print(fps_x.head())\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "fps_y = pd.read_csv(\"fps_y.csv\")\n",
    "print(fps_x.shape)\n",
    "print(fps_x.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, this dataset contains the sequence and the corresponding positive/negative class labels, with positive class labels corresponding to the presence of a essential gene. The amount of positive and negative examples is NOT evenly distributed across the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of each class\n",
    "fps_y.groupby('label').size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need now to split the dataset into training, test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(fps_x, fps_y, stratify=fps_y)\n",
    "\n",
    "scaler = StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data_utils\n",
    "import src.encoding as enc\n",
    "import os\n",
    "from torch import nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps_x = pd.read_csv(\"fps_x.csv\")\n",
    "print(fps_x.shape)\n",
    "print(fps_x.head())\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "fps_y = pd.read_csv(\"fps_y.csv\")\n",
    "print(fps_x.shape)\n",
    "print(fps_x.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of each class\n",
    "fps_y.groupby('label').size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need now to split the dataset into training, test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_test, y, y_test = train_test_split(\n",
    "    fps_x, fps_y,\n",
    "    test_size=0.2,\n",
    "    train_size=0.8,\n",
    "    stratify=fps_y\n",
    ")\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(\n",
    "    x, y,\n",
    "    test_size=0.25,\n",
    "    train_size=0.75,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to one hot encode the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_enc = enc.DNAEncoding(x_train)\n",
    "x_train = x_train_enc.one_hot_encode()\n",
    "\n",
    "x_test_enc = enc.DNAEncoding(x_test)\n",
    "x_test = x_test_enc.one_hot_encode()\n",
    "\n",
    "x_cv_enc = enc.DNAEncoding(x_cv)\n",
    "x_cv = x_cv_enc.one_hot_encode()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(x_cv.shape)\n",
    "print(y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to torch.tensor\n",
    "train_data = data_utils.TensorDataset(\n",
    "    torch.tensor(x_train, dtype=torch.float),\n",
    "    torch.tensor(y_train, dtype=torch.long)\n",
    ")\n",
    "test_data = data_utils.TensorDataset(\n",
    "    torch.tensor(x_test, dtype=torch.float),\n",
    "    torch.tensor(y_test, dtype=torch.long)\n",
    ")\n",
    "valid_data = data_utils.TensorDataset(\n",
    "    torch.tensor(x_cv, dtype=torch.float),\n",
    "    torch.tensor(y_cv, dtype=torch.long)\n",
    ")\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Data loader\n",
    "trainloader = data_utils.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "testloader = data_utils.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "validloader = data_utils.DataLoader(\n",
    "    valid_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model equivalent to the one in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import Net\n",
    "from src.train import traindata\n",
    "from src.test import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2022)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4,5'\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "paramDict = {\n",
    "    'epoch': 200,\n",
    "    'batchSize': 32,\n",
    "    'dropOut': 0.2,\n",
    "    'learning_rate': 0.004,\n",
    "    'loss': nn.CrossEntropyLoss(),\n",
    "    'metrics': ['accuracy'],\n",
    "    'activation1': 'relu',\n",
    "    'activation2': 'sigmoid',\n",
    "    'monitor': 'val_accuracy',\n",
    "    'save_best_only': True,\n",
    "    'mode': 'max'\n",
    "}\n",
    "\n",
    "class_weight = {0: 1.0, 1: 4.0}\n",
    "\n",
    "optimizerDict = {\n",
    "    'adam': Adam(model.parameters(), learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "}\n",
    "\n",
    "# epochs = 100\n",
    "# lr = 0.004\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "model = traindata(device, model, paramDict['epochs'], optimizerDict['adam'], paramDict['loss'], trainloader, validloader)\n",
    "\n",
    "# Test\n",
    "acc, mcc, report = test(device, model, testloader)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "print('MCC: %.3f' % mcc)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
