{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accurately predicting human essential genes based on deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will present a comparative analysis to demonstrate the application and performance of PyTorch models for addressing sequence-based prediction problems.\n",
    "\n",
    "We'll try to replicate the [DeepHE: Accurately predicting human essential genes based on deep learning](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008229) deep learning model and evaluate its performance. Other models will be compared to the DeepHE model.\n",
    "\n",
    "DeepHE's model is based on the multilayer perceptron structure. It includes one input layer, three hidden layers, and one output layer. All the hidden layers utilize the ReLU activation function. The output layer uses sigmoid activation function to perform discrete classification. The loss function in DeepHE is binary cross-entropy. A dropout layer is used after each hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, there is already 2 cleaned datasets which will be used in this notebook.\n",
    "- `deg.csv`: 16 datasets grouped that contain essential genes of the human genome. Each sequence has an EMBL id associated, the original dataset it came from, among other information. [Link](http://origin.tubic.org/deg/public/index.php)\n",
    "- `negative.csv`: contains the genome DNA sequences of humans for all annotated genes from Ensembl. Each sequence has an EMBL id associated. [Link](http://www.ensembl.org/Homo_sapiens/Info/Index)\n",
    "\n",
    "The process of cleaning each dataset is described below:\n",
    "- `deg.csv`:\n",
    "    - removed rows with unavailable sequences.\n",
    "- `negative.csv`:\n",
    "    - removed all rows which sequences belonged to the `deg.csv` dataset.\n",
    "    - removed all rows which EMBL id was in the `deg.csv` dataset.\n",
    "    - grouped all sequences with the same EMBL id, and kept only the first two.\n",
    "\n",
    "One of the tasks in this notebook is to also build the positive dataset (`essential_genes_positve.csv`), which will contain only the sequences that are in at least 5 different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26791, 15)\n",
      "(27684, 2)\n"
     ]
    }
   ],
   "source": [
    "deg_dataset = pd.read_csv(\"datasets/essential_genes/deg.csv\", sep=';')\n",
    "print(deg_dataset.shape)\n",
    "\n",
    "eg_negative = pd.read_csv(\"datasets/essential_genes/negative.csv\", sep=',')\n",
    "print(eg_negative.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating positive essential genes dataset. Each sequence needs to be in at least 5 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     id                                           sequence\n",
      "0  GI:-  ATGGTGCTGTCCCAGAGACAACGAGATGAACTAAATCGAGCTATAG...\n",
      "1  GI:-  ATGGCTGCAGCTTCATATGATCAGTTGTTAAAGCAAGTTGAGGCAC...\n",
      "2  GI:-  ATGAGCCGCCTGCTCTGGAGGAAGGTGGCCGGCGCCACCGTCGGGC...\n",
      "3  GI:-  ATGCAGAGCTGGAGTCGTGTGTACTGCTCCTTGGCCAAGAGAGGCC...\n",
      "4  GI:-  ATGGTTGGCTATGACCCCAAACCAGATGGCAGGAATAACACCAAGT...\n",
      "(2010, 2)\n"
     ]
    }
   ],
   "source": [
    "# for each sequence, get all the datasets that contain it\n",
    "d = {}\n",
    "for _, row in deg_dataset.iterrows():\n",
    "    if(row[\"sequence\"] in d):\n",
    "        d[row[\"sequence\"]].append((row[\"id1\"], row[\"id4\"]))\n",
    "    else:\n",
    "        d[row[\"sequence\"]] = [(row[\"id1\"], row[\"id4\"])]\n",
    "\n",
    "\n",
    "# get a list of sequences that are in more than 5 datasets\n",
    "essential_sequences = []\n",
    "for key, val in d.items():\n",
    "    if(len(val) >= 5):\n",
    "        essential_sequences.append((val[0][1], key))\n",
    "        \n",
    "# create dataframe with essential sequences\n",
    "eg_positive = pd.DataFrame(essential_sequences, columns=[\"id\", \"sequence\"])\n",
    "print(eg_positive.head())\n",
    "print(eg_positive.shape)\n",
    "\n",
    "# write to csv\n",
    "eg_positive.to_csv(\"datasets/essential_genes/positive.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique positive sequences: 2010\n",
      "unique negative sequences: 23443\n"
     ]
    }
   ],
   "source": [
    "print(\"unique positive sequences:\", len(set(eg_positive[\"sequence\"])))\n",
    "print(\"unique negative sequences:\", len(set(eg_negative[\"sequence\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the negative dataset has 27684 sequences, not all of them are unique. So, we need to remove the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23443, 2)\n"
     ]
    }
   ],
   "source": [
    "eg_negative = eg_negative.drop_duplicates(subset=\"sequence\")\n",
    "print(eg_negative.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have both positive and negative datasets:\n",
    "* eg_positive (2010,  2) -> positive dataset with essential genes \n",
    "* eg_negative (23443, 2) -> negative dataset with non essential genes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating length of each sequence and cleaning up the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_positive[\"length\"] = eg_positive[\"sequence\"].str.len()\n",
    "eg_negative[\"length\"] = eg_negative[\"sequence\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before removing\n",
      "(2010, 3)\n",
      "(23443, 3)\n",
      "after removing\n",
      "(2010, 3) 0 were removed\n",
      "(23439, 3) 4 were removed\n"
     ]
    }
   ],
   "source": [
    "print(\"before removing\")\n",
    "print(eg_positive.shape)\n",
    "print(eg_negative.shape)\n",
    "\n",
    "old_eg_positive = eg_positive.shape[0]\n",
    "old_eg_negative = eg_negative.shape[0]\n",
    "\n",
    "eg_positive = eg_positive[eg_positive[\"length\"] <= 20000]\n",
    "eg_negative = eg_negative[eg_negative[\"length\"] <= 20000]\n",
    "print(\"after removing\")\n",
    "print(eg_positive.shape, old_eg_positive - eg_positive.shape[0], \"were removed\")\n",
    "print(eg_negative.shape, old_eg_negative - eg_negative.shape[0], \"were removed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the positive and negative datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25449, 3)\n",
      "     id                                           sequence  label\n",
      "0  GI:-  ATGGTGCTGTCCCAGAGACAACGAGATGAACTAAATCGAGCTATAG...      1\n",
      "1  GI:-  ATGGCTGCAGCTTCATATGATCAGTTGTTAAAGCAAGTTGAGGCAC...      1\n",
      "2  GI:-  ATGAGCCGCCTGCTCTGGAGGAAGGTGGCCGGCGCCACCGTCGGGC...      1\n",
      "3  GI:-  ATGCAGAGCTGGAGTCGTGTGTACTGCTCCTTGGCCAAGAGAGGCC...      1\n",
      "4  GI:-  ATGGTTGGCTATGACCCCAAACCAGATGGCAGGAATAACACCAAGT...      1\n"
     ]
    }
   ],
   "source": [
    "# adding labels to the dataset\n",
    "eg_positive[\"label\"] = 1\n",
    "eg_negative[\"label\"] = 0\n",
    "\n",
    "# removing length column\n",
    "eg_positive = eg_positive.drop(columns=[\"length\"])\n",
    "eg_negative = eg_negative.drop(columns=[\"length\"])\n",
    "\n",
    "# joining the two datasets\n",
    "dataset = pd.concat([eg_positive, eg_negative])\n",
    "print(dataset.shape)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25449, 3)\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label'>"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of each class\n",
    "dataset.groupby('label').size().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using DNA descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "\n",
    "sys.path.append('../../../../src/')\n",
    "from propythia.DNA.descriptors.descriptors import DNADescriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature(data):\n",
    "    list_feature = []\n",
    "    count = 0\n",
    "    for seq in data['sequence']:\n",
    "        res = {'sequence': seq}\n",
    "        dna = DNADescriptor(seq)\n",
    "        feature = dna.get_descriptors()\n",
    "        res.update(feature)\n",
    "        list_feature.append(res)\n",
    "        # print progress every 100 sequences\n",
    "        if count % 100 == 0:\n",
    "            print(count, '/', len(data))\n",
    "\n",
    "        count += 1\n",
    "    print(\"Done!\")\n",
    "    df = pd.DataFrame(list_feature)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip the calculation of descriptors if `features.pkl` exists which already has them calculated. Skip all of this if `fps_x.pkl` exists because it already has the features calculated and **normalized**. The need of data normalization is explained in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features already calculated and normalized\n"
     ]
    }
   ],
   "source": [
    "if exists(\"datasets/essential_genes/fps_x.pkl\") == False:\n",
    "    if exists(\"datasets/essential_genes/features.pkl\"):\n",
    "        with open(\"datasets/essential_genes/features.pkl\", \"rb\") as f:\n",
    "            features = pickle.load(f)\n",
    "        print(\"Features loaded from pickle file\")\n",
    "    else:\n",
    "        features = calculate_feature(dataset)\n",
    "        with open(\"datasets/essential_genes/features.pkl\", \"wb\") as f:\n",
    "            pickle.dump(features, f)\n",
    "else:\n",
    "    print(\"Features already calculated and normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to help normalize the data.\n",
    "\n",
    "Without being normalized, we have a dataset with 17 columns. Each column is a result of a DNA descriptor, and some of these columns are numbers, dicts and even lists.\n",
    "\n",
    "We still need to normalize those who have dictionaries and lists because the model can't handle data in these types.\n",
    "\n",
    "To normalize the data, dicts and lists need to \"explode\" into more columns. \n",
    "\n",
    "E.g. dicts:\n",
    "\n",
    "| descriptor_hello |\n",
    "| ---------------- |\n",
    "| {'a': 1, 'b': 2} |\n",
    "\n",
    "will be transformed into:\n",
    "\n",
    "| descriptor_hello_a | descriptor_hello_b |\n",
    "| ------------------ | ------------------ |\n",
    "| 1                  | 2                  |\n",
    "\n",
    "E.g. lists:\n",
    "\n",
    "| descriptor_hello |\n",
    "| ---------------- |\n",
    "| [1, 2, 3]        |\n",
    "\n",
    "will be transformed into:\n",
    "\n",
    "| descriptor_hello_0 | descriptor_hello_1 | descriptor_hello_2 |\n",
    "| ------------------ | ------------------ | ------------------ |\n",
    "| 1                  | 2                  | 3                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lists(fps_x, field):\n",
    "    l = fps_x[field].to_list()\n",
    "    new_df = pd.DataFrame(l)\n",
    "    new_df.columns = [str(field) + \"_\" + str(i) for i in new_df.columns]\n",
    "    fps_x.drop(field, axis=1, inplace=True)\n",
    "    return new_df\n",
    "\n",
    "def process_lists_of_lists(fps_x, field):\n",
    "    l = fps_x[field].to_list()\n",
    "    new_df = pd.DataFrame(l)\n",
    "    new_df.columns = [str(field) + \"_\" + str(i) for i in new_df.columns]\n",
    "    empty_val = {} if field == \"enhanced_nucleic_acid_composition\" else []\n",
    "    small_processed = []\n",
    "    for f in new_df.columns:\n",
    "        col = [empty_val if i is None else i for i in new_df[f].to_list()]\n",
    "        sub = pd.DataFrame(col)\n",
    "        sub.columns = [str(f) + \"_\" + str(i) for i in sub.columns]\n",
    "        small_processed.append(sub)\n",
    "    fps_x.drop(field, axis=1, inplace=True)\n",
    "    return small_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features():\n",
    "    fps_y = dataset['label']\n",
    "    fps_x = features.loc[:, features.columns != 'label']\n",
    "    fps_x = fps_x.loc[:, fps_x.columns != 'sequence']\n",
    "    print(fps_x.shape)\n",
    "    \n",
    "    lists = [\"nucleic_acid_composition\",\"dinucleotide_composition\",\"trinucleotide_composition\",\"k_spaced_nucleic_acid_pairs\",\"kmer\",\"PseDNC\", \"PseKNC\", \"DAC\", \"DCC\", \"DACC\", \"TAC\",\"TCC\",\"TACC\"]\n",
    "    lists_of_lists = [\n",
    "        \"accumulated_nucleotide_frequency\"\n",
    "    ]\n",
    "\n",
    "    small_processed = []\n",
    "    for i in lists:\n",
    "        new_df = process_lists(fps_x, i)\n",
    "        small_processed.append(new_df)\n",
    "        \n",
    "    for i in lists_of_lists:\n",
    "        smaller_processed = process_lists_of_lists(fps_x, i)\n",
    "        small_processed += smaller_processed\n",
    "\n",
    "    # concat final with original\n",
    "    fps_x = pd.concat([fps_x, *small_processed], axis=1)\n",
    "\n",
    "    with open(\"datasets/essential_genes/fps_x.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fps_x, f)\n",
    "        \n",
    "    with open(\"datasets/essential_genes/fps_y.pkl\", \"wb\") as f:\n",
    "        pickle.dump(fps_y, f)\n",
    "    \n",
    "    return fps_x, fps_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip the data normalization if it was already performed (`fps_x.pkl` exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features loaded from pickle file\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>gc_content</th>\n",
       "      <th>at_content</th>\n",
       "      <th>nucleic_acid_composition_A</th>\n",
       "      <th>nucleic_acid_composition_C</th>\n",
       "      <th>nucleic_acid_composition_G</th>\n",
       "      <th>nucleic_acid_composition_T</th>\n",
       "      <th>dinucleotide_composition_AA</th>\n",
       "      <th>dinucleotide_composition_AC</th>\n",
       "      <th>dinucleotide_composition_AG</th>\n",
       "      <th>...</th>\n",
       "      <th>accumulated_nucleotide_frequency_0_G</th>\n",
       "      <th>accumulated_nucleotide_frequency_0_T</th>\n",
       "      <th>accumulated_nucleotide_frequency_1_A</th>\n",
       "      <th>accumulated_nucleotide_frequency_1_C</th>\n",
       "      <th>accumulated_nucleotide_frequency_1_G</th>\n",
       "      <th>accumulated_nucleotide_frequency_1_T</th>\n",
       "      <th>accumulated_nucleotide_frequency_2_A</th>\n",
       "      <th>accumulated_nucleotide_frequency_2_C</th>\n",
       "      <th>accumulated_nucleotide_frequency_2_G</th>\n",
       "      <th>accumulated_nucleotide_frequency_2_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1233</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8532</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.336</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3720</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1530</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.246</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>963</td>\n",
       "      <td>0.559</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.224</td>\n",
       "      <td>0.199</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25444</th>\n",
       "      <td>576</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25445</th>\n",
       "      <td>576</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25446</th>\n",
       "      <td>3363</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.606</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25447</th>\n",
       "      <td>1101</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.340</td>\n",
       "      <td>0.142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25448</th>\n",
       "      <td>96</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.302</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25449 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       length  gc_content  at_content  nucleic_acid_composition_A  \\\n",
       "0        1233       0.440       0.560                       0.303   \n",
       "1        8532       0.412       0.588                       0.344   \n",
       "2        3720       0.604       0.396                       0.215   \n",
       "3        1530       0.414       0.586                       0.303   \n",
       "4         963       0.559       0.441                       0.212   \n",
       "...       ...         ...         ...                         ...   \n",
       "25444     576       0.439       0.561                       0.288   \n",
       "25445     576       0.434       0.566                       0.286   \n",
       "25446    3363       0.394       0.606                       0.338   \n",
       "25447    1101       0.633       0.367                       0.227   \n",
       "25448      96       0.635       0.365                       0.146   \n",
       "\n",
       "       nucleic_acid_composition_C  nucleic_acid_composition_G  \\\n",
       "0                           0.193                       0.247   \n",
       "1                           0.208                       0.205   \n",
       "2                           0.290                       0.314   \n",
       "3                           0.169                       0.246   \n",
       "4                           0.286                       0.273   \n",
       "...                           ...                         ...   \n",
       "25444                       0.210                       0.229   \n",
       "25445                       0.212                       0.222   \n",
       "25446                       0.192                       0.202   \n",
       "25447                       0.292                       0.341   \n",
       "25448                       0.302                       0.333   \n",
       "\n",
       "       nucleic_acid_composition_T  dinucleotide_composition_AA  \\\n",
       "0                           0.256                        0.096   \n",
       "1                           0.244                        0.126   \n",
       "2                           0.181                        0.041   \n",
       "3                           0.282                        0.099   \n",
       "4                           0.229                        0.048   \n",
       "...                           ...                          ...   \n",
       "25444                       0.273                        0.089   \n",
       "25445                       0.280                        0.089   \n",
       "25446                       0.268                        0.122   \n",
       "25447                       0.140                        0.037   \n",
       "25448                       0.219                        0.011   \n",
       "\n",
       "       dinucleotide_composition_AC  dinucleotide_composition_AG  ...  \\\n",
       "0                            0.052                        0.075  ...   \n",
       "1                            0.054                        0.089  ...   \n",
       "2                            0.050                        0.089  ...   \n",
       "3                            0.044                        0.080  ...   \n",
       "4                            0.045                        0.076  ...   \n",
       "...                            ...                          ...  ...   \n",
       "25444                        0.068                        0.068  ...   \n",
       "25445                        0.068                        0.066  ...   \n",
       "25446                        0.055                        0.076  ...   \n",
       "25447                        0.056                        0.105  ...   \n",
       "25448                        0.042                        0.042  ...   \n",
       "\n",
       "       accumulated_nucleotide_frequency_0_G  \\\n",
       "0                                     0.224   \n",
       "1                                     0.240   \n",
       "2                                     0.340   \n",
       "3                                     0.248   \n",
       "4                                     0.282   \n",
       "...                                     ...   \n",
       "25444                                 0.229   \n",
       "25445                                 0.229   \n",
       "25446                                 0.193   \n",
       "25447                                 0.295   \n",
       "25448                                 0.333   \n",
       "\n",
       "       accumulated_nucleotide_frequency_0_T  \\\n",
       "0                                     0.247   \n",
       "1                                     0.250   \n",
       "2                                     0.153   \n",
       "3                                     0.290   \n",
       "4                                     0.224   \n",
       "...                                     ...   \n",
       "25444                                 0.312   \n",
       "25445                                 0.312   \n",
       "25446                                 0.262   \n",
       "25447                                 0.175   \n",
       "25448                                 0.333   \n",
       "\n",
       "       accumulated_nucleotide_frequency_1_A  \\\n",
       "0                                     0.324   \n",
       "1                                     0.336   \n",
       "2                                     0.205   \n",
       "3                                     0.298   \n",
       "4                                     0.199   \n",
       "...                                     ...   \n",
       "25444                                 0.292   \n",
       "25445                                 0.292   \n",
       "25446                                 0.353   \n",
       "25447                                 0.227   \n",
       "25448                                 0.125   \n",
       "\n",
       "       accumulated_nucleotide_frequency_1_C  \\\n",
       "0                                     0.182   \n",
       "1                                     0.193   \n",
       "2                                     0.311   \n",
       "3                                     0.163   \n",
       "4                                     0.286   \n",
       "...                                     ...   \n",
       "25444                                 0.181   \n",
       "25445                                 0.181   \n",
       "25446                                 0.190   \n",
       "25447                                 0.292   \n",
       "25448                                 0.271   \n",
       "\n",
       "       accumulated_nucleotide_frequency_1_G  \\\n",
       "0                                     0.227   \n",
       "1                                     0.220   \n",
       "2                                     0.322   \n",
       "3                                     0.247   \n",
       "4                                     0.278   \n",
       "...                                     ...   \n",
       "25444                                 0.215   \n",
       "25445                                 0.215   \n",
       "25446                                 0.196   \n",
       "25447                                 0.323   \n",
       "25448                                 0.375   \n",
       "\n",
       "       accumulated_nucleotide_frequency_1_T  \\\n",
       "0                                     0.267   \n",
       "1                                     0.251   \n",
       "2                                     0.162   \n",
       "3                                     0.292   \n",
       "4                                     0.237   \n",
       "...                                     ...   \n",
       "25444                                 0.312   \n",
       "25445                                 0.312   \n",
       "25446                                 0.262   \n",
       "25447                                 0.158   \n",
       "25448                                 0.229   \n",
       "\n",
       "       accumulated_nucleotide_frequency_2_A  \\\n",
       "0                                     0.321   \n",
       "1                                     0.345   \n",
       "2                                     0.208   \n",
       "3                                     0.301   \n",
       "4                                     0.213   \n",
       "...                                     ...   \n",
       "25444                                 0.296   \n",
       "25445                                 0.296   \n",
       "25446                                 0.351   \n",
       "25447                                 0.225   \n",
       "25448                                 0.125   \n",
       "\n",
       "       accumulated_nucleotide_frequency_2_C  \\\n",
       "0                                     0.188   \n",
       "1                                     0.198   \n",
       "2                                     0.303   \n",
       "3                                     0.164   \n",
       "4                                     0.289   \n",
       "...                                     ...   \n",
       "25444                                 0.206   \n",
       "25445                                 0.206   \n",
       "25446                                 0.187   \n",
       "25447                                 0.293   \n",
       "25448                                 0.292   \n",
       "\n",
       "       accumulated_nucleotide_frequency_2_G  \\\n",
       "0                                     0.240   \n",
       "1                                     0.211   \n",
       "2                                     0.319   \n",
       "3                                     0.244   \n",
       "4                                     0.267   \n",
       "...                                     ...   \n",
       "25444                                 0.208   \n",
       "25445                                 0.208   \n",
       "25446                                 0.198   \n",
       "25447                                 0.340   \n",
       "25448                                 0.361   \n",
       "\n",
       "       accumulated_nucleotide_frequency_2_T  \n",
       "0                                     0.251  \n",
       "1                                     0.246  \n",
       "2                                     0.171  \n",
       "3                                     0.291  \n",
       "4                                     0.230  \n",
       "...                                     ...  \n",
       "25444                                 0.289  \n",
       "25445                                 0.289  \n",
       "25446                                 0.264  \n",
       "25447                                 0.142  \n",
       "25448                                 0.222  \n",
       "\n",
       "[25449 rows x 247 columns]"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if exists(\"datasets/essential_genes/fps_x.pkl\"):\n",
    "    with open(\"datasets/essential_genes/fps_x.pkl\", \"rb\") as f:\n",
    "        fps_x = pickle.load(f)\n",
    "    with open(\"datasets/essential_genes/fps_y.pkl\", \"rb\") as f:\n",
    "        fps_y = pickle.load(f)\n",
    "    print(\"Features loaded from pickle file\")\n",
    "else:\n",
    "    fps_x, fps_y = normalize_features()\n",
    "fps_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.models import MLP\n",
    "from src.train import traindata\n",
    "from src.test import test\n",
    "from src.prepare_data import prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model and evaluating the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (15269, 247)\n",
      "y_train.shape: (15269,)\n",
      "x_test.shape: (5090, 247)\n",
      "y_test.shape: (5090,)\n",
      "x_cv.shape: (5090, 247)\n",
      "y_cv.shape: (5090,)\n",
      "[1/50, 0/478] loss: 0.69612831\n",
      "[1/50, 100/478] loss: 0.54193211\n",
      "[1/50, 200/478] loss: 0.50137359\n",
      "[1/50, 300/478] loss: 0.59616804\n",
      "[1/50, 400/478] loss: 0.56707215\n",
      "The Current Loss: 0.5241906262934208\n",
      "trigger times: 0\n",
      "[2/50, 0/478] loss: 0.36132064\n",
      "[2/50, 100/478] loss: 0.4450371\n",
      "[2/50, 200/478] loss: 0.72435057\n",
      "[2/50, 300/478] loss: 0.46684515\n",
      "[2/50, 400/478] loss: 0.57613087\n",
      "The Current Loss: 0.5171113163232803\n",
      "trigger times: 0\n",
      "[3/50, 0/478] loss: 0.43871167\n",
      "[3/50, 100/478] loss: 0.57861805\n",
      "[3/50, 200/478] loss: 0.53417456\n",
      "[3/50, 300/478] loss: 0.57263094\n",
      "[3/50, 400/478] loss: 0.64933813\n",
      "The Current Loss: 0.5355154132470489\n",
      "Trigger Times: 1\n",
      "[4/50, 0/478] loss: 0.34381378\n",
      "[4/50, 100/478] loss: 0.46595493\n",
      "[4/50, 200/478] loss: 0.61688215\n",
      "[4/50, 300/478] loss: 0.63536489\n",
      "[4/50, 400/478] loss: 0.52198923\n",
      "The Current Loss: 0.5150763653218746\n",
      "trigger times: 0\n",
      "[5/50, 0/478] loss: 0.47885022\n",
      "[5/50, 100/478] loss: 0.42489734\n",
      "[5/50, 200/478] loss: 0.69674814\n",
      "[5/50, 300/478] loss: 0.58668274\n",
      "[5/50, 400/478] loss: 0.49397486\n",
      "The Current Loss: 0.5138080015778541\n",
      "trigger times: 0\n",
      "[6/50, 0/478] loss: 0.50844824\n",
      "[6/50, 100/478] loss: 0.41496089\n",
      "[6/50, 200/478] loss: 0.47786102\n",
      "[6/50, 300/478] loss: 0.52888298\n",
      "[6/50, 400/478] loss: 0.41874072\n",
      "The Current Loss: 0.5239676794037222\n",
      "Trigger Times: 1\n",
      "[7/50, 0/478] loss: 0.5819366\n",
      "[7/50, 100/478] loss: 0.35431111\n",
      "[7/50, 200/478] loss: 0.59691989\n",
      "[7/50, 300/478] loss: 0.57670254\n",
      "[7/50, 400/478] loss: 0.34675765\n",
      "The Current Loss: 0.518371845036745\n",
      "trigger times: 0\n",
      "[8/50, 0/478] loss: 0.65901506\n",
      "[8/50, 100/478] loss: 0.45547906\n",
      "[8/50, 200/478] loss: 0.46256587\n",
      "[8/50, 300/478] loss: 0.46646586\n",
      "[8/50, 400/478] loss: 0.49536332\n",
      "The Current Loss: 0.5174870898947119\n",
      "trigger times: 0\n",
      "[9/50, 0/478] loss: 0.49260366\n",
      "[9/50, 100/478] loss: 0.3671827\n",
      "[9/50, 200/478] loss: 0.43615642\n",
      "[9/50, 300/478] loss: 0.52409339\n",
      "[9/50, 400/478] loss: 0.63147634\n",
      "The Current Loss: 0.521808267943561\n",
      "Trigger Times: 1\n",
      "[10/50, 0/478] loss: 0.47375602\n",
      "[10/50, 100/478] loss: 0.49471101\n",
      "[10/50, 200/478] loss: 0.57264429\n",
      "[10/50, 300/478] loss: 0.49944431\n",
      "[10/50, 400/478] loss: 0.43835294\n",
      "The Current Loss: 0.5176209915429354\n",
      "trigger times: 0\n",
      "[11/50, 0/478] loss: 0.58896202\n",
      "[11/50, 100/478] loss: 0.43791404\n",
      "[11/50, 200/478] loss: 0.37647036\n",
      "[11/50, 300/478] loss: 0.36201996\n",
      "[11/50, 400/478] loss: 0.37263986\n",
      "The Current Loss: 0.5160542819648981\n",
      "trigger times: 0\n",
      "[12/50, 0/478] loss: 0.54030257\n",
      "[12/50, 100/478] loss: 0.57695121\n",
      "[12/50, 200/478] loss: 0.71707463\n",
      "[12/50, 300/478] loss: 0.447391\n",
      "[12/50, 400/478] loss: 0.67328429\n",
      "The Current Loss: 0.5211437754333019\n",
      "Trigger Times: 1\n",
      "[13/50, 0/478] loss: 0.44152045\n",
      "[13/50, 100/478] loss: 0.35053709\n",
      "[13/50, 200/478] loss: 0.47896492\n",
      "[13/50, 300/478] loss: 0.33888412\n",
      "[13/50, 400/478] loss: 0.39135811\n",
      "The Current Loss: 0.5146624922752381\n",
      "trigger times: 0\n",
      "[14/50, 0/478] loss: 0.49797302\n",
      "[14/50, 100/478] loss: 0.51337135\n",
      "[14/50, 200/478] loss: 0.65176278\n",
      "[14/50, 300/478] loss: 0.45906666\n",
      "[14/50, 400/478] loss: 0.56785375\n",
      "The Current Loss: 0.5160138230770827\n",
      "Trigger Times: 1\n",
      "[15/50, 0/478] loss: 0.49714315\n",
      "[15/50, 100/478] loss: 0.50910878\n",
      "[15/50, 200/478] loss: 0.44257978\n",
      "[15/50, 300/478] loss: 0.3824963\n",
      "[15/50, 400/478] loss: 0.60424078\n",
      "The Current Loss: 0.5221985638141632\n",
      "Trigger Times: 2\n",
      "Early stopping!\n",
      "Start to test process.\n",
      "Accuracy: 0.899\n",
      "MCC: 0.192\n",
      "[[4492  196]\n",
      " [ 320   82]]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2022)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4,5'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "epochs = 50\n",
    "optimizer_label = 'adam'\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 4.0]).to(device))\n",
    "patience = 2 # patience for early stopping\n",
    "input_size = fps_x.shape[1]\n",
    "hidden_size = 128 # devia estar na config\n",
    "dropout = 0.2\n",
    "output_size = 2\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# these are hyperparameters to be tuned\n",
    "config = {\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 32,\n",
    "    'hidden_size': 128,\n",
    "}\n",
    "\n",
    "model = MLP(input_size, hidden_size, output_size, dropout).to(device)\n",
    "\n",
    "trainloader, testloader, validloader = prepare_data(\n",
    "    fps_x, fps_y, \n",
    "    mode='descriptors',\n",
    "    batch_size=paramDict['batch_size'],\n",
    ")\n",
    "model = traindata(device, model, trainloader, validloader, epochs, optimizer_label, loss_function, paramDict['patience'])\n",
    "\n",
    "# Test\n",
    "acc, mcc, report = test(device, model, testloader)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "print('MCC: %.3f' % mcc)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics about the data, including:\n",
    "* Total sequences\n",
    "* Top 5 longest and shortest sequences\n",
    "* Average length of sequences\n",
    "* Top 5 most and least common sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stats(dataset):\n",
    "#     dataset[\"length\"].hist(bins=100)\n",
    "#     plt.show()\n",
    "\n",
    "#     print(\"-\" * 40)\n",
    "\n",
    "#     print(\"Total sequences:\", dataset.shape[0])\n",
    "\n",
    "#     print(\"-\" * 40)\n",
    "\n",
    "#     print(\"Top 5 longest sequences:\")\n",
    "#     print(\"id       length\")\n",
    "#     print(dataset[\"length\"].nlargest(5).to_string())\n",
    "\n",
    "#     print(\"-\" * 40)\n",
    "\n",
    "#     print(\"Top 5 shortest sequences:\")\n",
    "#     print(\"id       length\")\n",
    "#     print(dataset[\"length\"].nsmallest(5).to_string())\n",
    "\n",
    "#     print(\"-\" * 40)\n",
    "\n",
    "#     average_length = dataset[\"length\"].mean()\n",
    "#     print(\"Average length:\", average_length)\n",
    "\n",
    "#     print(\"-\" * 40)\n",
    "\n",
    "#     print(\"Top 5 most common lengths:\")\n",
    "#     print(\"length   count\")\n",
    "#     print(dataset[\"length\"].value_counts().nlargest(5).to_string())\n",
    "    \n",
    "#     print(\"-\" * 40)\n",
    "\n",
    "#     print(\"Top 5 least common lengths:\")\n",
    "#     print(\"length   count\")\n",
    "#     print(dataset[\"length\"].value_counts().nsmallest(5).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats(eg_positive)\n",
    "# print(\"*\" * 100)\n",
    "# stats(eg_negative)\n",
    "\n",
    "# eg_positive = eg_positive.drop(columns=[\"length\"])\n",
    "# eg_negative = eg_negative.drop(columns=[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.utils.data as data_utils\n",
    "# import src.encoding as enc\n",
    "# import os\n",
    "# from torch import nn\n",
    "# from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without using descriptors and using one hot encoding, all sequences need to have the same length. So, some sequences will be truncated or padded with Ns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fps_x = dataset['sequence'].values\n",
    "# fps_y = dataset['label'].values\n",
    "\n",
    "# average_length = int(average_length)\n",
    "\n",
    "# cut sequences to the average length\n",
    "# seqs_dataset = seqs_dataset.str.slice(0, average_length)\n",
    "\n",
    "# fill with \"N\" the sequences that are shorter than average length\n",
    "# seqs_dataset = seqs_dataset.str.pad(average_length, side='right', fillchar='N')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need now to split the dataset into training, test and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, x_test, y, y_test = train_test_split(\n",
    "#     fps_x, fps_y,\n",
    "#     test_size=0.2,\n",
    "#     train_size=0.8,\n",
    "#     stratify=fps_y\n",
    "# )\n",
    "# x_train, x_cv, y_train, y_cv = train_test_split(\n",
    "#     x, y,\n",
    "#     test_size=0.25,\n",
    "#     train_size=0.75,\n",
    "#     stratify=y\n",
    "# )\n",
    "\n",
    "# print(fps_x.shape)\n",
    "# print(fps_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to one hot encode the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_enc = enc.DNAEncoding(x_train)\n",
    "# x_train = x_train_enc.one_hot_encode()\n",
    "\n",
    "# x_test_enc = enc.DNAEncoding(x_test)\n",
    "# x_test = x_test_enc.one_hot_encode()\n",
    "\n",
    "# x_cv_enc = enc.DNAEncoding(x_cv)\n",
    "# x_cv = x_cv_enc.one_hot_encode()\n",
    "\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(y_test.shape)\n",
    "# print(x_cv.shape)\n",
    "# print(y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert to torch.tensor\n",
    "# train_data = data_utils.TensorDataset(\n",
    "#     torch.tensor(x_train, dtype=torch.float),\n",
    "#     torch.tensor(y_train, dtype=torch.long)\n",
    "# )\n",
    "# test_data = data_utils.TensorDataset(\n",
    "#     torch.tensor(x_test, dtype=torch.float),\n",
    "#     torch.tensor(y_test, dtype=torch.long)\n",
    "# )\n",
    "# valid_data = data_utils.TensorDataset(\n",
    "#     torch.tensor(x_cv, dtype=torch.float),\n",
    "#     torch.tensor(y_cv, dtype=torch.long)\n",
    "# )\n",
    "\n",
    "# batch_size = 16\n",
    "\n",
    "# # Data loader\n",
    "# trainloader = data_utils.DataLoader(\n",
    "#     train_data,\n",
    "#     shuffle=True,\n",
    "#     batch_size=batch_size\n",
    "# )\n",
    "# testloader = data_utils.DataLoader(\n",
    "#     test_data,\n",
    "#     shuffle=True,\n",
    "#     batch_size=batch_size\n",
    "# )\n",
    "# validloader = data_utils.DataLoader(\n",
    "#     valid_data,\n",
    "#     shuffle=True,\n",
    "#     batch_size=batch_size\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model equivalent to the one in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.models import MLP\n",
    "# from src.train import traindata\n",
    "# from src.test import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(2022)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4,5'\n",
    "# device = torch.device('cuda:0')\n",
    "\n",
    "# model = MLP().to(device)\n",
    "\n",
    "# paramDict = {\n",
    "#     'epoch': 200,\n",
    "#     'batchSize': 32,\n",
    "#     'dropOut': 0.2,\n",
    "#     'learning_rate': 0.004,\n",
    "#     'loss': nn.CrossEntropyLoss(),\n",
    "#     'metrics': ['accuracy'],\n",
    "#     'activation1': 'relu',\n",
    "#     'activation2': 'sigmoid',\n",
    "#     'monitor': 'val_accuracy',\n",
    "#     'save_best_only': True,\n",
    "#     'mode': 'max'\n",
    "# }\n",
    "\n",
    "# class_weight = {0: 1.0, 1: 4.0}\n",
    "\n",
    "# optimizerDict = {\n",
    "#     'adam': Adam(model.parameters(), learning_rate=0.001, beta_1=0.9, beta_2=0.999),\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # epochs = 100\n",
    "# # lr = 0.004\n",
    "# # loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# #optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# model = traindata(device, model, paramDict['epochs'], optimizerDict['adam'], paramDict['loss'], trainloader, validloader)\n",
    "\n",
    "# acc, mcc, report = test(device, model, testloader)\n",
    "# print('Accuracy: %.3f' % acc)\n",
    "# print('MCC: %.3f' % mcc)\n",
    "# print(report)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba449ea13c29f64a91968d8f927cecceedd6e605eda30388903386e6cd94168d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dna-conda': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
